<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Bits of Knowledge</title>
<link>https://antomon.github.io/collections/free-knowledge.html</link>
<atom:link href="https://antomon.github.io/collections/free-knowledge.xml" rel="self" type="application/rss+xml"/>
<description>Ever wanted to really get a subject‚Äînot just skim the surface, but dive deep and come out feeling like an expert? Welcome to Free Knowledge, where we gather the best freely available resources to help you master new ideas, whether they come from top academics, self-taught geniuses, or brilliant weirdos with a knack for explaining things. From full-length books that unravel complex ideas to university-grade video courses that turn your screen time into real learning, from thought-provoking essays that cut straight to the core to documentaries and podcasts that make knowledge entertaining, everything here is curated to help you go beyond trivia and develop real understanding. No paywalls, no fluff‚Äîjust the best knowledge out there, waiting for curious minds like yours to dig in. So grab a book, queue up a lecture, and get ready to level up your brain.</description>
<generator>quarto-1.7.27</generator>
<lastBuildDate>Fri, 18 Apr 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Dimitri P. Bertsekas‚Äô <strong>A Course in Reinforcement Learning</strong> (2nd Edition) is a deeply insightful and uniquely structured textbook that bridges the gap between classical optimization and the modern field of reinforcement learning (RL). Unlike most RL books that emerge from the machine learning community and focus heavily on empirical performance and implementation, Bertsekas approaches the subject from the lens of <strong>dynamic programming (DP)</strong>, <strong>control theory</strong>, and <strong>operations research</strong>, offering a mathematically grounded, algorithmically rigorous, and conceptually unified perspective.</p>
<section id="conceptual-focus-value-centric-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-focus-value-centric-reinforcement-learning">Conceptual focus: value-centric reinforcement learning</h3>
<p>The core philosophy of the book revolves around <strong>approximation in value space</strong>‚Äîa concept that plays a foundational role in dynamic programming and is extended here to RL settings. This is in contrast to the increasingly popular <strong>policy-gradient</strong> methods that dominate deep RL literature. Rather than focusing solely on training end-to-end policies with neural networks, Bertsekas places heavy emphasis on computing or approximating <strong>value functions</strong>, and using those to derive or improve policies.</p>
<p>This is a natural continuation of the ideas developed in his seminal works on <strong>Dynamic Programming and Optimal Control</strong>. It also reflects the underlying architecture of high-performing RL systems such as <strong>AlphaZero</strong> and <strong>TD-Gammon</strong>, both of which use trained evaluators in conjunction with powerful online search or rollout techniques. In fact, one of the book‚Äôs major contributions is to formalize and explain these empirical successes within a Newton-like optimization framework for solving Bellman‚Äôs equation.</p>
</section>
<section id="structure-and-content-overview" class="level3">
<h3 class="anchored" data-anchor-id="structure-and-content-overview">Structure and content overview</h3>
<p>The book is divided into <strong>three major chapters</strong>, designed to support both linear reading and modular, instructor-tailored course planning:</p>
<section id="chapter-1-exact-and-approximate-dynamic-programming" class="level4">
<h4 class="anchored" data-anchor-id="chapter-1-exact-and-approximate-dynamic-programming">Chapter 1: exact and approximate dynamic programming</h4>
<p>This foundational chapter provides a broad overview of the RL landscape, rooted in dynamic programming. It begins with deterministic and stochastic finite-horizon problems, develops into approximation methods, and presents the conceptual framework of <strong>offline training + online play</strong>, drawing detailed analogies with AlphaZero and model predictive control (MPC). It also introduces the reader to concepts like <strong>rollout</strong>, <strong>policy iteration</strong>, <strong>cost-to-go approximations</strong>, and <strong>terminal cost evaluation</strong>. The chapter is comprehensive and forms a standalone platform for readers unfamiliar with RL but versed in optimization.</p>
</section>
<section id="chapter-2-approximation-in-value-space-rollout-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="chapter-2-approximation-in-value-space-rollout-algorithms">Chapter 2: approximation in value space ‚Äì rollout algorithms</h4>
<p>Here, Bertsekas deepens the discussion on value function-based methods. He details variants of <strong>rollout algorithms</strong>‚Äîmethods that combine a base heuristic with lookahead or simulation to improve decisions in real-time. Topics include constrained optimization, Monte Carlo Tree Search (MCTS), randomized rollout, Bayesian optimization, and their application to deterministic and stochastic control, POMDPs, and even adversarial games. This chapter shows the versatility of the rollout paradigm across discrete and continuous domains.</p>
</section>
<section id="chapter-3-learning-values-and-policies" class="level4">
<h4 class="anchored" data-anchor-id="chapter-3-learning-values-and-policies">Chapter 3: learning values and policies</h4>
<p>This is where the book engages with <strong>neural networks</strong> and other parametric function approximators. It examines how value functions and policies can be learned from data using <strong>fitted value iteration</strong>, <strong>Q-learning</strong>, <strong>SARSA</strong>, and <strong>policy gradient methods</strong>. While coverage of policy optimization is relatively lean compared to deep RL books like Sutton &amp; Barto‚Äôs <em>Reinforcement Learning</em>, it is sufficient to understand core ideas and their integration into the Newton-based value approximation framework. The focus remains on conceptual clarity and practical convergence issues.</p>
<p>Each chapter ends with <strong>detailed notes, sources, and exercises</strong>, often pointing to supplementary material in Bertsekas‚Äô other books, such as <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em> (2020) or <em>Lessons from AlphaZero</em> (2022). These references make the book an excellent launchpad for research or deeper specialization.</p>
</section>
</section>
<section id="highlights-and-unique-strengths" class="level3">
<h3 class="anchored" data-anchor-id="highlights-and-unique-strengths">Highlights and unique strengths</h3>
<p>What sets <em>A Course in Reinforcement Learning</em> apart is its rare synthesis of intellectual depth, conceptual clarity, and practical relevance. Bertsekas leverages <strong>decades of foundational work in optimization theory and dynamic programming</strong>, crafting a perspective on RL that feels both timeless and sharply attuned to contemporary developments like AlphaZero. Rather than presenting RL as a bag of tricks or an empirical race to outperform benchmarks, the book offers a <strong>structured, principled framework</strong> for thinking about sequential decision-making under uncertainty.</p>
<p>A central strength lies in the <strong>elegant articulation of the synergy between offline learning and online planning</strong>. Instead of treating training as a static preprocessing step, Bertsekas shows how real-time control and model predictive strategies can interact meaningfully with learned approximations‚Äîa conceptual bridge that is both underexplored and urgently needed in modern RL discourse. This also gives the book a unique relevance to high-stakes engineering domains where stability, safety, and interpretability matter.</p>
<p>Another distinguishing trait is the author‚Äôs ability to <strong>speak fluently across disciplinary boundaries</strong>. Readers from control theory, operations research, artificial intelligence, and applied mathematics will all find familiar anchors‚Äîbut also be challenged to expand their mental models. The book avoids dense formalism without sacrificing rigor, preferring <strong>geometric insights, intuitive visualizations, and algorithmic thinking</strong> to heavy abstraction. This makes it an accessible yet intellectually satisfying read for those seeking more than surface-level understanding.</p>
<p>Moreover, the text is <strong>modular and customizable</strong>, making it ideal for various course structures‚Äîfrom short introductory classes to advanced research seminars on RL theory. Each chapter is self-contained yet richly interlinked, allowing instructors or self-learners to navigate the material according to their background and goals.</p>
</section>
<section id="enduring-relevance-in-a-fast-evolving-field" class="level3">
<h3 class="anchored" data-anchor-id="enduring-relevance-in-a-fast-evolving-field">Enduring relevance in a fast-evolving field</h3>
<p>As machine learning matures beyond supervised pattern recognition, the methods described in <em>A Course in Reinforcement Learning</em> are becoming increasingly vital. Bertsekas‚Äô emphasis on <strong>approximation in value space</strong>, <strong>policy iteration</strong>, and <strong>offline-online synergy</strong> aligns directly with the architecture of some of the most successful and well-known AI systems to date‚Äî<strong>AlphaZero</strong>, <strong>MuZero</strong>, and <strong>ChatGPT‚Äôs planning-inspired extensions</strong>. For example, MuZero (Schrittwieser et al., 2020) generalizes AlphaZero by learning its own model dynamics, yet it retains the <strong>value-based planning loop</strong> that Bertsekas formalizes through Newton-like updates on Bellman‚Äôs equation. Similarly, the resurgence of <strong>model-based reinforcement learning</strong> (e.g., DreamerV3, EfficientZero) is built on the same principle: learning value estimates offline and refining them online via planning‚Äîprecisely the synergy this book explores in depth.</p>
<p>In addition, current research on <strong>LLMs with planning capabilities</strong> (e.g., ReAct, Tree of Thoughts, and OpenAI‚Äôs work on tool-use agents) echoes the structure of <strong>lookahead planning guided by learned evaluators</strong>, another core theme in the book. These methods increasingly blend <strong>rollout-based reasoning</strong>, <strong>value estimation</strong>, and <strong>policy improvement</strong>‚Äîeven if not framed as reinforcement learning per se. Likewise, in robotics and safety-critical applications, algorithms must <strong>generalize reliably, adapt to perturbations</strong>, and <strong>provide interpretable decision-making</strong>‚Äîobjectives far better served by Bertsekas‚Äô structured, optimization-rooted methods than by end-to-end neural policy training alone.</p>
<p>Thus, what this book presents is not a retrospective‚Äîit is a <strong>forward-looking foundation</strong>. As machine learning turns toward <strong>long-horizon reasoning, planning under uncertainty, and adaptive control</strong>, the concepts in Bertsekas‚Äô work are proving to be not only relevant, but essential.</p>
</section>
<section id="who-is-this-book-for" class="level3">
<h3 class="anchored" data-anchor-id="who-is-this-book-for">Who is this book for?</h3>
<p>This textbook is not for everyone‚Äîand that‚Äôs a strength, not a weakness.</p>
<ul>
<li>Ideal readers include:
<ul>
<li>Graduate students in <strong>electrical engineering</strong>, <strong>applied mathematics</strong>, <strong>operations research</strong>, or <strong>control systems</strong>.</li>
<li>Researchers and professionals interested in <strong>optimization</strong>, <strong>model predictive control</strong>, <strong>robotics</strong>, or <strong>multiagent systems</strong>.</li>
<li>Advanced undergraduates with a solid mathematical background and interest in decision-making under uncertainty.</li>
<li>Practitioners who have experience with algorithms and modeling, and want to understand the ‚Äúwhy‚Äù behind reinforcement learning, not just the ‚Äúhow‚Äù.</li>
</ul></li>
<li>Less ideal for:
<ul>
<li>Beginners looking for an introductory, code-heavy book. For this, <em>Reinforcement Learning: An Introduction</em> by Sutton &amp; Barto or <em>Deep Reinforcement Learning Hands-On</em> by Maxim Lapan may be more approachable.</li>
<li>Readers wanting a focus on PyTorch/TensorFlow, environment setups, or software engineering best practices for RL pipelines.</li>
</ul></li>
</ul>
<p>Instead, Bertsekas offers something increasingly rare: a textbook that <strong>doesn‚Äôt treat reinforcement learning as a subfield of deep learning</strong>, but rather as a <strong>mathematically grounded discipline</strong> that can integrate with‚Äîbut also stand apart from‚Äîmodern AI trends.</p>
</section>
<section id="supplementary-materials" class="level3">
<h3 class="anchored" data-anchor-id="supplementary-materials">Supplementary materials</h3>
<p>The book is accompanied by <strong>video lectures, slides</strong>, and supplemental content from Bertsekas‚Äô Arizona State University course, accessible via his website. This makes the book particularly useful for <strong>self-study</strong>, especially for learners who appreciate visual explanation and conceptual repetition across formats.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p><em>A Course in Reinforcement Learning (2nd Edition)</em> is a rigorous, insightful, and conceptually rich text that helps readers <strong>understand reinforcement learning as a structured decision-making framework</strong>, not just a toolbox of tricks. If you‚Äôre serious about applying RL to real-world, high-stakes systems‚Äîwhere stability, interpretability, and theoretical guarantees matter‚Äîthis is the book you‚Äôve been looking for.</p>
<p>It challenges, rewards, and broadens the reader‚Äôs view of what reinforcement learning is and what it can be. Highly recommended for those looking to <strong>build durable understanding</strong>, not just quick implementations.</p>
</section>
<section id="further-readings" class="level3">
<h3 class="anchored" data-anchor-id="further-readings">Further readings</h3>
<ul>
<li><p>Bertsekas, D. P. (2022). <em>Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control</em>. Athena Scientific. <a href="https://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf">Download from author website</a></p></li>
<li><p>Bertsekas, D. P. (2020). <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em>. Athena Scientific. <a href="http://www.athenasc.com/rolloutbook_athena.html">More information</a></p></li>
<li><p>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., ‚Ä¶ &amp; Silver, D. (2020). <em>Mastering Atari, Go, Chess and Shogi by planning with a learned model</em>. Nature, 588(7839), 604‚Äì609. <a href="https://www.nature.com/articles/s41586-020-03051-4">Read the paper</a></p></li>
<li><p>Hafner, D., Lillicrap, T., Norouzi, M., &amp; Ba, J. (2023). <em>Mastering diverse domains through world models</em>. arXiv preprint arXiv:2301.04104. <a href="https://arxiv.org/abs/2301.04104">Read the paper</a></p></li>
<li><p>Ye, J., Lin, G., Xu, H., Liao, R., Yang, E., Lu, H., ‚Ä¶ &amp; Liu, S. (2021). <em>Mastering Atari Games with Limited Data</em>. Advances in Neural Information Processing Systems, 34. <a href="https://arxiv.org/abs/2111.00210">Read the paper</a></p></li>
<li><p>Yao, S., Zhao, J., Yu, D., Narasimhan, K., &amp; Zhang, Y. (2023). <em>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</em>. arXiv preprint arXiv:2305.10601. <a href="https://arxiv.org/abs/2305.10601">Read the paper</a></p></li>
<li><p>Yao, S., Zhao, J., Yu, D., Kasai, J., Wong, A., &amp; Zhang, Y. (2022). <em>ReAct: Synergizing Reasoning and Acting in Language Models</em>. arXiv preprint arXiv:2210.03629. <a href="https://arxiv.org/abs/2210.03629">Read the paper</a></p></li>
<li><p>Aghzal, M., Plaku, E., Stein, G. J., &amp; Yao, Z. (2025). <em>A Survey on Large Language Models for Automated Planning</em>. arXiv preprint arXiv:2502.12435. <a href="https://arxiv.org/abs/2502.12435">Read the paper</a></p></li>
<li><p>Tantakoun, M., Zhu, X., &amp; Muise, C. (2025). <em>LLMs as Planning Modelers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models</em>. arXiv preprint arXiv:2503.18971. <a href="https://arxiv.org/abs/2503.18971">Read the paper</a></p></li>
<li><p>Li, H., Chen, Z., Zhang, J., &amp; Liu, F. (2024). <em>LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning</em>. arXiv preprint arXiv:2409.01806. <a href="https://arxiv.org/abs/2409.01806">Read the paper</a></p></li>
<li><p>Changle, Q., et al.&nbsp;(2024). <em>Tool Learning with Large Language Models: A Survey</em>. Frontiers of Computer Science. <a href="https://arxiv.org/abs/2405.17935">Read the paper</a></p></li>
</ul>
</section>
</section>
<section id="about-the-author" class="level2">
<h2 class="anchored" data-anchor-id="about-the-author">About the author</h2>
<p>Dimitri P. Bertsekas is a world-renowned scholar in optimization and control theory. He received his Ph.D.&nbsp;in system science from MIT and has held faculty positions at Stanford, the University of Illinois, and MIT, where he remains McAfee Professor of Engineering. Since 2019, he has been Fulton Professor of Computational Decision Making at Arizona State University.</p>
<p>Over a prolific academic career, Bertsekas has authored over twenty influential books covering topics from nonlinear programming to data networks, and notably, dynamic programming and reinforcement learning. His contributions have earned him numerous accolades, including the IEEE Control Systems Award, INFORMS John von Neumann Theory Prize, and election to the U.S. National Academy of Engineering. His recent focus on RL reflects decades of foundational work in dynamic programming and optimization, making him a key figure in bridging classical control with modern machine learning.</p>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>A Course in Reinforcement Learning</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025 (2nd edition)</td>
</tr>
<tr class="odd">
<td><strong>Author</strong></td>
<td><a href="https://www.mit.edu/~dimitrib/home.html">Dimitri P. Bertsekas</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="http://www.athenasc.com/">Athena Scientific</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Reinforcement learning, Model predictive control, Dynamics programming, Machine learning</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE%202ndEDITION.pdf">Book PDF</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLbook.html">Course videolectures and materials</a></td>
</tr>
<tr class="odd">
<td><strong>ISBN/DOI</strong></td>
<td>1-886529-29-9</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="social-media" class="level2">
<h2 class="anchored" data-anchor-id="social-media">Social media</h2>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I am pleased to share a review of my book "A course in reinforcement learning" (2nd edition) <a href="https://t.co/VkRcIl6TB2">https://t.co/VkRcIl6TB2</a><br>This is the textbook for my RL course at ASU (free PDF at <a href="https://t.co/rB9CcChOHS">https://t.co/rB9CcChOHS</a>)<a href="https://twitter.com/hashtag/reinforcementlearning?src=hash&amp;ref_src=twsrc%5Etfw">#reinforcementlearning</a> <a href="https://twitter.com/hashtag/machinelearning?src=hash&amp;ref_src=twsrc%5Etfw">#machinelearning</a>
</p>
‚Äî Dimitri Bertsekas (<span class="citation" data-cites="DBertsekas">@DBertsekas</span>) <a href="https://twitter.com/DBertsekas/status/1913627106552209493?ref_src=twsrc%5Etfw">April 19, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>üá¨üáß</category>
  <guid>https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/</guid>
  <pubDate>Fri, 18 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Probabilistic Machine Learning - Kevin Murphy</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/1.jpg" class="img-fluid"></p>
</div><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/2.jpg" class="img-fluid"></p>
</div><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/3.jpg" class="img-fluid"></p>
</div></div>

<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<section id="introduction-what-is-probabilistic-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="introduction-what-is-probabilistic-machine-learning">Introduction: what Is probabilistic machine learning?</h3>
<p>Machine learning (ML) is the discipline that focuses on designing algorithms that allow computers to learn from data, make predictions, and improve over time without being explicitly programmed for each task. At its core, traditional machine learning can often be understood as a function approximation problem: given inputs <img src="https://latex.codecogs.com/png.latex?X">, learn a function <img src="https://latex.codecogs.com/png.latex?f:%20X%20%5Crightarrow%20Y"> that maps those inputs to outputs. Many successful techniques‚Äîincluding decision trees, support vector machines, and deep neural networks‚Äîfit this paradigm and have achieved outstanding performance on tasks such as image recognition, language modeling, and game playing.</p>
<p>However, real-world problems are rarely deterministic or noiseless. Uncertainty, ambiguity, and incomplete information are the norm rather than the exception. This is where <strong>probabilistic machine learning (PML)</strong> becomes essential.</p>
<p>Probabilistic machine learning is a subfield of ML that emphasizes modeling uncertainty explicitly. Rather than producing a single output or estimate, a probabilistic model produces a <strong>distribution</strong> over possible outcomes, quantifying the confidence in each prediction. In this paradigm, learning is formulated as a problem of <strong>statistical inference</strong>: given observed data, infer the posterior distribution over unobserved variables (such as model parameters, latent states, or predictions).</p>
<p>At the heart of PML is the application of probability theory to every stage of the learning process:</p>
<ul>
<li>Bayesian reasoning allows incorporation of prior knowledge and principled uncertainty estimation.</li>
<li>Latent variable models capture hidden structure in data.</li>
<li>Graphical models express dependencies between variables clearly and concisely.</li>
<li>Approximate inference methods (e.g., variational inference, MCMC) make complex models computationally tractable.</li>
</ul>
<p>In short, probabilistic machine learning provides a <strong>principled, coherent, and extensible framework</strong> for designing models that reason under uncertainty and adapt to changing data distributions.</p>
<p>While traditional ML methods often prioritize point predictions and empirical accuracy, PML emphasizes:</p>
<ul>
<li>Uncertainty quantification</li>
<li>Interpretability and diagnostics</li>
<li>Robustness to noise and model misspecification</li>
<li>Principled model comparison</li>
<li>Decision-making under uncertainty</li>
</ul>
<p>The rise of large-scale deep learning has produced models that are often opaque and poorly calibrated. As machine learning systems are increasingly used in critical applications‚Äîsuch as healthcare, finance, climate modeling, and autonomous vehicles‚Äîthe ability to <strong>reason about uncertainty, causality, and decision-making</strong> becomes vital. PML provides the tools to:</p>
<ul>
<li>Assess when a model is unsure or operating out-of-distribution</li>
<li>Build systems that can update beliefs as new evidence arrives</li>
<li>Integrate domain knowledge via priors and structured models</li>
<li>Make robust decisions in the face of uncertainty and risk</li>
</ul>
<p>Modern advances‚Äîsuch as <strong>variational autoencoders</strong>, <strong>normalizing flows</strong>, <strong>Bayesian deep learning</strong>, <strong>causal inference frameworks</strong>, and <strong>probabilistic programming languages</strong>‚Äîhave dramatically expanded the scope and scalability of PML.</p>
<p>Kevin Murphy‚Äôs trilogy of books stands out for presenting the field of machine learning entirely through the probabilistic lens. His work provides a unified view of learning, inference, prediction, and decision-making, making the case that probabilistic modeling is not an alternative to machine learning‚Äîit is its natural and rigorous generalization.</p>
</section>
<section id="the-trilogy" class="level3">
<h3 class="anchored" data-anchor-id="the-trilogy">The trilogy</h3>
<section id="machine-learning-a-probabilistic-perspective-2012" class="level4">
<h4 class="anchored" data-anchor-id="machine-learning-a-probabilistic-perspective-2012">1. Machine Learning: A Probabilistic Perspective (2012)</h4>
<p>Murphy‚Äôs first book, published in 2012, was a landmark achievement that laid the groundwork for much of the modern probabilistic approach to machine learning. Spanning over 1000 pages, it offers a comprehensive and mathematically rigorous treatment of probabilistic modeling, treating learning as a problem of statistical inference. The book begins with foundational material in probability theory and Bayesian statistics before delving into a broad array of topics such as supervised learning (regression and classification), unsupervised learning (clustering, dimensionality reduction), probabilistic graphical models (Bayesian networks and Markov random fields), and approximate inference techniques (variational inference, Markov Chain Monte Carlo). It also covers expectation-maximization, hidden Markov models, kernel methods, and Gaussian processes. This volume is notable for its unified framework that emphasizes modeling uncertainty, integrating prior knowledge, and reasoning about complex data using structured probabilistic methods.</p>
<p>Core content:</p>
<ul>
<li>Probability theory, Bayesian statistics, decision theory.</li>
<li>Supervised learning: regression, classification, support vector machines.</li>
<li>Unsupervised learning: clustering, dimensionality reduction.</li>
<li>Probabilistic graphical models: Bayesian networks and Markov random fields.</li>
<li>Approximate inference: variational methods, MCMC.</li>
<li>Expectation-maximization (EM), mixture models, and hidden Markov models.</li>
<li>Model comparison, selection, and overfitting.</li>
<li>Kernel methods and Gaussian processes.</li>
</ul>
<p>The book is methodical and mathematically grounded. It doesn‚Äôt shy away from complexity but presents concepts in a pedagogically sound manner. It serves both as a textbook and a reference, rich in examples and code snippets (originally in MATLAB/Octave).</p>
<p>While deep learning has dramatically changed the machine learning landscape since 2012, the foundations presented in MLAPP remain critically relevant. The probabilistic treatment of learning, inference, and model uncertainty continues to underpin research in Bayesian deep learning, causal inference, reinforcement learning, and decision-making.</p>
<p>However, the book does not cover recent developments such as generative models (GANs, VAEs, diffusion models), large-scale optimization in neural networks, or probabilistic programming systems, making it less suitable for those primarily focused on modern deep learning pipelines.</p>
</section>
<section id="probabilistic-machine-learning-an-introduction-2022" class="level4">
<h4 class="anchored" data-anchor-id="probabilistic-machine-learning-an-introduction-2022">2. Probabilistic Machine Learning: An Introduction (2022)</h4>
<p>This volume represents a pedagogical reboot and modernization of MLAPP. It revisits many of the same foundational topics but presents them with improved clarity, a more focused scope, and integration with modern tools such as JAX and NumPyro for probabilistic programming. The book begins with the fundamentals of probability theory and statistical inference, emphasizing both Bayesian and frequentist approaches. It then covers key techniques in supervised learning, such as linear and logistic regression, and introduces Gaussian distributions, conjugate priors, and hierarchical models. Latent variable models such as PCA and Gaussian mixture models are explored in accessible terms. Notably, the book introduces approximate inference using variational methods and connects them to probabilistic programming frameworks. It also addresses model evaluation techniques, such as cross-validation and information criteria, and concludes with an overview of Gaussian processes. Overall, this volume offers a solid, modern foundation in probabilistic modeling, suitable for both students and practitioners entering the field.</p>
<p>Core content:</p>
<ul>
<li>Probability and statistics for machine learning.</li>
<li>Bayesian and frequentist inference.</li>
<li>Maximum likelihood estimation, MAP, posterior predictive distributions.</li>
<li>Conjugate priors, hierarchical models, and empirical Bayes.</li>
<li>Gaussian distributions, linear regression, logistic regression.</li>
<li>Gaussian processes (including scalable approximations).</li>
<li>Probabilistic programming and variational inference.</li>
<li>Decision theory and model evaluation (cross-validation, information criteria).</li>
<li>Basic latent variable models: PCA, mixture models, factor analysis.</li>
</ul>
<p>Several pedagogical enhancements that support learning and experimentation are introduced in this book. Readers are provided with runnable Jupyter notebooks, conveniently hosted in Google Colab, which allow for hands-on interaction with code and models. The book‚Äôs figures and illustrations are generated directly from these notebooks, enabling a tight integration between theory, visualization, and practice. The writing style emphasizes conceptual clarity without compromising on mathematical rigor, making advanced topics more approachable and easier to internalize for a wide audience.</p>
<p>This book is arguably more relevant than MLAPP for a newcomer or intermediate reader in 2025. It reflects the shift toward integrating probability with scalable modern tools and introduces practical workflows for modeling and inference. While it deliberately avoids diving deep into generative models or reinforcement learning, it sets a strong foundation for those topics.</p>
<p>Importantly, readers do not need to read <em>Machine Learning: A Probabilistic Perspective</em> before tackling this book. <em>Probabilistic Machine Learning: An Introduction</em> is designed to be self-contained and more accessible, making it an ideal starting point for those new to the field or those seeking a practical yet principled approach to probabilistic modeling.</p>
<p>For instructors designing machine learning or probabilistic modeling courses, it is an excellent primary textbook. than MLAPP for a newcomer or intermediate reader in 2025. It reflects the shift toward integrating probability with scalable modern tools and introduces practical workflows for modeling and inference. While it deliberately avoids diving deep into generative models or reinforcement learning, it sets a strong foundation for those topics.</p>
<p>For instructors designing machine learning or probabilistic modeling courses, it is an excellent primary textbook.</p>
</section>
<section id="probabilistic-machine-learning-advanced-topics-2025-draft" class="level4">
<h4 class="anchored" data-anchor-id="probabilistic-machine-learning-advanced-topics-2025-draft">3. Probabilistic Machine Learning: Advanced Topics (2025 Draft)</h4>
<p>The newly released draft of <em>Advanced Topics</em> is the final and most ambitious volume in Murphy‚Äôs trilogy. Designed as a natural progression from <em>An Introduction</em>, it targets readers already comfortable with the fundamentals of probabilistic modeling and Bayesian inference. This volume delves into state-of-the-art techniques that define the cutting edge of modern probabilistic machine learning.</p>
<section id="structure" class="level5">
<h5 class="anchored" data-anchor-id="structure">Structure</h5>
<p>The book is structured into six thematic parts:</p>
<ul>
<li><p><strong>Part I (Fundamentals)</strong> provides a deep theoretical base, covering advanced probability, exponential family models, divergence measures, and optimization principles. It also revisits graphical models in greater depth, including conditional random fields and structured representations.</p></li>
<li><p><strong>Part II (Inference)</strong> focuses on modern approximate inference techniques. It presents Gaussian filtering and smoothing (e.g., Kalman filters and their nonlinear extensions), belief propagation on graphs, variational inference (both classic and black-box forms), and a suite of Monte Carlo methods including Hamiltonian Monte Carlo and sequential Monte Carlo.</p></li>
<li><p><strong>Part III (Prediction)</strong> explores models for supervised learning and uncertainty-aware prediction. This includes generalized linear models, Bayesian neural networks, Gaussian processes with deep kernels, and models for handling non-iid data and distributional shift.</p></li>
<li><p><strong>Part IV (Generation)</strong> is devoted to deep generative models. It introduces variational autoencoders, normalizing flows, diffusion models, autoregressive networks, and energy-based models, with careful attention to training objectives, model evaluation, and sampling.</p></li>
<li><p><strong>Part V (Discovery)</strong> addresses unsupervised and representation learning. It covers latent factor models, state-space models, topic models, deep sequence modeling, graph structure discovery, and interpretability through the lens of probabilistic inference.</p></li>
<li><p><strong>Part VI (Action)</strong> focuses on decision-making, reinforcement learning, and causality. Topics include decision theory, active learning, policy search, model-based and model-free reinforcement learning, influence diagrams, and modern approaches to causal inference including do-calculus, instrumental variables, and counterfactual reasoning.</p></li>
</ul>
<p>Throughout, the book maintains a strong emphasis on scalable inference, modern software tools, and connections between theory and real-world applications. It is both technically deep and broad in scope, providing readers with the tools and intuition needed to work on contemporary research problems in probabilistic modeling, decision-making, and AI.</p>
</section>
<section id="technical-depth-and-breadth" class="level5">
<h5 class="anchored" data-anchor-id="technical-depth-and-breadth">Technical depth and breadth</h5>
<p>This volume is the most technically rigorous of the trilogy. It introduces advanced concepts from statistics, stochastic processes, signal processing, and control theory‚Äîdisciplines that underpin many of today‚Äôs most impactful machine learning innovations. Its comprehensive treatment of probabilistic graphical models and information theory lays the groundwork for structured generative models and modern approaches to representation learning. The discussion of variational inference and Monte Carlo techniques directly supports scalable inference in applications such as variational autoencoders and Bayesian neural networks.</p>
<p>Reinforcement learning is explored through a probabilistic lens, emphasizing the ‚Äúcontrol as inference‚Äù paradigm, which has gained traction in contemporary deep reinforcement learning frameworks. The causal inference section equips readers with formal tools like do-calculus and instrumental variables, essential for understanding causality-aware systems now central to policy evaluation and scientific discovery.</p>
<p>Practical applications that embody these methods include: uncertainty quantification in medical diagnostics using Bayesian neural networks; model-based reinforcement learning in robotic control systems; causal effect estimation in healthcare and social science through counterfactual reasoning; high-fidelity image generation using diffusion models; and structural learning in genomics with probabilistic graphical models. These examples demonstrate how the book‚Äôs theoretical foundation translates into applied machine learning systems that operate under uncertainty with reliability and interpretability.</p>
<p>Overall, the volume serves as a bridge connecting classical probabilistic theory with the most recent developments in generative AI, causal discovery, and probabilistic decision-making under uncertainty.</p>
</section>
<section id="relevance-today" class="level5">
<h5 class="anchored" data-anchor-id="relevance-today">Relevance today</h5>
<p>Few books capture the breadth and depth of modern probabilistic machine learning like this one. The inclusion of cutting-edge generative models, reinforcement learning frameworks, and causal reasoning makes it highly relevant. The tight integration with the current state of research ensures that the book will remain a key reference for years to come.</p>
<p>However, it is not intended for beginners. It presupposes comfort with advanced probability, linear algebra, and statistical inference, as well as practical fluency with modern ML tools.</p>
</section>
</section>
</section>
<section id="recommendations" class="level3">
<h3 class="anchored" data-anchor-id="recommendations">Recommendations</h3>
<p>The following table is designed to help readers at different levels identify the most appropriate entry point into Kevin Murphy‚Äôs trilogy. Whether you‚Äôre a student just beginning to explore probabilistic methods or a researcher seeking depth in advanced generative modeling, this summary suggests a tailored path through the books. The guidance considers both the technical demands and the intended pedagogical focus of each volume.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 44%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Reader profile</th>
<th>Suggested reading path</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Undergraduate ML student</td>
<td><em>Probabilistic Machine Learning: An Introduction</em></td>
</tr>
<tr class="even">
<td>Graduate student in ML/statistics</td>
<td><em>Introduction</em> ‚Üí <em>Advanced Topics</em></td>
</tr>
<tr class="odd">
<td>ML instructor or course designer</td>
<td><em>Introduction</em> as textbook, <em>Advanced Topics</em> for graduate seminar</td>
</tr>
<tr class="even">
<td>Probabilistic programming/research engineer</td>
<td><em>Advanced Topics</em> (esp.&nbsp;inference and generative models)</td>
</tr>
<tr class="odd">
<td>Causal inference/decision science researcher</td>
<td><em>Advanced Topics</em>, especially Part VI</td>
</tr>
<tr class="even">
<td>Deep learning expert seeking interpretability and uncertainty tools</td>
<td><em>Advanced Topics</em>, especially Bayesian deep learning and conformal prediction</td>
</tr>
<tr class="odd">
<td>General ML practitioner</td>
<td><em>Introduction</em> for foundation, followed by selected chapters from <em>Advanced Topics</em></td>
</tr>
</tbody>
</table>
</section>
<section id="relation-to-contemporary-trends-in-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="relation-to-contemporary-trends-in-machine-learning">Relation to contemporary trends in machine learning</h3>
<p>Murphy‚Äôs trilogy is deeply relevant to the dominant themes and architectures that currently define machine learning research and application. Notably, the series intersects with several state-of-the-art paradigms such as large language models (LLMs), diffusion models, and other generative frameworks that have become increasingly influential.</p>
<p>In the context of LLMs, while Murphy‚Äôs books do not focus on transformer architectures directly, they provide the underlying probabilistic theory necessary to reason about uncertainty, representation learning, and approximate inference in high-dimensional models. This is critical for efforts in Bayesian deep learning applied to LLMs, such as uncertainty-aware language generation, model calibration, and adaptive fine-tuning. Additionally, the treatment of autoregressive modeling and structured sequence generation in the <em>Advanced Topics</em> volume aligns closely with the mathematical principles that underlie language models.</p>
<p>Regarding diffusion models‚Äîwhich have become central to generative image and video synthesis‚ÄîMurphy‚Äôs final volume offers one of the few textbook treatments of these architectures from a probabilistic perspective. It places diffusion models in the broader landscape of score-based generative models, stochastic differential equations, and probabilistic denoising. This not only clarifies how these models work but also situates them in a principled framework for likelihood-based training and sampling.</p>
<p>Moreover, the books‚Äô emphasis on latent variable models, variational inference, and probabilistic programming provides essential context for understanding hybrid approaches that combine deterministic deep networks with stochastic components‚Äîan increasingly common design in modern ML systems.</p>
<p>In sum, while Murphy‚Äôs books are not focused on deep learning trends per se, they offer foundational and theoretical insight that is crucial for interpreting, extending, and critiquing today‚Äôs most influential machine learning models.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p>Kevin Murphy‚Äôs <em>Probabilistic Machine Learning</em> trilogy offers an exceptional and enduring contribution to the field of machine learning. It accomplishes what few educational resources have: it builds a conceptual and mathematical bridge between foundational statistical thinking and the evolving frontier of machine learning research.</p>
<p><em>Machine Learning: A Probabilistic Perspective</em> serves as a deep and comprehensive reference text, best suited to readers with a solid foundation in mathematics and a desire to explore classical probabilistic modeling in depth. Despite its age, it remains highly relevant for understanding the theoretical underpinnings of the field.</p>
<p><em>Probabilistic Machine Learning: An Introduction</em> is the most accessible and pedagogically refined volume. It strikes a balance between formal rigor and practical usability, making it the best entry point for students, practitioners, and instructors aiming to teach or learn probabilistic reasoning in modern contexts.</p>
<p><em>Probabilistic Machine Learning: Advanced Topics</em> is a masterful synthesis of recent innovations, making it a must-read for researchers, PhD students, and experienced engineers interested in state-of-the-art techniques for uncertainty modeling, generative modeling, causality, and decision-making under uncertainty.</p>
<p>Collectively, these volumes are more than just textbooks‚Äîthey form a modern curriculum for anyone serious about understanding and building intelligent systems capable of reasoning under uncertainty. Whether used in academia, research, or applied settings, Murphy‚Äôs trilogy provides the theoretical backbone and practical insight necessary to advance the field of machine learning responsibly and rigorously.</p>
</section>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td><em>Probabilistic Machine Learning</em> (3-volume series)</td>
</tr>
<tr class="even">
<td><strong>Years</strong></td>
<td>2012 (<em>MLAPP</em>), 2022 (<em>Introduction</em>), 2025 (<em>Advanced Topics</em>, draft)</td>
</tr>
<tr class="odd">
<td><strong>Author</strong></td>
<td><a href="https://www.cs.ubc.ca/~murphyk/">Kevin P. Murphy</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Bayesian statistics, probabilistic inference, graphical models, generative models, reinforcement learning, causality</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://probml.github.io/">Books site</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://github.com/probml/pyprobml">GitHub notebooks</a></td>
</tr>
<tr class="odd">
<td><strong>ISBNs</strong></td>
<td>978-0262046824 (<em>Introduction</em>), 978-0262048378 (<em>Advanced Topics</em>)</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td><a href="https://mitpress.mit.edu/9780262046824/probabilistic-machine-learning/">MIT Press ‚Äì Introduction</a> / <a href="https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/">MIT Press ‚Äì Advanced Topics</a></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>üá¨üáß</category>
  <guid>https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/</guid>
  <pubDate>Sat, 05 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/3.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Differential Privacy - Simson L. Garfinkel</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/m_9780262382168-cover.jpeg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>This monograph provides a non-mathematical yet rigorous conceptual introduction to the theory and application of <strong>differential privacy (DP)</strong>. It is part of MIT Press‚Äôs <em>Essential Knowledge</em> series and is explicitly written for a <strong>general audience</strong>, without requiring prior familiarity with statistics, mathematics, or computer science.</p>
<p>Rather than offering algorithmic recipes or formal proofs, the book aims to:</p>
<ul>
<li>Build <strong>intuitive understanding</strong> of the DP paradigm.</li>
<li>Trace the <strong>historical development</strong> of the field.</li>
<li>Contextualize DP through <strong>real-world implementations</strong> (notably the 2020 U.S. Census).</li>
<li>Examine the <strong>philosophical and policy implications</strong> of deploying DP systems.</li>
</ul>
<section id="content-summary" class="level3">
<h3 class="anchored" data-anchor-id="content-summary">Content summary</h3>
<section id="conceptual-foundations" class="level4">
<h4 class="anchored" data-anchor-id="conceptual-foundations">Conceptual foundations</h4>
<p>Garfinkel introduces differential privacy as a <strong>mathematically formal approach to privacy protection</strong>, contrasted with heuristic or ‚Äúbest effort‚Äù techniques such as de-identification and data masking.</p>
<p>Key theoretical components are explained using analogies:</p>
<ul>
<li><strong>Sensitivity</strong> and the use of <strong>calibrated noise</strong> via the Laplace or Gaussian mechanisms.</li>
<li>The <strong>privacy loss budget</strong> (Œµ) and the implications of tuning it.</li>
<li><strong>Composability</strong> and how DP offers provable guarantees even under repeated queries.</li>
<li>The <strong>mosaic effect</strong>, demonstrating how auxiliary data enables reidentification even from aggregated or anonymized data.</li>
</ul>
<p>Although mathematical notation is absent, the conceptual treatment aligns with the formal definitions of (Œµ, Œ¥)-differential privacy as used in foundational literature.</p>
</section>
<section id="historical-and-institutional-context" class="level4">
<h4 class="anchored" data-anchor-id="historical-and-institutional-context">Historical and institutional context</h4>
<p>The author documents the trajectory of DP from its introduction by Dwork, McSherry, Nissim, and Smith (2006) to its adoption in high-stakes settings such as:</p>
<ul>
<li>The U.S. Census Bureau‚Äôs <strong>Disclosure Avoidance System</strong>.</li>
<li>Google‚Äôs and Apple‚Äôs telemetry systems.</li>
<li>NIST‚Äôs evolving standards for privacy-enhancing technologies (e.g., SP 800-188).</li>
</ul>
<p>The tension between <strong>statistical utility and privacy guarantees</strong> is explored through case studies, such as the controversy surrounding the use of DP in the 2020 Census.</p>
</section>
<section id="critique-and-alternatives" class="level4">
<h4 class="anchored" data-anchor-id="critique-and-alternatives">Critique and Alternatives</h4>
<p>Garfinkel acknowledges the limitations of DP:</p>
<ul>
<li>Not suitable for <strong>non-tabular data</strong> (e.g., video, audio).</li>
<li>Noise injection can <strong>degrade data utility</strong>, especially for small subpopulations.</li>
<li>DP is often <strong>challenging to tune</strong>, and the setting of Œµ is as much a <strong>policy decision</strong> as a technical one.</li>
</ul>
<p>He contrasts DP with legacy disclosure limitation methods (e.g., k-anonymity, cell suppression, top-coding), arguing that while they lack formal guarantees, they may provide better practical utility in some scenarios.</p>
<p>Strengths:</p>
<ul>
<li><strong>Clarity of exposition</strong>: Abstract concepts are explained using accessible examples (e.g., student grades, public statistics).</li>
<li><strong>Policy relevance</strong>: The author‚Äôs experience at NIST and the Census Bureau enables an authoritative treatment of institutional use cases.</li>
<li><strong>Balanced perspective</strong>: Both the promises and criticisms of DP are articulated, with references to academic and legal debates (e.g., lawsuits surrounding the 2020 Census).</li>
</ul>
<p>Limitations:</p>
<ul>
<li><strong>Lack of formalism</strong>: No mathematical definitions, algorithms, or implementation details are presented.</li>
<li><strong>Limited scope of mechanisms</strong>: The book focuses almost exclusively on the <strong>central differential privacy model</strong>, with minimal treatment of <strong>local DP</strong>, <strong>privacy accounting methods</strong> (e.g., R√©nyi DP), or advanced techniques like <strong>privacy amplification</strong>.</li>
<li><strong>No discussion of implementation frameworks</strong>: Libraries like Google‚Äôs DP library, OpenDP, and SmartNoise are only briefly mentioned in the notes, with no comparative discussion.</li>
</ul>
</section>
</section>
<section id="related-works" class="level3">
<h3 class="anchored" data-anchor-id="related-works">Related works</h3>
<p>The book is best read alongside more technical treatments, such as:</p>
<ul>
<li>Dwork, C., &amp; Roth, A. (2014). <em>The algorithmic foundations of differential privacy</em>. Now Publishers. <a href="https://doi.org/10.1561/0400000042">DOI</a></li>
<li>Gaboardi, M., Honaker, J., &amp; Stoddard, J. (2023). <em>Programming differential privacy</em>. O‚ÄôReilly Media.</li>
<li>National Institute of Standards and Technology. (2023). <em>De-identifying government datasets: Techniques and governance (NIST Special Publication 800-188)</em>. U.S. Department of Commerce. <a href="https://doi.org/10.6028/NIST.SP.800-188">DOI</a></li>
</ul>
</section>
<section id="recommendation" class="level3">
<h3 class="anchored" data-anchor-id="recommendation">Recommendation</h3>
<p>This book is recommended as a policy- and systems-oriented primer for:</p>
<ul>
<li>Privacy professionals and data protection officers.</li>
<li>Statisticians and social scientists new to formal privacy models.</li>
<li>Government and industry decision-makers evaluating privacy technologies.</li>
</ul>
<p>It is <strong>not</strong> a substitute for a formal or computational introduction for practitioners seeking to implement DP mechanisms in production systems.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p>Simson L. Garfinkel‚Äôs <em>Differential Privacy</em> is an essential conceptual guide that democratizes access to one of the most profound ideas in data privacy. While limited in technical depth, it succeeds in making the case for DP‚Äôs significance and encourages broader adoption and informed critique.</p>
</section>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>Differential Privacy</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025</td>
</tr>
<tr class="odd">
<td><strong>Author</strong></td>
<td><a href="https://simson.net/page/Main_Page">Simson L. Garfinkel</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Differential privacy, Statistical disclosure limitation, Privacy-preserving data analysis, Data governance</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf">PDF | CC BY-NC-ND 4.0</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://direct.mit.edu/books/book/5935/Differential-Privacy">Publisher book page</a></td>
</tr>
<tr class="odd">
<td><strong>ISBN</strong></td>
<td>9780262382168</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td><a href="https://mitpress.mit.edu/9780262551656/differential-privacy/">MIT Press</a></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>data privacy</category>
  <category>üá¨üáß</category>
  <guid>https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/</guid>
  <pubDate>Tue, 01 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/m_9780262382168-cover.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Design Rules, Volume 2: How Technology Shapes Organizations - Carliss Y. Baldwin</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/m_9780262380232-cover.jpeg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In a world increasingly dominated by digital technologies, the traditional boundaries of organizations are being redrawn. Vertically integrated firms are giving way to globally distributed ecosystems. Software is being built by loosely affiliated communities rather than centralized R&amp;D labs. And platform-based businesses like Apple, Amazon, and Google now dominate entire sectors. But what exactly is driving these structural shifts in how work is organized? What is the role of technology in shaping the form and function of modern organizations?</p>
<p>These are the questions at the heart of <em>Design Rules, Volume 2: How Technology Shapes Organizations</em>, the latest contribution by Harvard Business School professor <strong>Carliss Y. Baldwin</strong>. This volume is the long-anticipated sequel to <em>Design Rules, Volume 1: The Power of Modularity</em> (2000), co-authored with Kim B. Clark. Together, the <em>Design Rules</em> series represents one of the most ambitious efforts in the past two decades to build a <strong>unified theory of technology and organizational design</strong>.</p>
<p>Where Volume 1 introduced the foundational concept of <strong>modularity</strong>‚Äîthe idea that complex technical systems can be broken into loosely coupled, well-defined components‚ÄîVolume 2 builds on this to offer a much broader and more general framework. The new volume is not just about modularity, but about <strong>how technologies, by their very structure and dynamics, shape the organizations that implement them</strong>. It goes beyond software and hardware, offering a theory with implications for economics, management science, industrial organization, and systems engineering.</p>
<p>Baldwin‚Äôs central premise is simple but profound: the <strong>design of technology</strong> imposes both <strong>constraints and affordances</strong> on how organizations must be structured if they are to implement that technology efficiently and capture value from it. Organizations, in this view, are not just social or legal constructs‚Äîthey are functional responses to the demands of the technologies they use.</p>
<p>Through theoretical innovation and in-depth case analysis, Baldwin argues that different technologies give rise to different forms of organization: from tightly controlled hierarchies to loose, distributed ecosystems and open-source communities. The key lies in understanding the <strong>complementarity relationships</strong> among the components of a technical system‚Äîwhether they are so tightly interdependent that they require centralized control, or loosely connected enough to permit decentralized innovation.</p>
<p>Volume 2 is a sweeping, multidisciplinary work that synthesizes insights from organizational economics, evolutionary theory, systems engineering, and the history of technology. It is both conceptually rigorous and empirically grounded, offering a unique and powerful perspective on how the deep structure of technology influences the surface form of organizational life.</p>
<section id="key-contributions" class="level3">
<h3 class="anchored" data-anchor-id="key-contributions">Key contributions</h3>
<p>At the heart of <em>Design Rules, Volume 2</em> lies an ambitious effort to explain how the structure of technology determines the structure of the organizations that use it. Carliss Baldwin lays out four key contributions that form the backbone of this sweeping theoretical and empirical work.</p>
<section id="a-general-theory-of-technology-and-organization" class="level4">
<h4 class="anchored" data-anchor-id="a-general-theory-of-technology-and-organization">A general theory of technology and organization</h4>
<p>First, Baldwin develops a general theory that places technology at the center of organizational design. She argues that technologies are not neutral tools‚Äîthey come with <strong>material requirements</strong> and <strong>structural affordances</strong> that shape how organizations must be arranged in order to implement them effectively. Those organizations that align well with the constraints and opportunities presented by a given technology are more likely to survive, scale, and capture value in a competitive environment. This co-evolutionary view reframes technology as a driving force in the emergence, transformation, and demise of organizational forms.</p>
</section>
<section id="the-spectrum-of-complementarity" class="level4">
<h4 class="anchored" data-anchor-id="the-spectrum-of-complementarity">The spectrum of complementarity</h4>
<p>Second, Baldwin introduces the concept of the <strong>spectrum of complementarity</strong>, a powerful framework for understanding how the relationships between components of a technical system influence organizational choices. On one end of the spectrum are <strong>strong complements</strong>, where components must work together in tightly integrated ways. These favor <strong>centralized, hierarchical firms</strong> with unified governance. On the other end are <strong>weak complements</strong>, where components can be combined more flexibly and independently. These favor <strong>modular ecosystems</strong>, <strong>platform-based business models</strong>, and even <strong>open-source networks</strong>, where distributed governance, minimal hierarchy, and radical transparency can flourish. The degree of complementarity thus predicts the most efficient and sustainable form of organization for a given technology.</p>
</section>
<section id="value-structure-analysis" class="level4">
<h4 class="anchored" data-anchor-id="value-structure-analysis">Value structure analysis</h4>
<p>Third, the book introduces a novel method for visualizing and analyzing technical systems: <strong>value structure analysis</strong>. This approach maps the components of a system in terms of the tasks they perform and the value they contribute. By focusing on <strong>‚Äúthin crossing points‚Äù</strong>‚Äîinterfaces between loosely coupled modules‚ÄîBaldwin identifies where <strong>transaction costs are lowest</strong> and where opportunities for coordination, specialization, or platformization are greatest. This method is used throughout the book to analyze how systems create value, how that value can be captured (or lost), and how technical design decisions ripple outward into economic and organizational consequences.</p>
</section>
<section id="historical-and-contemporary-case-studies" class="level4">
<h4 class="anchored" data-anchor-id="historical-and-contemporary-case-studies">Historical and contemporary case studies</h4>
<p>Finally, Baldwin grounds her theory in a series of richly detailed case studies that span more than a century of industrial evolution. She traces the transition from the <strong>vertically integrated mass production firms</strong> of the early 20th century (like Ford and IBM) to the <strong>horizontally layered ecosystems</strong> of the digital age (such as Wintel, Dell, and Google). She explores the rise of <strong>open-source software</strong> and <strong>DevOps cultures</strong> as new organizational responses to the unique properties of software as a technology. And she examines the economic dynamics unleashed by <strong>Moore‚Äôs Law</strong>, showing how rapid technical change incentivized modular architectures and platform governance. These examples serve not just to illustrate the theory, but to demonstrate its explanatory power across a wide range of industries and technological domains.</p>
</section>
</section>
<section id="strengths-and-impact" class="level3">
<h3 class="anchored" data-anchor-id="strengths-and-impact">Strengths and impact</h3>
<p>One of the book‚Äôs greatest strengths is its theoretical originality. Baldwin brings together insights from engineering design, economics, organizational theory, and innovation studies to craft a truly interdisciplinary framework. Her use of concepts like value structure maps and the spectrum of complementarity is both novel and methodologically robust, offering readers tools to think more precisely about the relationship between technology and organizational form.</p>
<p>Equally impressive is the book‚Äôs empirical rigor. Each theoretical idea is grounded in detailed case studies spanning multiple industries and historical periods‚Äîfrom the rise of mass production in the early 20th century to the modern dominance of digital platforms and open-source ecosystems. Baldwin‚Äôs treatment of these examples is comprehensive, deeply researched, and intellectually generous, allowing the reader to see how abstract models play out in the real world.</p>
<p>Despite the complexity of its subject matter, the book maintains a remarkable degree of conceptual clarity. Technical ideas‚Äîsuch as design structure matrices (DSMs), modularity, or transaction cost placement‚Äîare illustrated through intuitive diagrams and historical analogies that make the material accessible even to those outside the academic core of the field. This clarity, combined with Baldwin‚Äôs forward-looking perspective, makes the book especially relevant for anyone trying to understand contemporary shifts in how value is created and captured in an era of rapid technological change.</p>
</section>
<section id="challenges-and-limitations" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and limitations</h3>
<p>That said, Design Rules, Volume 2 is not a light read. It carries a high cognitive load, packed with formal reasoning, abstract models, and layered argumentation. Readers unfamiliar with the foundations of organizational theory or economic modeling may need to invest time and effort to fully absorb its arguments.</p>
<p>Additionally, while the book offers a powerful framework, its case studies and examples are primarily drawn from technology-intensive sectors, especially computing, electronics, and software. As such, the theory‚Äôs direct applicability to more traditional or service-based industries may require further adaptation. Finally, although the book stands alone as a major contribution, readers who have not engaged with Design Rules, Volume 1 may occasionally feel they are missing important conceptual background‚Äîespecially regarding the original treatment of modularity.</p>
</section>
<section id="who-should-read-this" class="level3">
<h3 class="anchored" data-anchor-id="who-should-read-this">Who should read this?</h3>
<p>Design Rules, Volume 2 is ideally suited for researchers and graduate students in fields such as organizational theory, innovation management, and platform economics. It is also a valuable resource for technology strategists and digital transformation leaders seeking to align business strategy with technical architecture. Policy makers working on industrial policy and digital governance will find it useful in understanding the structural underpinnings of modern ecosystems. Finally, systems engineers and enterprise architects will benefit from its deep insights into the interplay between design decisions and organizational constraints.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p><em>Design Rules, Volume 2</em> is a landmark work‚Äîambitious in scope, rigorous in method, and deeply relevant to understanding the shifting terrain of technology and organization in the 21st century. Carliss Y. Baldwin does not merely build on the foundations laid in Volume 1; she expands the intellectual architecture to accommodate a more complex, dynamic world‚Äîone where platforms, ecosystems, and distributed forms of collaboration are rapidly supplanting the vertically integrated structures of the past.</p>
<p>What sets this book apart is not only its explanatory power but its <strong>unifying vision</strong>. Baldwin succeeds in weaving together disparate fields‚Äîeconomics, engineering, organizational design, strategy‚Äîinto a coherent framework that makes sense of how technologies evolve and how they reshape the organizational landscape around them. Her concepts, particularly the <strong>spectrum of complementarity</strong> and <strong>value structure analysis</strong>, are not only insightful but immediately applicable. They offer a way of seeing and reasoning about organizations that transcends conventional business school wisdom.</p>
<p>This is not just a book for theorists. For practitioners‚Äîparticularly those leading digital transformation efforts, architecting platform strategies, or navigating the rise of open-source and remote collaboration‚Äî<em>Design Rules, Volume 2</em> offers a vocabulary and logic for understanding what works, what doesn‚Äôt, and why. It provides a strategic lens through which to evaluate not just individual decisions, but the overall coherence between an organization‚Äôs technological infrastructure and its structure, governance, and long-term viability.</p>
<p>That said, it is also a demanding book. Readers without prior exposure to Volume 1 or to the literature on modularity, platform strategy, or organizational economics may find the intellectual terrain challenging. But for those willing to invest the time, the <strong>intellectual return is substantial</strong>.</p>
<p>In the end, <em>Design Rules, Volume 2</em> doesn‚Äôt just describe how technology shapes organizations‚Äîit shows us how to <strong>design organizations for a technological world</strong>. It will likely stand as a foundational reference for years to come, not only in academia but in boardrooms, design studios, and strategy workshops wherever technology meets enterprise.</p>
</section>
<section id="further-readings" class="level3">
<h3 class="anchored" data-anchor-id="further-readings">Further readings</h3>
<p>Core foundations:</p>
<ul>
<li><p>Baldwin, C. Y. (2023). Design rules: Past and future. <em>Industrial and Corporate Change, 32</em>(1), 11‚Äì27. <a href="https://doi.org/10.1093/icc/dtac055">DOI</a></p></li>
<li><p>Baldwin, C. Y., &amp; Clark, K. B. (2000). <em>Design rules, volume 1: The power of modularity</em>. MIT Press. <a href="https://mitpress.mit.edu/9780262024662/design-rules-vol-1/">Link</a></p></li>
<li><p>Simon, H. A. (1962). The architecture of complexity. <em>Proceedings of the American Philosophical Society</em>, <em>106</em>(6), 467‚Äì482. <a href="https://www.jstor.org/stable/985254">Link</a></p></li>
</ul>
<p>Organizational theory and economics:</p>
<ul>
<li><p>Williamson, O. E. (1985). <em>The economic institutions of capitalism: Firms, markets, relational contracting</em>. Free Press. ISBN: 002934820X, 9780029348208</p></li>
<li><p>Milgrom, P., &amp; Roberts, J. (1992). <em>Economics, organization and management</em>. Prentice Hall. ISBN: 9780132239677</p></li>
<li><p>Nelson, R. R., &amp; Winter, S. G. (1982). <em>An evolutionary theory of economic change</em>. Harvard University Press. <a href="https://www.hup.harvard.edu/books/9780674272286">Link</a></p></li>
</ul>
<p>Platforms and ecosystems:</p>
<ul>
<li><p>Cusumano, M. A., Gawer, A., &amp; Yoffie, D. B. (2019). <em>The business of platforms: Strategy in the age of digital competition, innovation, and power</em>. Harper Business. <a href="https://www.harpercollins.com/products/the-business-of-platforms-michael-a-cusumanoannabelle-gawerdavid-b-yoffie">Link</a></p></li>
<li><p>Jacobides, M. G., Cennamo, C., &amp; Gawer, A. (2018). Towards a theory of ecosystems. <em>Strategic Management Journal</em>, <em>39</em>(8), 2255‚Äì2276. <a href="https://doi.org/10.1002/smj.2904">DOI</a></p></li>
</ul>
<p>Software and Open Source</p>
<ul>
<li><p>von Hippel, E. (2005). <em>Democratizing innovation</em>. MIT Press. <a href="https://direct.mit.edu/books/book/2821/Democratizing-Innovation">Link</a></p></li>
<li><p>Raymond, E. S. (2001). <em>The cathedral and the bazaar: Musings on Linux and open source by an accidental revolutionary</em>. O‚ÄôReilly Media. <a href="https://www.oreilly.com/library/view/the-cathedral-the/0596001088/">Link</a></p></li>
</ul>
</section>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td><h1 id="subject">Subject</h1>
<p><strong>Title</strong></p></td>
<td><h1 id="content">Content</h1>
<p>Design Rules, Volume 2: How Technology Shapes Organization</p></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Year</strong></td>
<td>2024</td>
<td></td>
</tr>
<tr class="even">
<td><strong>Author</strong></td>
<td><a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=6417">Carliss Y. Baldwin</a></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Language</strong></td>
<td>English</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Topics</strong></td>
<td>Modularity, Organizational design, Technology strategy, Platform ecosystems, Value networks</td>
<td></td>
</tr>
<tr class="even">
<td><strong>Downloads</strong></td>
<td><a href="https://direct.mit.edu/books/book-pdf/2487108/book_9780262380232.pdf">PDF | CC BY-NC-ND 4.0</a></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Other links</strong></td>
<td><a href="https://direct.mit.edu/books/oa-monograph/5887/Design-Rules-Volume-2How-Technology-Shapes">Publisher book page</a></td>
<td></td>
</tr>
<tr class="even">
<td><strong>ISBN/DOI</strong></td>
<td>9780262380232</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Buy online</strong></td>
<td><a href="https://mitpress.mit.edu/9780262049337/design-rules-volume-2/">MIT Press</a></td>
<td></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>enterprise architecture</category>
  <category>üá¨üáß</category>
  <guid>https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/</guid>
  <pubDate>Fri, 28 Mar 2025 23:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/m_9780262380232-cover.jpeg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
