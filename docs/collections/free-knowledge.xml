<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Bits of Knowledge</title>
<link>https://antomon.github.io/collections/free-knowledge.html</link>
<atom:link href="https://antomon.github.io/collections/free-knowledge.xml" rel="self" type="application/rss+xml"/>
<description>Ever wanted to really get a subject‚Äînot just skim the surface, but dive deep and come out feeling like an expert? Welcome to Free Knowledge, where we gather the best freely available resources to help you master new ideas, whether they come from top academics, self-taught geniuses, or brilliant weirdos with a knack for explaining things. From full-length books that unravel complex ideas to university-grade video courses that turn your screen time into real learning, from thought-provoking essays that cut straight to the core to documentaries and podcasts that make knowledge entertaining, everything here is curated to help you go beyond trivia and develop real understanding. No paywalls, no fluff‚Äîjust the best knowledge out there, waiting for curious minds like yours to dig in. So grab a book, queue up a lecture, and get ready to level up your brain.</description>
<generator>quarto-1.7.27</generator>
<lastBuildDate>Fri, 18 Apr 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Dimitri P. Bertsekas‚Äô <strong>A Course in Reinforcement Learning</strong> (2nd Edition) is a deeply insightful and uniquely structured textbook that bridges the gap between classical optimization and the modern field of reinforcement learning (RL). Unlike most RL books that emerge from the machine learning community and focus heavily on empirical performance and implementation, Bertsekas approaches the subject from the lens of <strong>dynamic programming (DP)</strong>, <strong>control theory</strong>, and <strong>operations research</strong>, offering a mathematically grounded, algorithmically rigorous, and conceptually unified perspective.</p>
<section id="conceptual-focus-value-centric-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-focus-value-centric-reinforcement-learning">Conceptual focus: value-centric reinforcement learning</h3>
<p>The core philosophy of the book revolves around <strong>approximation in value space</strong>‚Äîa concept that plays a foundational role in dynamic programming and is extended here to RL settings. This is in contrast to the increasingly popular <strong>policy-gradient</strong> methods that dominate deep RL literature. Rather than focusing solely on training end-to-end policies with neural networks, Bertsekas places heavy emphasis on computing or approximating <strong>value functions</strong>, and using those to derive or improve policies.</p>
<p>This is a natural continuation of the ideas developed in his seminal works on <strong>Dynamic Programming and Optimal Control</strong>. It also reflects the underlying architecture of high-performing RL systems such as <strong>AlphaZero</strong> and <strong>TD-Gammon</strong>, both of which use trained evaluators in conjunction with powerful online search or rollout techniques. In fact, one of the book‚Äôs major contributions is to formalize and explain these empirical successes within a Newton-like optimization framework for solving Bellman‚Äôs equation.</p>
</section>
<section id="structure-and-content-overview" class="level3">
<h3 class="anchored" data-anchor-id="structure-and-content-overview">Structure and content overview</h3>
<p>The book is divided into <strong>three major chapters</strong>, designed to support both linear reading and modular, instructor-tailored course planning:</p>
<section id="chapter-1-exact-and-approximate-dynamic-programming" class="level4">
<h4 class="anchored" data-anchor-id="chapter-1-exact-and-approximate-dynamic-programming">Chapter 1: exact and approximate dynamic programming</h4>
<p>This foundational chapter provides a broad overview of the RL landscape, rooted in dynamic programming. It begins with deterministic and stochastic finite-horizon problems, develops into approximation methods, and presents the conceptual framework of <strong>offline training + online play</strong>, drawing detailed analogies with AlphaZero and model predictive control (MPC). It also introduces the reader to concepts like <strong>rollout</strong>, <strong>policy iteration</strong>, <strong>cost-to-go approximations</strong>, and <strong>terminal cost evaluation</strong>. The chapter is comprehensive and forms a standalone platform for readers unfamiliar with RL but versed in optimization.</p>
</section>
<section id="chapter-2-approximation-in-value-space-rollout-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="chapter-2-approximation-in-value-space-rollout-algorithms">Chapter 2: approximation in value space ‚Äì rollout algorithms</h4>
<p>Here, Bertsekas deepens the discussion on value function-based methods. He details variants of <strong>rollout algorithms</strong>‚Äîmethods that combine a base heuristic with lookahead or simulation to improve decisions in real-time. Topics include constrained optimization, Monte Carlo Tree Search (MCTS), randomized rollout, Bayesian optimization, and their application to deterministic and stochastic control, POMDPs, and even adversarial games. This chapter shows the versatility of the rollout paradigm across discrete and continuous domains.</p>
</section>
<section id="chapter-3-learning-values-and-policies" class="level4">
<h4 class="anchored" data-anchor-id="chapter-3-learning-values-and-policies">Chapter 3: learning values and policies</h4>
<p>This is where the book engages with <strong>neural networks</strong> and other parametric function approximators. It examines how value functions and policies can be learned from data using <strong>fitted value iteration</strong>, <strong>Q-learning</strong>, <strong>SARSA</strong>, and <strong>policy gradient methods</strong>. While coverage of policy optimization is relatively lean compared to deep RL books like Sutton &amp; Barto‚Äôs <em>Reinforcement Learning</em>, it is sufficient to understand core ideas and their integration into the Newton-based value approximation framework. The focus remains on conceptual clarity and practical convergence issues.</p>
<p>Each chapter ends with <strong>detailed notes, sources, and exercises</strong>, often pointing to supplementary material in Bertsekas‚Äô other books, such as <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em> (2020) or <em>Lessons from AlphaZero</em> (2022). These references make the book an excellent launchpad for research or deeper specialization.</p>
</section>
</section>
<section id="highlights-and-unique-strengths" class="level3">
<h3 class="anchored" data-anchor-id="highlights-and-unique-strengths">Highlights and unique strengths</h3>
<ul>
<li>Grounded in <strong>decades of optimization theory</strong>, yet deeply aware of recent breakthroughs like AlphaZero.</li>
<li>Clarifies the synergy between <strong>offline learning</strong> and <strong>online planning</strong>, rather than viewing training as a monolithic task.</li>
<li>A rare textbook that can <strong>speak fluently across disciplines</strong>: control theory, artificial intelligence, operations research.</li>
<li>Emphasizes <strong>rigorous intuition</strong> and <strong>geometric/visual understanding</strong> over dense formalism.</li>
<li>Modular and customizable for different courses: short introductory tracks or deep RL theory paths.</li>
</ul>
</section>
<section id="who-is-this-book-for" class="level3">
<h3 class="anchored" data-anchor-id="who-is-this-book-for">Who is this book for?</h3>
<p>This textbook is not for everyone‚Äîand that‚Äôs a strength, not a weakness.</p>
<ul>
<li>Ideal readers include:
<ul>
<li>Graduate students in <strong>electrical engineering</strong>, <strong>applied mathematics</strong>, <strong>operations research</strong>, or <strong>control systems</strong>.</li>
<li>Researchers and professionals interested in <strong>optimization</strong>, <strong>model predictive control</strong>, <strong>robotics</strong>, or <strong>multiagent systems</strong>.</li>
<li>Advanced undergraduates with a solid mathematical background and interest in decision-making under uncertainty.</li>
<li>Practitioners who have experience with algorithms and modeling, and want to understand the ‚Äúwhy‚Äù behind reinforcement learning, not just the ‚Äúhow‚Äù.</li>
</ul></li>
<li>Less ideal for:
<ul>
<li>Beginners looking for an introductory, code-heavy book. For this, <em>Reinforcement Learning: An Introduction</em> by Sutton &amp; Barto or <em>Deep Reinforcement Learning Hands-On</em> by Maxim Lapan may be more approachable.</li>
<li>Readers wanting a focus on PyTorch/TensorFlow, environment setups, or software engineering best practices for RL pipelines.</li>
</ul></li>
</ul>
<p>Instead, Bertsekas offers something increasingly rare: a textbook that <strong>doesn‚Äôt treat reinforcement learning as a subfield of deep learning</strong>, but rather as a <strong>mathematically grounded discipline</strong> that can integrate with‚Äîbut also stand apart from‚Äîmodern AI trends.</p>
</section>
<section id="supplementary-materials" class="level3">
<h3 class="anchored" data-anchor-id="supplementary-materials">Supplementary materials</h3>
<p>The book is accompanied by <strong>video lectures, slides</strong>, and supplemental content from Bertsekas‚Äô Arizona State University course, accessible via his website. This makes the book particularly useful for <strong>self-study</strong>, especially for learners who appreciate visual explanation and conceptual repetition across formats.</p>
</section>
<section id="final-verdict" class="level3">
<h3 class="anchored" data-anchor-id="final-verdict">Final verdict</h3>
<p><em>A Course in Reinforcement Learning (2nd Edition)</em> is a rigorous, insightful, and conceptually rich text that helps readers <strong>understand reinforcement learning as a structured decision-making framework</strong>, not just a toolbox of tricks. If you‚Äôre serious about applying RL to real-world, high-stakes systems‚Äîwhere stability, interpretability, and theoretical guarantees matter‚Äîthis is the book you‚Äôve been looking for.</p>
<p>It challenges, rewards, and broadens the reader‚Äôs view of what reinforcement learning is and what it can be. Highly recommended for those looking to <strong>build durable understanding</strong>, not just quick implementations.</p>
</section>
</section>
<section id="about-the-author" class="level2">
<h2 class="anchored" data-anchor-id="about-the-author">About the author</h2>
<p>Dimitri P. Bertsekas is a world-renowned scholar in optimization and control theory. He received his Ph.D.&nbsp;in system science from MIT and has held faculty positions at Stanford, the University of Illinois, and MIT, where he remains McAfee Professor of Engineering. Since 2019, he has been Fulton Professor of Computational Decision Making at Arizona State University.</p>
<p>Over a prolific academic career, Bertsekas has authored over twenty influential books covering topics from nonlinear programming to data networks, and notably, dynamic programming and reinforcement learning. His contributions have earned him numerous accolades, including the IEEE Control Systems Award, INFORMS John von Neumann Theory Prize, and election to the U.S. National Academy of Engineering. His recent focus on RL reflects decades of foundational work in dynamic programming and optimization, making him a key figure in bridging classical control with modern machine learning.</p>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>A Course in Reinforcement Learning</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025 (2nd edition)</td>
</tr>
<tr class="odd">
<td><strong>Authors</strong></td>
<td>Dimitri P. Bertsekas</td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="http://www.athenasc.com/">Athena Scientific</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Reinforcement learning, Model predictive control, Dynamics programming, Machine learning</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE%202ndEDITION.pdf">Book PDF</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLbook.html">Course videolectures and materials</a></td>
</tr>
<tr class="odd">
<td><strong>ISBN/DOI</strong></td>
<td>1-886529-29-9</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>üá¨üáß</category>
  <guid>https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/</guid>
  <pubDate>Fri, 18 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Differential Privacy - Simson L. Garfinkel</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/m_9780262382168-cover.jpeg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>This monograph provides a non-mathematical yet rigorous conceptual introduction to the theory and application of <strong>differential privacy (DP)</strong>. It is part of MIT Press‚Äôs <em>Essential Knowledge</em> series and is explicitly written for a <strong>general audience</strong>, without requiring prior familiarity with statistics, mathematics, or computer science.</p>
<p>Rather than offering algorithmic recipes or formal proofs, the book aims to: - Build <strong>intuitive understanding</strong> of the DP paradigm. - Trace the <strong>historical development</strong> of the field. - Contextualize DP through <strong>real-world implementations</strong> (notably the 2020 U.S. Census). - Examine the <strong>philosophical and policy implications</strong> of deploying DP systems.</p>
<section id="content-summary" class="level3">
<h3 class="anchored" data-anchor-id="content-summary">Content summary</h3>
<section id="conceptual-foundations" class="level4">
<h4 class="anchored" data-anchor-id="conceptual-foundations">Conceptual foundations</h4>
<p>Garfinkel introduces differential privacy as a <strong>mathematically formal approach to privacy protection</strong>, contrasted with heuristic or ‚Äúbest effort‚Äù techniques such as de-identification and data masking.</p>
<p>Key theoretical components are explained using analogies: - <strong>Sensitivity</strong> and the use of <strong>calibrated noise</strong> via the Laplace or Gaussian mechanisms. - The <strong>privacy loss budget</strong> (Œµ) and the implications of tuning it. - <strong>Composability</strong> and how DP offers provable guarantees even under repeated queries. - The <strong>mosaic effect</strong>, demonstrating how auxiliary data enables reidentification even from aggregated or anonymized data.</p>
<p>Although mathematical notation is absent, the conceptual treatment aligns with the formal definitions of (Œµ, Œ¥)-differential privacy as used in foundational literature.</p>
</section>
<section id="historical-and-institutional-context" class="level4">
<h4 class="anchored" data-anchor-id="historical-and-institutional-context">Historical and institutional context</h4>
<p>The author documents the trajectory of DP from its introduction by Dwork, McSherry, Nissim, and Smith (2006) to its adoption in high-stakes settings such as: - The U.S. Census Bureau‚Äôs <strong>Disclosure Avoidance System</strong>. - Google‚Äôs and Apple‚Äôs telemetry systems. - NIST‚Äôs evolving standards for privacy-enhancing technologies (e.g., SP 800-188).</p>
<p>The tension between <strong>statistical utility and privacy guarantees</strong> is explored through case studies, such as the controversy surrounding the use of DP in the 2020 Census.</p>
</section>
<section id="critique-and-alternatives" class="level4">
<h4 class="anchored" data-anchor-id="critique-and-alternatives">Critique and Alternatives</h4>
<p>Garfinkel acknowledges the limitations of DP: - Not suitable for <strong>non-tabular data</strong> (e.g., video, audio). - Noise injection can <strong>degrade data utility</strong>, especially for small subpopulations. - DP is often <strong>challenging to tune</strong>, and the setting of Œµ is as much a <strong>policy decision</strong> as a technical one.</p>
<p>He contrasts DP with legacy disclosure limitation methods (e.g., k-anonymity, cell suppression, top-coding), arguing that while they lack formal guarantees, they may provide better practical utility in some scenarios.</p>
<p>Strengths: - <strong>Clarity of exposition</strong>: Abstract concepts are explained using accessible examples (e.g., student grades, public statistics). - <strong>Policy relevance</strong>: The author‚Äôs experience at NIST and the Census Bureau enables an authoritative treatment of institutional use cases. - <strong>Balanced perspective</strong>: Both the promises and criticisms of DP are articulated, with references to academic and legal debates (e.g., lawsuits surrounding the 2020 Census).</p>
<p>Limitations: - <strong>Lack of formalism</strong>: No mathematical definitions, algorithms, or implementation details are presented. - <strong>Limited scope of mechanisms</strong>: The book focuses almost exclusively on the <strong>central differential privacy model</strong>, with minimal treatment of <strong>local DP</strong>, <strong>privacy accounting methods</strong> (e.g., R√©nyi DP), or advanced techniques like <strong>privacy amplification</strong>. - <strong>No discussion of implementation frameworks</strong>: Libraries like Google‚Äôs DP library, OpenDP, and SmartNoise are only briefly mentioned in the notes, with no comparative discussion.</p>
</section>
</section>
<section id="related-works" class="level3">
<h3 class="anchored" data-anchor-id="related-works">Related works</h3>
<p>The book is best read alongside more technical treatments, such as: - Dwork, C., &amp; Roth, A. (2014). <em>The algorithmic foundations of differential privacy</em>. Now Publishers. <a href="https://doi.org/10.1561/0400000042">DOI</a> - Gaboardi, M., Honaker, J., &amp; Stoddard, J. (2023). <em>Programming differential privacy</em>. O‚ÄôReilly Media. - National Institute of Standards and Technology. (2023). <em>De-identifying government datasets: Techniques and governance (NIST Special Publication 800-188)</em>. U.S. Department of Commerce. <a href="https://doi.org/10.6028/NIST.SP.800-188">DOI</a></p>
</section>
<section id="recommendation" class="level3">
<h3 class="anchored" data-anchor-id="recommendation">Recommendation</h3>
<p>This book is recommended as a policy- and systems-oriented primer for: - Privacy professionals and data protection officers. - Statisticians and social scientists new to formal privacy models. - Government and industry decision-makers evaluating privacy technologies.</p>
<p>It is <strong>not</strong> a substitute for a formal or computational introduction for practitioners seeking to implement DP mechanisms in production systems.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p>Simson L. Garfinkel‚Äôs <em>Differential Privacy</em> is an essential conceptual guide that democratizes access to one of the most profound ideas in data privacy. While limited in technical depth, it succeeds in making the case for DP‚Äôs significance and encourages broader adoption and informed critique.</p>
</section>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>Differential Privacy</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025</td>
</tr>
<tr class="odd">
<td><strong>Authors</strong></td>
<td><a href="https://simson.net/page/Main_Page">Simson L. Garfinkel</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Differential privacy, Statistical disclosure limitation, Privacy-preserving data analysis, Data governance</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf">PDF | CC BY-NC-ND 4.0</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://direct.mit.edu/books/book/5935/Differential-Privacy">Publisher book page</a></td>
</tr>
<tr class="odd">
<td><strong>ISBN/DOI</strong></td>
<td>9780262382168</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td><a href="https://mitpress.mit.edu/9780262551656/differential-privacy/">MIT Press</a></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>data privacy</category>
  <category>üá¨üáß</category>
  <guid>https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/</guid>
  <pubDate>Tue, 01 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/m_9780262382168-cover.jpeg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
