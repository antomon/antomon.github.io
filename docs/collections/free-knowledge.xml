<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Bits of Knowledge</title>
<link>https://antomon.github.io/collections/free-knowledge.html</link>
<atom:link href="https://antomon.github.io/collections/free-knowledge.xml" rel="self" type="application/rss+xml"/>
<description>Ever wanted to really get a subject‚Äînot just skim the surface, but dive deep and come out feeling like an expert? Welcome to Free Knowledge, where we gather the best freely available resources to help you master new ideas, whether they come from top academics, self-taught geniuses, or brilliant weirdos with a knack for explaining things. From full-length books that unravel complex ideas to university-grade video courses that turn your screen time into real learning, from thought-provoking essays that cut straight to the core to documentaries and podcasts that make knowledge entertaining, everything here is curated to help you go beyond trivia and develop real understanding. No paywalls, no fluff‚Äîjust the best knowledge out there, waiting for curious minds like yours to dig in. So grab a book, queue up a lecture, and get ready to level up your brain.</description>
<generator>quarto-1.7.27</generator>
<lastBuildDate>Fri, 18 Apr 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/bertsekas-reinforcement-learning-textbook/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/bertsekas-reinforcement-learning-textbook/Course-in-RL-2nd-ed.jpg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Dimitri P. Bertsekas‚Äô <strong>A Course in Reinforcement Learning</strong> (2nd Edition) is a deeply insightful and uniquely structured textbook that bridges the gap between classical optimization and the modern field of reinforcement learning (RL). Unlike most RL books that emerge from the machine learning community and focus heavily on empirical performance and implementation, Bertsekas approaches the subject from the lens of <strong>dynamic programming (DP)</strong>, <strong>control theory</strong>, and <strong>operations research</strong>, offering a mathematically grounded, algorithmically rigorous, and conceptually unified perspective.</p>
<section id="conceptual-focus-value-centric-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-focus-value-centric-reinforcement-learning">Conceptual focus: value-centric reinforcement learning</h3>
<p>The core philosophy of the book revolves around <strong>approximation in value space</strong>‚Äîa concept that plays a foundational role in dynamic programming and is extended here to RL settings. This is in contrast to the increasingly popular <strong>policy-gradient</strong> methods that dominate deep RL literature. Rather than focusing solely on training end-to-end policies with neural networks, Bertsekas places heavy emphasis on computing or approximating <strong>value functions</strong>, and using those to derive or improve policies.</p>
<p>This is a natural continuation of the ideas developed in his seminal works on <strong>Dynamic Programming and Optimal Control</strong>. It also reflects the underlying architecture of high-performing RL systems such as <strong>AlphaZero</strong> and <strong>TD-Gammon</strong>, both of which use trained evaluators in conjunction with powerful online search or rollout techniques. In fact, one of the book‚Äôs major contributions is to formalize and explain these empirical successes within a Newton-like optimization framework for solving Bellman‚Äôs equation.</p>
</section>
<section id="structure-and-content-overview" class="level3">
<h3 class="anchored" data-anchor-id="structure-and-content-overview">Structure and content overview</h3>
<p>The book is divided into <strong>three major chapters</strong>, designed to support both linear reading and modular, instructor-tailored course planning:</p>
<section id="chapter-1-exact-and-approximate-dynamic-programming" class="level4">
<h4 class="anchored" data-anchor-id="chapter-1-exact-and-approximate-dynamic-programming"><strong>Chapter 1: exact and approximate dynamic programming</strong></h4>
<p>This foundational chapter provides a broad overview of the RL landscape, rooted in dynamic programming. It begins with deterministic and stochastic finite-horizon problems, develops into approximation methods, and presents the conceptual framework of <strong>offline training + online play</strong>, drawing detailed analogies with AlphaZero and model predictive control (MPC). It also introduces the reader to concepts like <strong>rollout</strong>, <strong>policy iteration</strong>, <strong>cost-to-go approximations</strong>, and <strong>terminal cost evaluation</strong>. The chapter is comprehensive and forms a standalone platform for readers unfamiliar with RL but versed in optimization.</p>
</section>
<section id="chapter-2-approximation-in-value-space-rollout-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="chapter-2-approximation-in-value-space-rollout-algorithms"><strong>Chapter 2: approximation in value space ‚Äì rollout algorithms</strong></h4>
<p>Here, Bertsekas deepens the discussion on value function-based methods. He details variants of <strong>rollout algorithms</strong>‚Äîmethods that combine a base heuristic with lookahead or simulation to improve decisions in real-time. Topics include constrained optimization, Monte Carlo Tree Search (MCTS), randomized rollout, Bayesian optimization, and their application to deterministic and stochastic control, POMDPs, and even adversarial games. This chapter shows the versatility of the rollout paradigm across discrete and continuous domains.</p>
</section>
<section id="chapter-3-learning-values-and-policies" class="level4">
<h4 class="anchored" data-anchor-id="chapter-3-learning-values-and-policies"><strong>Chapter 3: learning values and policies</strong></h4>
<p>This is where the book engages with <strong>neural networks</strong> and other parametric function approximators. It examines how value functions and policies can be learned from data using <strong>fitted value iteration</strong>, <strong>Q-learning</strong>, <strong>SARSA</strong>, and <strong>policy gradient methods</strong>. While coverage of policy optimization is relatively lean compared to deep RL books like Sutton &amp; Barto‚Äôs <em>Reinforcement Learning</em>, it is sufficient to understand core ideas and their integration into the Newton-based value approximation framework. The focus remains on conceptual clarity and practical convergence issues.</p>
<p>Each chapter ends with <strong>detailed notes, sources, and exercises</strong>, often pointing to supplementary material in Bertsekas‚Äô other books, such as <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em> (2020) or <em>Lessons from AlphaZero</em> (2022). These references make the book an excellent launchpad for research or deeper specialization.</p>
</section>
</section>
<section id="highlights-and-unique-strengths" class="level3">
<h3 class="anchored" data-anchor-id="highlights-and-unique-strengths">Highlights and unique strengths**</h3>
<ul>
<li>Grounded in <strong>decades of optimization theory</strong>, yet deeply aware of recent breakthroughs like AlphaZero.</li>
<li>Clarifies the synergy between <strong>offline learning</strong> and <strong>online planning</strong>, rather than viewing training as a monolithic task.</li>
<li>A rare textbook that can <strong>speak fluently across disciplines</strong>: control theory, artificial intelligence, operations research.</li>
<li>Emphasizes <strong>rigorous intuition</strong> and <strong>geometric/visual understanding</strong> over dense formalism.</li>
<li>Modular and customizable for different courses: short introductory tracks or deep RL theory paths.</li>
</ul>
</section>
<section id="who-is-this-book-for" class="level3">
<h3 class="anchored" data-anchor-id="who-is-this-book-for">Who is this book for?</h3>
<p>This textbook is not for everyone‚Äîand that‚Äôs a strength, not a weakness.</p>
<ul>
<li>Ideal readers include:
<ul>
<li>Graduate students in <strong>electrical engineering</strong>, <strong>applied mathematics</strong>, <strong>operations research</strong>, or <strong>control systems</strong>.</li>
<li>Researchers and professionals interested in <strong>optimization</strong>, <strong>model predictive control</strong>, <strong>robotics</strong>, or <strong>multiagent systems</strong>.</li>
<li>Advanced undergraduates with a solid mathematical background and interest in decision-making under uncertainty.</li>
<li>Practitioners who have experience with algorithms and modeling, and want to understand the ‚Äúwhy‚Äù behind reinforcement learning, not just the ‚Äúhow‚Äù.</li>
</ul></li>
<li>Less ideal for:
<ul>
<li>Beginners looking for an introductory, code-heavy book. For this, <em>Reinforcement Learning: An Introduction</em> by Sutton &amp; Barto or <em>Deep Reinforcement Learning Hands-On</em> by Maxim Lapan may be more approachable.</li>
<li>Readers wanting a focus on PyTorch/TensorFlow, environment setups, or software engineering best practices for RL pipelines.</li>
</ul></li>
</ul>
<p>Instead, Bertsekas offers something increasingly rare: a textbook that <strong>doesn‚Äôt treat reinforcement learning as a subfield of deep learning</strong>, but rather as a <strong>mathematically grounded discipline</strong> that can integrate with‚Äîbut also stand apart from‚Äîmodern AI trends.</p>
</section>
<section id="supplementary-materials" class="level3">
<h3 class="anchored" data-anchor-id="supplementary-materials">Supplementary materials</h3>
<p>The book is accompanied by <strong>video lectures, slides</strong>, and supplemental content from Bertsekas‚Äô Arizona State University course, accessible via his website. This makes the book particularly useful for <strong>self-study</strong>, especially for learners who appreciate visual explanation and conceptual repetition across formats.</p>
</section>
<section id="final-verdict" class="level3">
<h3 class="anchored" data-anchor-id="final-verdict">Final verdict</h3>
<p><em>A Course in Reinforcement Learning (2nd Edition)</em> is a rigorous, insightful, and conceptually rich text that helps readers <strong>understand reinforcement learning as a structured decision-making framework</strong>, not just a toolbox of tricks. If you‚Äôre serious about applying RL to real-world, high-stakes systems‚Äîwhere stability, interpretability, and theoretical guarantees matter‚Äîthis is the book you‚Äôve been looking for.</p>
<p>It challenges, rewards, and broadens the reader‚Äôs view of what reinforcement learning is and what it can be. Highly recommended for those looking to <strong>build durable understanding</strong>, not just quick implementations.</p>
</section>
</section>
<section id="about-the-author" class="level2">
<h2 class="anchored" data-anchor-id="about-the-author">About the author</h2>
<p>Dimitri P. Bertsekas is a world-renowned scholar in optimization and control theory. He received his Ph.D.&nbsp;in system science from MIT and has held faculty positions at Stanford, the University of Illinois, and MIT, where he remains McAfee Professor of Engineering. Since 2019, he has been Fulton Professor of Computational Decision Making at Arizona State University.</p>
<p>Over a prolific academic career, Bertsekas has authored over twenty influential books covering topics from nonlinear programming to data networks, and notably, dynamic programming and reinforcement learning. His contributions have earned him numerous accolades, including the IEEE Control Systems Award, INFORMS John von Neumann Theory Prize, and election to the U.S. National Academy of Engineering. His recent focus on RL reflects decades of foundational work in dynamic programming and optimization, making him a key figure in bridging classical control with modern machine learning.</p>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<p><a href="https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE%202ndEDITION.pdf">Free textbook download PDF</a> <a href="https://web.mit.edu/dimitrib/www/RLbook.html">Course website</a></p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>üá¨üáß</category>
  <guid>https://antomon.github.io/collections/fkposts/bertsekas-reinforcement-learning-textbook/</guid>
  <pubDate>Fri, 18 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/bertsekas-reinforcement-learning-textbook/Course-in-RL-2nd-ed.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
