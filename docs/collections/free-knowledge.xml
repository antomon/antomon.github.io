<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Bits of Knowledge</title>
<link>https://antomon.github.io/collections/free-knowledge.html</link>
<atom:link href="https://antomon.github.io/collections/free-knowledge.xml" rel="self" type="application/rss+xml"/>
<description>Ever wanted to really get a subject—not just skim the surface, but dive deep and come out feeling like an expert? Welcome to Free Knowledge, where we gather the best freely available resources to help you master new ideas, whether they come from top academics, self-taught geniuses, or brilliant weirdos with a knack for explaining things. From full-length books that unravel complex ideas to university-grade video courses that turn your screen time into real learning, from thought-provoking essays that cut straight to the core to documentaries and podcasts that make knowledge entertaining, everything here is curated to help you go beyond trivia and develop real understanding. No paywalls, no fluff—just the best knowledge out there, waiting for curious minds like yours to dig in. So grab a book, queue up a lecture, and get ready to level up your brain.</description>
<generator>quarto-1.7.33</generator>
<lastBuildDate>Wed, 23 Apr 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>Machine Learning in Production: From Models to Products - Christian Kästner</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/kästner-machine-learning-production/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/kästner-machine-learning-production/9780262049726.avif" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Christian Kästner’s <em>Machine Learning in Production: From Models to Products</em> is an ambitious and deeply considered contribution to the emerging discipline of machine learning systems engineering. It is not a programming book, nor is it a guide to training better models. Rather, it is an engineering textbook for practitioners and researchers interested in the complex and often overlooked journey of turning machine-learned models into fully-fledged, reliable, maintainable, and ethical software systems deployed in production environments.</p>
<p>The book opens by confronting an uncomfortable reality in the ML community: the vast majority of machine learning models never make it into production. Estimates cited in the opening chapter suggest that nearly 87% of ML projects fail to deliver usable results. The reasons for this failure are multifaceted—ranging from fragile infrastructure and inadequate testing, to insufficient cross-disciplinary collaboration and poorly understood business requirements. Kästner addresses this failure head-on, offering the book as both a corrective to narrow model-centric thinking and a roadmap for those who want to build systems that actually work.</p>
<p><strong>Prerequisites</strong>:</p>
<ul>
<li>Familiarity with machine learning concepts (e.g., training, testing, overfitting, common algorithms).</li>
<li>Experience in software engineering or systems development is helpful but not required.</li>
</ul>
<p><strong>Reader Level</strong>:</p>
<ul>
<li>Intermediate to advanced practitioners.</li>
<li>Appropriate for graduate students, engineers, and technical product managers.</li>
</ul>
<section id="conceptual-framework" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-framework">Conceptual framework</h3>
<p>The central thesis of the book is that machine learning is not a solution in itself—it is a component of a system. This perspective shifts the reader’s attention from optimizing models in isolation to integrating them into software products that must be usable, testable, secure, interpretable, and resilient in real-world contexts. Kästner emphasizes a <strong>systems mindset</strong>, rooted in decades of software engineering research, as a way to manage the inherent uncertainties and risks of ML deployments.</p>
<p>What distinguishes this book is not only its emphasis on engineering principles, but its persistent grounding in <strong>interdisciplinary collaboration</strong>. Kästner argues that successful ML products require deep collaboration between data scientists, software engineers, DevOps specialists, UX designers, product managers, and often legal and ethical advisors. The book repeatedly returns to the theme that the human and organizational dimensions of machine learning systems are just as critical as their technical foundations.</p>
</section>
<section id="topics-covered" class="level3">
<h3 class="anchored" data-anchor-id="topics-covered">Topics covered</h3>
<p>The book offers a comprehensive, systems-oriented view of how to bring machine learning into production—not as an isolated technical milestone, but as a deeply collaborative, architectural, and ethical endeavor. Structured into <strong>29 chapters across six thematic parts</strong>, <em>Machine Learning in Production</em> methodically walks the reader through the entire <strong>lifecycle of a machine learning product</strong>, from initial scoping to post-deployment accountability.</p>
<p>It begins by laying a solid foundation in <strong>systems thinking</strong>, clarifying how ML models function as components within larger software ecosystems. Early chapters address the essential question of <strong>when to use machine learning</strong>, and how to <strong>formulate goals and requirements</strong> that reflect both technical feasibility and organizational objectives. Rather than assuming that a model is the product, Kästner reframes ML as one piece of a broader sociotechnical system.</p>
<p>The next sections delve into <strong>architecture and design</strong>, showing how to translate high-level goals into resilient, modular, and scalable infrastructure. Chapters cover quality attributes unique to ML components—such as model stability and drift resistance—as well as standard engineering concerns like latency, cost, and maintainability. Readers are equipped to design deployment strategies (cloud, edge, batch, real-time), automate pipelines, and plan for growth. Kästner introduces <strong>MLOps</strong> not as a set of tools, but as a discipline focused on <strong>reproducibility</strong>, <strong>observability</strong>, and <strong>iterative improvement</strong>. While tools like MLflow, TFX, and Airflow are mentioned, the emphasis remains on principles and trade-offs, making the discussion accessible and vendor-neutral.</p>
<p>A major portion of the book is devoted to <strong>quality assurance across the stack</strong>—from unit testing ML pipelines to evaluating model fairness and running safe experiments in production. Readers learn to assess model behavior using behavioral testing, subgroup analysis, and online testing, while simultaneously monitoring data pipelines and system boundaries for breakage or drift.</p>
<p>In the final sections, Kästner turns to <strong>processes, team collaboration, and responsible engineering</strong>. These chapters are where the book’s ethical commitments come to the fore. Topics like <strong>fairness</strong>, <strong>explainability</strong>, <strong>security</strong>, <strong>privacy</strong>, and <strong>transparency</strong> are treated not as afterthoughts, but as <strong>design constraints</strong> as important as latency or accuracy. Kästner offers concrete practices for incorporating value-sensitive design, auditing, and accountability into both technical and organizational workflows. The final chapters on versioning, technical debt, and ethical review provide a roadmap for <strong>long-term sustainability</strong>, both in code and in culture.</p>
<p>Throughout, the book is anchored by a <strong>running case study</strong> of a startup deploying a speech transcription system powered by domain-adapted neural models. This example, rich in evolving technical and organizational complexity, recurs across chapters to illustrate how teams confront issues like data drift, deployment bottlenecks, regulatory concerns, and user interface design. It serves as a pedagogical thread that makes abstract concerns concrete and highlights how every architectural and ethical decision shapes the product and its impact.</p>
<p>Ultimately, the book is not a manual for building models—it is a <strong>handbook for building machine learning systems</strong>: reliable, maintainable, collaborative, and ethically sound. It offers a blueprint for professionals who understand that the true challenge in ML is not achieving high test accuracy, but creating enduring systems that work responsibly in the real world. chapter, exposing new layers of difficulty and new demands on the engineering team, serving as a pedagogical through-line that makes the theory tangible.</p>
</section>
<section id="style-and-accessibility" class="level3">
<h3 class="anchored" data-anchor-id="style-and-accessibility">Style and accessibility</h3>
<p>The writing is precise, articulate, and assumes a technically literate audience. This is not a pop-science or executive summary-style book. It is aimed at <strong>practitioners, advanced students, and researchers</strong> who already understand how machine learning works and now want to understand how to use it effectively in the real world. It assumes familiarity with concepts such as training/validation/test splits, overfitting, regularization, and inference latency, though it revisits these ideas through the lens of product engineering rather than model optimization.</p>
<p>Kästner’s prose is analytical rather than prescriptive. Rather than giving “recipes,” he presents frameworks for reasoning about architectural choices, engineering trade-offs, and organizational process. As such, it is particularly valuable for readers in leadership or systems architecture roles, who must weigh competing objectives such as model accuracy, speed, interpretability, user experience, and business value.</p>
<p>Despite its conceptual nature, the book is highly practical. It offers actionable insights into questions such as:</p>
<ul>
<li>How should system requirements change when one component is a probabilistic black box?</li>
<li>What testing strategies are appropriate when outputs are non-deterministic?</li>
<li>How do you monitor an ML model in production when it evolves in response to user behavior?</li>
<li>How can developers responsibly design interactions around fallible models?</li>
</ul>
<p>The answers to these questions are grounded not only in ML theory or software engineering literature but in an integrated synthesis of both. Moreover, the book frequently references recent case studies and academic research, linking practice to cutting-edge thinking.</p>
</section>
<section id="comparison-with-other-works" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-other-works">Comparison with other works</h3>
<p>While there are other books in this space—such as <em>Designing Machine Learning Systems</em> by Chip Huyen or <em>Building Machine Learning Powered Applications</em> by Emmanuel Ameisen—<em>Machine Learning in Production</em> distinguishes itself with its breadth, depth, and intellectual rigor. Where Huyen’s book is more hands-on and tool-oriented, Kästner’s work is broader in scope and more focused on long-term sustainability, system design, and organizational structure. It is more abstract, but correspondingly more foundational.</p>
<p>Unlike more tactical guides, this book doesn’t aim to teach you a specific deployment platform or how to optimize model performance for Kaggle competitions. Instead, it aims to make you a better ML system designer—someone who can build products that succeed in production environments and can evolve responsibly over time.</p>
</section>
<section id="chapters" class="level3">
<h3 class="anchored" data-anchor-id="chapters">Chapters</h3>
<p>This section offers deeper insight into the first foundational chapters of the book, which together establish the conceptual framework for building ML-enabled systems that go beyond isolated model training. Each chapter builds on the last, transitioning the reader from broad motivation to concrete system and team design.</p>
<section id="part-i-setting-the-stage" class="level4">
<h4 class="anchored" data-anchor-id="part-i-setting-the-stage">Part I: Setting the Stage</h4>
<p>This section introduces the foundational mindset of the book: that machine learning should not be viewed in isolation but as a subsystem within a broader product and software environment. It defines the scope of the engineering challenges to come.</p>
<section id="chapter-1-introduction" class="level5">
<h5 class="anchored" data-anchor-id="chapter-1-introduction">Chapter 1 – Introduction</h5>
<p>Introduces the book’s core motivation: many ML projects fail not due to modeling deficiencies, but because engineering and operational concerns are neglected. Kästner presents the idea that building a usable, responsible, and maintainable system requires more than just training a good model. He lays out the structure of the book and previews themes such as interdisciplinary teamwork, system reliability, and ethical design.</p>
</section>
<section id="chapter-2-from-models-to-systems" class="level5">
<h5 class="anchored" data-anchor-id="chapter-2-from-models-to-systems">Chapter 2 – From Models to Systems</h5>
<p>Transitions the reader from model-centric thinking to system-centric thinking. Kästner explains how ML models must integrate into complex software environments and how this integration introduces new failure modes and architectural concerns. Concepts like latency, observability, and modularization are framed as essential to building stable systems.</p>
</section>
<section id="chapter-3-machine-learning-for-software-engineers-in-a-nutshell" class="level5">
<h5 class="anchored" data-anchor-id="chapter-3-machine-learning-for-software-engineers-in-a-nutshell">Chapter 3 – Machine Learning for Software Engineers, in a Nutshell</h5>
<p>Offers a compact but comprehensive introduction to machine learning for readers with a software engineering background. Covers the ML pipeline, types of models, loss functions, and training paradigms—primarily to help engineers understand what ML practitioners do and how it affects engineering decisions.</p>
</section>
</section>
<section id="part-ii-requirements-engineering" class="level4">
<h4 class="anchored" data-anchor-id="part-ii-requirements-engineering">Part II: Requirements Engineering</h4>
<p>This part focuses on how to approach ML-enabled systems from a product requirements standpoint—emphasizing the importance of goal-setting, risk analysis, and planning for uncertainty.</p>
<section id="chapter-4-when-to-use-machine-learning" class="level5">
<h5 class="anchored" data-anchor-id="chapter-4-when-to-use-machine-learning">Chapter 4 – When to Use Machine Learning</h5>
<p>Helps teams assess whether machine learning is even appropriate for a given problem. Encourages choosing simpler rule-based systems where possible and cautions against using ML when interpretability, traceability, or simplicity is more critical.</p>
</section>
<section id="chapter-5-setting-and-measuring-goals" class="level5">
<h5 class="anchored" data-anchor-id="chapter-5-setting-and-measuring-goals">Chapter 5 – Setting and Measuring Goals</h5>
<p>Focuses on defining product-level and system-level goals that guide ML development. Covers the dangers of optimizing for the wrong metric and how different stakeholders (users, engineers, product leads) interpret success differently.</p>
</section>
<section id="chapter-6-gathering-requirements" class="level5">
<h5 class="anchored" data-anchor-id="chapter-6-gathering-requirements">Chapter 6 – Gathering Requirements</h5>
<p>Presents methods for collecting and documenting requirements when working with uncertain or probabilistic components. Includes user stories, use cases, and documentation templates designed for ML teams.</p>
</section>
<section id="chapter-7-planning-for-mistakes" class="level5">
<h5 class="anchored" data-anchor-id="chapter-7-planning-for-mistakes">Chapter 7 – Planning for Mistakes</h5>
<p>Introduces fault-tolerant thinking for ML systems. Discusses graceful degradation, fallback strategies, and explicit acknowledgment of model failure as an engineering challenge. Encourages building user expectations and safeguards into the design.</p>
</section>
</section>
<section id="part-iii-architecture-and-design" class="level4">
<h4 class="anchored" data-anchor-id="part-iii-architecture-and-design">Part III: Architecture and Design</h4>
<p>This section focuses on system design and software architecture tailored to the challenges of integrating ML components into reliable production environments.</p>
<section id="chapter-8-thinking-like-a-software-architect" class="level5">
<h5 class="anchored" data-anchor-id="chapter-8-thinking-like-a-software-architect">Chapter 8 – Thinking Like a Software Architect</h5>
<p>Guides readers through architectural decision-making, design patterns, and technical trade-offs relevant to ML systems. Encourages modular design and separation of concerns to better support change, testing, and responsibility separation.</p>
</section>
<section id="chapter-9-quality-attributes-of-ml-components" class="level5">
<h5 class="anchored" data-anchor-id="chapter-9-quality-attributes-of-ml-components">Chapter 9 – Quality Attributes of ML Components</h5>
<p>Goes beyond accuracy to consider non-functional properties like latency, memory usage, robustness to drift, cost, and stability across retraining. These attributes influence deployment decisions and impact user experience.</p>
</section>
<section id="chapter-10-deploying-a-model" class="level5">
<h5 class="anchored" data-anchor-id="chapter-10-deploying-a-model">Chapter 10 – Deploying a Model</h5>
<p>Discusses deployment strategies for real-time and batch inference. Covers practical patterns like two-phase prediction and feature stores. Also explains version control and rollback mechanisms as essential to safe, iterative releases.</p>
</section>
<section id="chapter-11-automating-the-pipeline" class="level5">
<h5 class="anchored" data-anchor-id="chapter-11-automating-the-pipeline">Chapter 11 – Automating the Pipeline</h5>
<p>Presents design principles for building robust, maintainable ML pipelines. Emphasizes reproducibility, modularity, and automation. Warns against large, opaque scripts that entangle too many concerns and are prone to breaking on updates.</p>
</section>
<section id="chapter-12-scaling-the-system" class="level5">
<h5 class="anchored" data-anchor-id="chapter-12-scaling-the-system">Chapter 12 – Scaling the System</h5>
<p>Addresses scalability challenges in data ingestion, model training, and inference. Explores distributed training, asynchronous data flows, streaming architectures, and how to partition workloads in high-traffic environments.</p>
</section>
<section id="chapter-13-planning-for-operations" class="level5">
<h5 class="anchored" data-anchor-id="chapter-13-planning-for-operations">Chapter 13 – Planning for Operations</h5>
<p>Covers operational readiness for production ML. Introduces SLOs (Service-Level Objectives), CI/CD practices, and MLOps concepts such as infrastructure as code and monitoring-by-design.</p>
</section>
</section>
<section id="part-iv-quality-assurance" class="level4">
<h4 class="anchored" data-anchor-id="part-iv-quality-assurance">Part IV: Quality Assurance</h4>
<p>This section explores how to test and validate ML systems, accounting for their non-determinism and data-dependence. It introduces techniques for ensuring quality at both the component and system levels.</p>
<section id="chapter-14-quality-assurance-basics" class="level5">
<h5 class="anchored" data-anchor-id="chapter-14-quality-assurance-basics">Chapter 14 – Quality Assurance Basics</h5>
<p>Introduces core software testing principles and maps them to the unique needs of ML. Differentiates between testing models offline and validating them within production systems.</p>
</section>
<section id="chapter-15-model-quality" class="level5">
<h5 class="anchored" data-anchor-id="chapter-15-model-quality">Chapter 15 – Model Quality</h5>
<p>Examines model performance in depth: not just accuracy, but fairness, calibration, subgroup performance, and robustness to out-of-distribution inputs. Promotes continuous evaluation over time, not just one-time testing.</p>
</section>
<section id="chapter-16-data-quality" class="level5">
<h5 class="anchored" data-anchor-id="chapter-16-data-quality">Chapter 16 – Data Quality</h5>
<p>Emphasizes that good models require good data. Describes common data problems such as label errors, schema shifts, concept drift, and sampling bias. Recommends monitoring, validation rules, and better data documentation.</p>
</section>
<section id="chapter-17-pipeline-quality" class="level5">
<h5 class="anchored" data-anchor-id="chapter-17-pipeline-quality">Chapter 17 – Pipeline Quality</h5>
<p>Focuses on the reliability of the data pipelines themselves. Introduces integration tests, pipeline-specific monitoring, and techniques to ensure intermediate steps do not corrupt data or predictions.</p>
</section>
<section id="chapter-18-system-quality" class="level5">
<h5 class="anchored" data-anchor-id="chapter-18-system-quality">Chapter 18 – System Quality</h5>
<p>Looks at how ML models perform when integrated into complete systems. Introduces chaos testing, fault injection, and other resilience techniques to uncover hidden failure modes.</p>
</section>
<section id="chapter-19-testing-and-experimenting-in-production" class="level5">
<h5 class="anchored" data-anchor-id="chapter-19-testing-and-experimenting-in-production">Chapter 19 – Testing and Experimenting in Production</h5>
<p>Discusses experimentation practices like A/B testing and canary releases. Provides guidelines for testing safely with real users and discusses the ethical implications of experimentation.</p>
</section>
</section>
<section id="part-v-process-and-teams" class="level4">
<h4 class="anchored" data-anchor-id="part-v-process-and-teams">Part V: Process and Teams</h4>
<p>This part looks at the organizational and cultural dynamics of ML development, including process models and team collaboration.</p>
<section id="chapter-20-data-science-and-software-engineering-process-models" class="level5">
<h5 class="anchored" data-anchor-id="chapter-20-data-science-and-software-engineering-process-models">Chapter 20 – Data Science and Software Engineering Process Models</h5>
<p>Compares Agile, CRISP-DM, and DevOps in the context of ML projects. Advocates for iterative processes with integrated feedback loops between teams and across time.</p>
</section>
<section id="chapter-21-interdisciplinary-teams" class="level5">
<h5 class="anchored" data-anchor-id="chapter-21-interdisciplinary-teams">Chapter 21 – Interdisciplinary Teams</h5>
<p>Explores collaboration among data scientists, software engineers, product managers, and other stakeholders. Offers practical strategies for communication, documentation, and role boundaries.</p>
</section>
<section id="chapter-22-technical-debt" class="level5">
<h5 class="anchored" data-anchor-id="chapter-22-technical-debt">Chapter 22 – Technical Debt</h5>
<p>Describes technical debt specific to ML systems: from data entanglement and version mismatch to retraining costs and infrastructure complexity. Encourages documentation, modularity, and strategic refactoring.</p>
</section>
</section>
<section id="part-vi-responsible-ml-engineering" class="level4">
<h4 class="anchored" data-anchor-id="part-vi-responsible-ml-engineering">Part VI: Responsible ML Engineering</h4>
<p>This final section addresses ethics, accountability, and the broader societal impact of deploying ML systems. It advocates for embedding responsible design throughout the ML lifecycle.</p>
<section id="chapter-23-responsible-engineering" class="level5">
<h5 class="anchored" data-anchor-id="chapter-23-responsible-engineering">Chapter 23 – Responsible Engineering</h5>
<p>Introduces a design philosophy grounded in values such as fairness, transparency, and safety. Encourages engineers to treat ethics as a primary design consideration, not a regulatory afterthought.</p>
</section>
<section id="chapter-24-versioning-provenance-and-reproducibility" class="level5">
<h5 class="anchored" data-anchor-id="chapter-24-versioning-provenance-and-reproducibility">Chapter 24 – Versioning, Provenance, and Reproducibility</h5>
<p>Covers how to track and manage changes in models, datasets, and pipelines. Emphasizes metadata, lineage tracking, and tools for auditability.</p>
</section>
<section id="chapter-25-explainability" class="level5">
<h5 class="anchored" data-anchor-id="chapter-25-explainability">Chapter 25 – Explainability</h5>
<p>Discusses technical and UX approaches to model interpretability and explanation. Differentiates between explanations for developers, users, and regulators.</p>
</section>
<section id="chapter-26-fairness" class="level5">
<h5 class="anchored" data-anchor-id="chapter-26-fairness">Chapter 26 – Fairness</h5>
<p>Explores fairness definitions, auditing tools, and strategies for mitigating systemic bias. Includes discussion of disparate impact and trade-offs between fairness and accuracy.</p>
</section>
<section id="chapter-27-safety" class="level5">
<h5 class="anchored" data-anchor-id="chapter-27-safety">Chapter 27 – Safety</h5>
<p>Details how to evaluate risks in ML systems and design them to fail safely. Covers accident analysis, fallback strategies, and fail-stop mechanisms.</p>
</section>
<section id="chapter-28-security-and-privacy" class="level5">
<h5 class="anchored" data-anchor-id="chapter-28-security-and-privacy">Chapter 28 – Security and Privacy</h5>
<p>Addresses threats like adversarial inputs, model extraction, and data leakage. Suggests design patterns and technical defenses, including privacy-preserving computation.</p>
</section>
<section id="chapter-29-transparency-and-accountability" class="level5">
<h5 class="anchored" data-anchor-id="chapter-29-transparency-and-accountability">Chapter 29 – Transparency and Accountability</h5>
<p>Concludes with practices for external visibility and internal responsibility. Encourages ethical review processes, documentation standards, and public communication strategies.</p>
</section>
</section>
</section>
<section id="who-is-this-book-for" class="level3">
<h3 class="anchored" data-anchor-id="who-is-this-book-for">Who is this book for?</h3>
<p>This book is intended for:</p>
<ul>
<li>Data scientists who want to go beyond notebooks and develop production-ready solutions.</li>
<li>Software engineers integrating machine learning into complex systems.</li>
<li>ML engineers focused on MLOps and reliability.</li>
<li>Technical product managers and tech leads overseeing ML product development.</li>
<li>Graduate-level students in applied ML, software engineering, or systems architecture.</li>
<li>Startups or teams outside Big Tech that need guidance on deploying ML with limited resources.</li>
</ul>
<p>It is not suitable for complete beginners or for readers seeking hands-on tutorials or specific tool walkthroughs.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p><em>Machine Learning in Production</em> is a landmark contribution to the literature on applied machine learning. It fills the gap between research-oriented ML and production engineering. While it does not provide hands-on code or tool-specific tutorials, its strength lies in delivering a conceptual and strategic framework for designing ML-enabled products responsibly and at scale.</p>
<p>This book is best suited for engineers and scientists seeking depth in architecture, process, and system design for machine learning. It provides enduring knowledge that transcends any specific framework or API. Readers seeking tactical implementation knowledge may need to pair it with more hands-on material, but this book provides the mental architecture necessary to do so effectively.</p>
</section>
</section>
<section id="about-the-author" class="level2">
<h2 class="anchored" data-anchor-id="about-the-author">About the author</h2>
<p>Christian Kästner is a professor of Software Engineering at Carnegie Mellon University. His research spans systems engineering, variability, and human factors in software development. He is the creator of the CMU course “Machine Learning in Production,” which inspired this book. Kästner is known for bridging gaps between software engineering, data science, and systems thinking. All royalties from this book are donated to charity, underscoring his commitment to responsible technology.</p>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>Machine Learning in Production: From Models to Products</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025</td>
</tr>
<tr class="odd">
<td><strong>Author</strong></td>
<td><a href="https://www.cs.cmu.edu/~ckaestne/">Christian Kästner</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>ML systems, MLOps, Requirements engineering, Software architecture, Data pipelines, ML fairness, ML explainability</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><p><a href="https://mlip-cmu.github.io/book/">Online version | CC BY-NC-ND 4.0</a></p>
<p><a href="https://mlip-cmu.github.io/">Course homepage</a></p>
<p><a href="https://github.com/ckaestne/seaibib">Annotated bibliography</a></p></td>
</tr>
<tr class="odd">
<td><strong>ISBN</strong></td>
<td>978-0262049726</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td><a href="https://mitpress.mit.edu/9780262049726/machine-learning-in-production/">MIT Press</a></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>🇬🇧</category>
  <guid>https://antomon.github.io/collections/fkposts/kästner-machine-learning-production/</guid>
  <pubDate>Wed, 23 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/kästner-machine-learning-production/9780262049726.avif" medium="image" type="image/avif"/>
</item>
<item>
  <title>A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Dimitri P. Bertsekas’ <strong>A Course in Reinforcement Learning</strong> (2nd Edition) is a deeply insightful and uniquely structured textbook that bridges the gap between classical optimization and the modern field of reinforcement learning (RL). Unlike most RL books that emerge from the machine learning community and focus heavily on empirical performance and implementation, Bertsekas approaches the subject from the lens of <strong>dynamic programming (DP)</strong>, <strong>control theory</strong>, and <strong>operations research</strong>, offering a mathematically grounded, algorithmically rigorous, and conceptually unified perspective.</p>
<section id="conceptual-focus-value-centric-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-focus-value-centric-reinforcement-learning">Conceptual focus: value-centric reinforcement learning</h3>
<p>The core philosophy of the book revolves around <strong>approximation in value space</strong>—a concept that plays a foundational role in dynamic programming and is extended here to RL settings. This is in contrast to the increasingly popular <strong>policy-gradient</strong> methods that dominate deep RL literature. Rather than focusing solely on training end-to-end policies with neural networks, Bertsekas places heavy emphasis on computing or approximating <strong>value functions</strong>, and using those to derive or improve policies.</p>
<p>This is a natural continuation of the ideas developed in his seminal works on <strong>Dynamic Programming and Optimal Control</strong>. It also reflects the underlying architecture of high-performing RL systems such as <strong>AlphaZero</strong> and <strong>TD-Gammon</strong>, both of which use trained evaluators in conjunction with powerful online search or rollout techniques. In fact, one of the book’s major contributions is to formalize and explain these empirical successes within a Newton-like optimization framework for solving Bellman’s equation.</p>
</section>
<section id="structure-and-content-overview" class="level3">
<h3 class="anchored" data-anchor-id="structure-and-content-overview">Structure and content overview</h3>
<p>The book is divided into <strong>three major chapters</strong>, designed to support both linear reading and modular, instructor-tailored course planning:</p>
<section id="chapter-1-exact-and-approximate-dynamic-programming" class="level4">
<h4 class="anchored" data-anchor-id="chapter-1-exact-and-approximate-dynamic-programming">Chapter 1: exact and approximate dynamic programming</h4>
<p>This foundational chapter provides a broad overview of the RL landscape, rooted in dynamic programming. It begins with deterministic and stochastic finite-horizon problems, develops into approximation methods, and presents the conceptual framework of <strong>offline training + online play</strong>, drawing detailed analogies with AlphaZero and model predictive control (MPC). It also introduces the reader to concepts like <strong>rollout</strong>, <strong>policy iteration</strong>, <strong>cost-to-go approximations</strong>, and <strong>terminal cost evaluation</strong>. The chapter is comprehensive and forms a standalone platform for readers unfamiliar with RL but versed in optimization.</p>
</section>
<section id="chapter-2-approximation-in-value-space-rollout-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="chapter-2-approximation-in-value-space-rollout-algorithms">Chapter 2: approximation in value space – rollout algorithms</h4>
<p>Here, Bertsekas deepens the discussion on value function-based methods. He details variants of <strong>rollout algorithms</strong>—methods that combine a base heuristic with lookahead or simulation to improve decisions in real-time. Topics include constrained optimization, Monte Carlo Tree Search (MCTS), randomized rollout, Bayesian optimization, and their application to deterministic and stochastic control, POMDPs, and even adversarial games. This chapter shows the versatility of the rollout paradigm across discrete and continuous domains.</p>
</section>
<section id="chapter-3-learning-values-and-policies" class="level4">
<h4 class="anchored" data-anchor-id="chapter-3-learning-values-and-policies">Chapter 3: learning values and policies</h4>
<p>This is where the book engages with <strong>neural networks</strong> and other parametric function approximators. It examines how value functions and policies can be learned from data using <strong>fitted value iteration</strong>, <strong>Q-learning</strong>, <strong>SARSA</strong>, and <strong>policy gradient methods</strong>. While coverage of policy optimization is relatively lean compared to deep RL books like Sutton &amp; Barto’s <em>Reinforcement Learning</em>, it is sufficient to understand core ideas and their integration into the Newton-based value approximation framework. The focus remains on conceptual clarity and practical convergence issues.</p>
<p>Each chapter ends with <strong>detailed notes, sources, and exercises</strong>, often pointing to supplementary material in Bertsekas’ other books, such as <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em> (2020) or <em>Lessons from AlphaZero</em> (2022). These references make the book an excellent launchpad for research or deeper specialization.</p>
</section>
</section>
<section id="highlights-and-unique-strengths" class="level3">
<h3 class="anchored" data-anchor-id="highlights-and-unique-strengths">Highlights and unique strengths</h3>
<p>What sets <em>A Course in Reinforcement Learning</em> apart is its rare synthesis of intellectual depth, conceptual clarity, and practical relevance. Bertsekas leverages <strong>decades of foundational work in optimization theory and dynamic programming</strong>, crafting a perspective on RL that feels both timeless and sharply attuned to contemporary developments like AlphaZero. Rather than presenting RL as a bag of tricks or an empirical race to outperform benchmarks, the book offers a <strong>structured, principled framework</strong> for thinking about sequential decision-making under uncertainty.</p>
<p>A central strength lies in the <strong>elegant articulation of the synergy between offline learning and online planning</strong>. Instead of treating training as a static preprocessing step, Bertsekas shows how real-time control and model predictive strategies can interact meaningfully with learned approximations—a conceptual bridge that is both underexplored and urgently needed in modern RL discourse. This also gives the book a unique relevance to high-stakes engineering domains where stability, safety, and interpretability matter.</p>
<p>Another distinguishing trait is the author’s ability to <strong>speak fluently across disciplinary boundaries</strong>. Readers from control theory, operations research, artificial intelligence, and applied mathematics will all find familiar anchors—but also be challenged to expand their mental models. The book avoids dense formalism without sacrificing rigor, preferring <strong>geometric insights, intuitive visualizations, and algorithmic thinking</strong> to heavy abstraction. This makes it an accessible yet intellectually satisfying read for those seeking more than surface-level understanding.</p>
<p>Moreover, the text is <strong>modular and customizable</strong>, making it ideal for various course structures—from short introductory classes to advanced research seminars on RL theory. Each chapter is self-contained yet richly interlinked, allowing instructors or self-learners to navigate the material according to their background and goals.</p>
</section>
<section id="enduring-relevance-in-a-fast-evolving-field" class="level3">
<h3 class="anchored" data-anchor-id="enduring-relevance-in-a-fast-evolving-field">Enduring relevance in a fast-evolving field</h3>
<p>As machine learning matures beyond supervised pattern recognition, the methods described in <em>A Course in Reinforcement Learning</em> are becoming increasingly vital. Bertsekas’ emphasis on <strong>approximation in value space</strong>, <strong>policy iteration</strong>, and <strong>offline-online synergy</strong> aligns directly with the architecture of some of the most successful and well-known AI systems to date—<strong>AlphaZero</strong>, <strong>MuZero</strong>, and <strong>ChatGPT’s planning-inspired extensions</strong>. For example, MuZero (Schrittwieser et al., 2020) generalizes AlphaZero by learning its own model dynamics, yet it retains the <strong>value-based planning loop</strong> that Bertsekas formalizes through Newton-like updates on Bellman’s equation. Similarly, the resurgence of <strong>model-based reinforcement learning</strong> (e.g., DreamerV3, EfficientZero) is built on the same principle: learning value estimates offline and refining them online via planning—precisely the synergy this book explores in depth.</p>
<p>In addition, current research on <strong>LLMs with planning capabilities</strong> (e.g., ReAct, Tree of Thoughts, and OpenAI’s work on tool-use agents) echoes the structure of <strong>lookahead planning guided by learned evaluators</strong>, another core theme in the book. These methods increasingly blend <strong>rollout-based reasoning</strong>, <strong>value estimation</strong>, and <strong>policy improvement</strong>—even if not framed as reinforcement learning per se. Likewise, in robotics and safety-critical applications, algorithms must <strong>generalize reliably, adapt to perturbations</strong>, and <strong>provide interpretable decision-making</strong>—objectives far better served by Bertsekas’ structured, optimization-rooted methods than by end-to-end neural policy training alone.</p>
<p>Thus, what this book presents is not a retrospective—it is a <strong>forward-looking foundation</strong>. As machine learning turns toward <strong>long-horizon reasoning, planning under uncertainty, and adaptive control</strong>, the concepts in Bertsekas’ work are proving to be not only relevant, but essential.</p>
</section>
<section id="who-is-this-book-for" class="level3">
<h3 class="anchored" data-anchor-id="who-is-this-book-for">Who is this book for?</h3>
<p>This textbook is not for everyone—and that’s a strength, not a weakness.</p>
<ul>
<li>Ideal readers include:
<ul>
<li>Graduate students in <strong>electrical engineering</strong>, <strong>applied mathematics</strong>, <strong>operations research</strong>, or <strong>control systems</strong>.</li>
<li>Researchers and professionals interested in <strong>optimization</strong>, <strong>model predictive control</strong>, <strong>robotics</strong>, or <strong>multiagent systems</strong>.</li>
<li>Advanced undergraduates with a solid mathematical background and interest in decision-making under uncertainty.</li>
<li>Practitioners who have experience with algorithms and modeling, and want to understand the “why” behind reinforcement learning, not just the “how”.</li>
</ul></li>
<li>Less ideal for:
<ul>
<li>Beginners looking for an introductory, code-heavy book. For this, <em>Reinforcement Learning: An Introduction</em> by Sutton &amp; Barto or <em>Deep Reinforcement Learning Hands-On</em> by Maxim Lapan may be more approachable.</li>
<li>Readers wanting a focus on PyTorch/TensorFlow, environment setups, or software engineering best practices for RL pipelines.</li>
</ul></li>
</ul>
<p>Instead, Bertsekas offers something increasingly rare: a textbook that <strong>doesn’t treat reinforcement learning as a subfield of deep learning</strong>, but rather as a <strong>mathematically grounded discipline</strong> that can integrate with—but also stand apart from—modern AI trends.</p>
</section>
<section id="supplementary-materials" class="level3">
<h3 class="anchored" data-anchor-id="supplementary-materials">Supplementary materials</h3>
<p>The book is accompanied by <strong>video lectures, slides</strong>, and supplemental content from Bertsekas’ Arizona State University course, accessible via his website. This makes the book particularly useful for <strong>self-study</strong>, especially for learners who appreciate visual explanation and conceptual repetition across formats.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p><em>A Course in Reinforcement Learning (2nd Edition)</em> is a rigorous, insightful, and conceptually rich text that helps readers <strong>understand reinforcement learning as a structured decision-making framework</strong>, not just a toolbox of tricks. If you’re serious about applying RL to real-world, high-stakes systems—where stability, interpretability, and theoretical guarantees matter—this is the book you’ve been looking for.</p>
<p>It challenges, rewards, and broadens the reader’s view of what reinforcement learning is and what it can be. Highly recommended for those looking to <strong>build durable understanding</strong>, not just quick implementations.</p>
</section>
<section id="further-readings" class="level3">
<h3 class="anchored" data-anchor-id="further-readings">Further readings</h3>
<ul>
<li><p>Bertsekas, D. P. (2022). <em>Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control</em>. Athena Scientific. <a href="https://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf">Download from author website</a></p></li>
<li><p>Bertsekas, D. P. (2020). <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em>. Athena Scientific. <a href="http://www.athenasc.com/rolloutbook_athena.html">More information</a></p></li>
<li><p>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., … &amp; Silver, D. (2020). <em>Mastering Atari, Go, Chess and Shogi by planning with a learned model</em>. Nature, 588(7839), 604–609. <a href="https://www.nature.com/articles/s41586-020-03051-4">Read the paper</a></p></li>
<li><p>Hafner, D., Lillicrap, T., Norouzi, M., &amp; Ba, J. (2023). <em>Mastering diverse domains through world models</em>. arXiv preprint arXiv:2301.04104. <a href="https://arxiv.org/abs/2301.04104">Read the paper</a></p></li>
<li><p>Ye, J., Lin, G., Xu, H., Liao, R., Yang, E., Lu, H., … &amp; Liu, S. (2021). <em>Mastering Atari Games with Limited Data</em>. Advances in Neural Information Processing Systems, 34. <a href="https://arxiv.org/abs/2111.00210">Read the paper</a></p></li>
<li><p>Yao, S., Zhao, J., Yu, D., Narasimhan, K., &amp; Zhang, Y. (2023). <em>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</em>. arXiv preprint arXiv:2305.10601. <a href="https://arxiv.org/abs/2305.10601">Read the paper</a></p></li>
<li><p>Yao, S., Zhao, J., Yu, D., Kasai, J., Wong, A., &amp; Zhang, Y. (2022). <em>ReAct: Synergizing Reasoning and Acting in Language Models</em>. arXiv preprint arXiv:2210.03629. <a href="https://arxiv.org/abs/2210.03629">Read the paper</a></p></li>
<li><p>Aghzal, M., Plaku, E., Stein, G. J., &amp; Yao, Z. (2025). <em>A Survey on Large Language Models for Automated Planning</em>. arXiv preprint arXiv:2502.12435. <a href="https://arxiv.org/abs/2502.12435">Read the paper</a></p></li>
<li><p>Tantakoun, M., Zhu, X., &amp; Muise, C. (2025). <em>LLMs as Planning Modelers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models</em>. arXiv preprint arXiv:2503.18971. <a href="https://arxiv.org/abs/2503.18971">Read the paper</a></p></li>
<li><p>Li, H., Chen, Z., Zhang, J., &amp; Liu, F. (2024). <em>LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning</em>. arXiv preprint arXiv:2409.01806. <a href="https://arxiv.org/abs/2409.01806">Read the paper</a></p></li>
<li><p>Changle, Q., et al.&nbsp;(2024). <em>Tool Learning with Large Language Models: A Survey</em>. Frontiers of Computer Science. <a href="https://arxiv.org/abs/2405.17935">Read the paper</a></p></li>
</ul>
</section>
</section>
<section id="about-the-author" class="level2">
<h2 class="anchored" data-anchor-id="about-the-author">About the author</h2>
<p>Dimitri P. Bertsekas is a world-renowned scholar in optimization and control theory. He received his Ph.D.&nbsp;in system science from MIT and has held faculty positions at Stanford, the University of Illinois, and MIT, where he remains McAfee Professor of Engineering. Since 2019, he has been Fulton Professor of Computational Decision Making at Arizona State University.</p>
<p>Over a prolific academic career, Bertsekas has authored over twenty influential books covering topics from nonlinear programming to data networks, and notably, dynamic programming and reinforcement learning. His contributions have earned him numerous accolades, including the IEEE Control Systems Award, INFORMS John von Neumann Theory Prize, and election to the U.S. National Academy of Engineering. His recent focus on RL reflects decades of foundational work in dynamic programming and optimization, making him a key figure in bridging classical control with modern machine learning.</p>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>A Course in Reinforcement Learning</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025 (2nd edition)</td>
</tr>
<tr class="odd">
<td><strong>Author</strong></td>
<td><a href="https://www.mit.edu/~dimitrib/home.html">Dimitri P. Bertsekas</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="http://www.athenasc.com/">Athena Scientific</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Reinforcement learning, Model predictive control, Dynamics programming, Machine learning</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE%202ndEDITION.pdf">Book PDF</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLbook.html">Course videolectures and materials</a></td>
</tr>
<tr class="odd">
<td><strong>ISBN/DOI</strong></td>
<td>1-886529-29-9</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="social-media" class="level2">
<h2 class="anchored" data-anchor-id="social-media">Social media</h2>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I am pleased to share a review of my book "A course in reinforcement learning" (2nd edition) <a href="https://t.co/VkRcIl6TB2">https://t.co/VkRcIl6TB2</a><br>This is the textbook for my RL course at ASU (free PDF at <a href="https://t.co/rB9CcChOHS">https://t.co/rB9CcChOHS</a>)<a href="https://twitter.com/hashtag/reinforcementlearning?src=hash&amp;ref_src=twsrc%5Etfw">#reinforcementlearning</a> <a href="https://twitter.com/hashtag/machinelearning?src=hash&amp;ref_src=twsrc%5Etfw">#machinelearning</a>
</p>
— Dimitri Bertsekas (<span class="citation" data-cites="DBertsekas">@DBertsekas</span>) <a href="https://twitter.com/DBertsekas/status/1913627106552209493?ref_src=twsrc%5Etfw">April 19, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>🇬🇧</category>
  <guid>https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/</guid>
  <pubDate>Fri, 18 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Probabilistic Machine Learning - Kevin Murphy</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/1.jpg" class="img-fluid"></p>
</div><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/2.jpg" class="img-fluid"></p>
</div><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/3.jpg" class="img-fluid"></p>
</div></div>

<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<section id="introduction-what-is-probabilistic-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="introduction-what-is-probabilistic-machine-learning">Introduction: what Is probabilistic machine learning?</h3>
<p>Machine learning (ML) is the discipline that focuses on designing algorithms that allow computers to learn from data, make predictions, and improve over time without being explicitly programmed for each task. At its core, traditional machine learning can often be understood as a function approximation problem: given inputs <img src="https://latex.codecogs.com/png.latex?X">, learn a function <img src="https://latex.codecogs.com/png.latex?f:%20X%20%5Crightarrow%20Y"> that maps those inputs to outputs. Many successful techniques—including decision trees, support vector machines, and deep neural networks—fit this paradigm and have achieved outstanding performance on tasks such as image recognition, language modeling, and game playing.</p>
<p>However, real-world problems are rarely deterministic or noiseless. Uncertainty, ambiguity, and incomplete information are the norm rather than the exception. This is where <strong>probabilistic machine learning (PML)</strong> becomes essential.</p>
<p>Probabilistic machine learning is a subfield of ML that emphasizes modeling uncertainty explicitly. Rather than producing a single output or estimate, a probabilistic model produces a <strong>distribution</strong> over possible outcomes, quantifying the confidence in each prediction. In this paradigm, learning is formulated as a problem of <strong>statistical inference</strong>: given observed data, infer the posterior distribution over unobserved variables (such as model parameters, latent states, or predictions).</p>
<p>At the heart of PML is the application of probability theory to every stage of the learning process:</p>
<ul>
<li>Bayesian reasoning allows incorporation of prior knowledge and principled uncertainty estimation.</li>
<li>Latent variable models capture hidden structure in data.</li>
<li>Graphical models express dependencies between variables clearly and concisely.</li>
<li>Approximate inference methods (e.g., variational inference, MCMC) make complex models computationally tractable.</li>
</ul>
<p>In short, probabilistic machine learning provides a <strong>principled, coherent, and extensible framework</strong> for designing models that reason under uncertainty and adapt to changing data distributions.</p>
<p>While traditional ML methods often prioritize point predictions and empirical accuracy, PML emphasizes:</p>
<ul>
<li>Uncertainty quantification</li>
<li>Interpretability and diagnostics</li>
<li>Robustness to noise and model misspecification</li>
<li>Principled model comparison</li>
<li>Decision-making under uncertainty</li>
</ul>
<p>The rise of large-scale deep learning has produced models that are often opaque and poorly calibrated. As machine learning systems are increasingly used in critical applications—such as healthcare, finance, climate modeling, and autonomous vehicles—the ability to <strong>reason about uncertainty, causality, and decision-making</strong> becomes vital. PML provides the tools to:</p>
<ul>
<li>Assess when a model is unsure or operating out-of-distribution</li>
<li>Build systems that can update beliefs as new evidence arrives</li>
<li>Integrate domain knowledge via priors and structured models</li>
<li>Make robust decisions in the face of uncertainty and risk</li>
</ul>
<p>Modern advances—such as <strong>variational autoencoders</strong>, <strong>normalizing flows</strong>, <strong>Bayesian deep learning</strong>, <strong>causal inference frameworks</strong>, and <strong>probabilistic programming languages</strong>—have dramatically expanded the scope and scalability of PML.</p>
<p>Kevin Murphy’s trilogy of books stands out for presenting the field of machine learning entirely through the probabilistic lens. His work provides a unified view of learning, inference, prediction, and decision-making, making the case that probabilistic modeling is not an alternative to machine learning—it is its natural and rigorous generalization.</p>
</section>
<section id="the-trilogy" class="level3">
<h3 class="anchored" data-anchor-id="the-trilogy">The trilogy</h3>
<section id="machine-learning-a-probabilistic-perspective-2012" class="level4">
<h4 class="anchored" data-anchor-id="machine-learning-a-probabilistic-perspective-2012">1. Machine Learning: A Probabilistic Perspective (2012)</h4>
<p>Murphy’s first book, published in 2012, was a landmark achievement that laid the groundwork for much of the modern probabilistic approach to machine learning. Spanning over 1000 pages, it offers a comprehensive and mathematically rigorous treatment of probabilistic modeling, treating learning as a problem of statistical inference. The book begins with foundational material in probability theory and Bayesian statistics before delving into a broad array of topics such as supervised learning (regression and classification), unsupervised learning (clustering, dimensionality reduction), probabilistic graphical models (Bayesian networks and Markov random fields), and approximate inference techniques (variational inference, Markov Chain Monte Carlo). It also covers expectation-maximization, hidden Markov models, kernel methods, and Gaussian processes. This volume is notable for its unified framework that emphasizes modeling uncertainty, integrating prior knowledge, and reasoning about complex data using structured probabilistic methods.</p>
<p>Core content:</p>
<ul>
<li>Probability theory, Bayesian statistics, decision theory.</li>
<li>Supervised learning: regression, classification, support vector machines.</li>
<li>Unsupervised learning: clustering, dimensionality reduction.</li>
<li>Probabilistic graphical models: Bayesian networks and Markov random fields.</li>
<li>Approximate inference: variational methods, MCMC.</li>
<li>Expectation-maximization (EM), mixture models, and hidden Markov models.</li>
<li>Model comparison, selection, and overfitting.</li>
<li>Kernel methods and Gaussian processes.</li>
</ul>
<p>The book is methodical and mathematically grounded. It doesn’t shy away from complexity but presents concepts in a pedagogically sound manner. It serves both as a textbook and a reference, rich in examples and code snippets (originally in MATLAB/Octave).</p>
<p>While deep learning has dramatically changed the machine learning landscape since 2012, the foundations presented in MLAPP remain critically relevant. The probabilistic treatment of learning, inference, and model uncertainty continues to underpin research in Bayesian deep learning, causal inference, reinforcement learning, and decision-making.</p>
<p>However, the book does not cover recent developments such as generative models (GANs, VAEs, diffusion models), large-scale optimization in neural networks, or probabilistic programming systems, making it less suitable for those primarily focused on modern deep learning pipelines.</p>
</section>
<section id="probabilistic-machine-learning-an-introduction-2022" class="level4">
<h4 class="anchored" data-anchor-id="probabilistic-machine-learning-an-introduction-2022">2. Probabilistic Machine Learning: An Introduction (2022)</h4>
<p>This volume represents a pedagogical reboot and modernization of MLAPP. It revisits many of the same foundational topics but presents them with improved clarity, a more focused scope, and integration with modern tools such as JAX and NumPyro for probabilistic programming. The book begins with the fundamentals of probability theory and statistical inference, emphasizing both Bayesian and frequentist approaches. It then covers key techniques in supervised learning, such as linear and logistic regression, and introduces Gaussian distributions, conjugate priors, and hierarchical models. Latent variable models such as PCA and Gaussian mixture models are explored in accessible terms. Notably, the book introduces approximate inference using variational methods and connects them to probabilistic programming frameworks. It also addresses model evaluation techniques, such as cross-validation and information criteria, and concludes with an overview of Gaussian processes. Overall, this volume offers a solid, modern foundation in probabilistic modeling, suitable for both students and practitioners entering the field.</p>
<p>Core content:</p>
<ul>
<li>Probability and statistics for machine learning.</li>
<li>Bayesian and frequentist inference.</li>
<li>Maximum likelihood estimation, MAP, posterior predictive distributions.</li>
<li>Conjugate priors, hierarchical models, and empirical Bayes.</li>
<li>Gaussian distributions, linear regression, logistic regression.</li>
<li>Gaussian processes (including scalable approximations).</li>
<li>Probabilistic programming and variational inference.</li>
<li>Decision theory and model evaluation (cross-validation, information criteria).</li>
<li>Basic latent variable models: PCA, mixture models, factor analysis.</li>
</ul>
<p>Several pedagogical enhancements that support learning and experimentation are introduced in this book. Readers are provided with runnable Jupyter notebooks, conveniently hosted in Google Colab, which allow for hands-on interaction with code and models. The book’s figures and illustrations are generated directly from these notebooks, enabling a tight integration between theory, visualization, and practice. The writing style emphasizes conceptual clarity without compromising on mathematical rigor, making advanced topics more approachable and easier to internalize for a wide audience.</p>
<p>This book is arguably more relevant than MLAPP for a newcomer or intermediate reader in 2025. It reflects the shift toward integrating probability with scalable modern tools and introduces practical workflows for modeling and inference. While it deliberately avoids diving deep into generative models or reinforcement learning, it sets a strong foundation for those topics.</p>
<p>Importantly, readers do not need to read <em>Machine Learning: A Probabilistic Perspective</em> before tackling this book. <em>Probabilistic Machine Learning: An Introduction</em> is designed to be self-contained and more accessible, making it an ideal starting point for those new to the field or those seeking a practical yet principled approach to probabilistic modeling.</p>
<p>For instructors designing machine learning or probabilistic modeling courses, it is an excellent primary textbook. than MLAPP for a newcomer or intermediate reader in 2025. It reflects the shift toward integrating probability with scalable modern tools and introduces practical workflows for modeling and inference. While it deliberately avoids diving deep into generative models or reinforcement learning, it sets a strong foundation for those topics.</p>
<p>For instructors designing machine learning or probabilistic modeling courses, it is an excellent primary textbook.</p>
</section>
<section id="probabilistic-machine-learning-advanced-topics-2025-draft" class="level4">
<h4 class="anchored" data-anchor-id="probabilistic-machine-learning-advanced-topics-2025-draft">3. Probabilistic Machine Learning: Advanced Topics (2025 Draft)</h4>
<p>The newly released draft of <em>Advanced Topics</em> is the final and most ambitious volume in Murphy’s trilogy. Designed as a natural progression from <em>An Introduction</em>, it targets readers already comfortable with the fundamentals of probabilistic modeling and Bayesian inference. This volume delves into state-of-the-art techniques that define the cutting edge of modern probabilistic machine learning.</p>
<section id="structure" class="level5">
<h5 class="anchored" data-anchor-id="structure">Structure</h5>
<p>The book is structured into six thematic parts:</p>
<ul>
<li><p><strong>Part I (Fundamentals)</strong> provides a deep theoretical base, covering advanced probability, exponential family models, divergence measures, and optimization principles. It also revisits graphical models in greater depth, including conditional random fields and structured representations.</p></li>
<li><p><strong>Part II (Inference)</strong> focuses on modern approximate inference techniques. It presents Gaussian filtering and smoothing (e.g., Kalman filters and their nonlinear extensions), belief propagation on graphs, variational inference (both classic and black-box forms), and a suite of Monte Carlo methods including Hamiltonian Monte Carlo and sequential Monte Carlo.</p></li>
<li><p><strong>Part III (Prediction)</strong> explores models for supervised learning and uncertainty-aware prediction. This includes generalized linear models, Bayesian neural networks, Gaussian processes with deep kernels, and models for handling non-iid data and distributional shift.</p></li>
<li><p><strong>Part IV (Generation)</strong> is devoted to deep generative models. It introduces variational autoencoders, normalizing flows, diffusion models, autoregressive networks, and energy-based models, with careful attention to training objectives, model evaluation, and sampling.</p></li>
<li><p><strong>Part V (Discovery)</strong> addresses unsupervised and representation learning. It covers latent factor models, state-space models, topic models, deep sequence modeling, graph structure discovery, and interpretability through the lens of probabilistic inference.</p></li>
<li><p><strong>Part VI (Action)</strong> focuses on decision-making, reinforcement learning, and causality. Topics include decision theory, active learning, policy search, model-based and model-free reinforcement learning, influence diagrams, and modern approaches to causal inference including do-calculus, instrumental variables, and counterfactual reasoning.</p></li>
</ul>
<p>Throughout, the book maintains a strong emphasis on scalable inference, modern software tools, and connections between theory and real-world applications. It is both technically deep and broad in scope, providing readers with the tools and intuition needed to work on contemporary research problems in probabilistic modeling, decision-making, and AI.</p>
</section>
<section id="technical-depth-and-breadth" class="level5">
<h5 class="anchored" data-anchor-id="technical-depth-and-breadth">Technical depth and breadth</h5>
<p>This volume is the most technically rigorous of the trilogy. It introduces advanced concepts from statistics, stochastic processes, signal processing, and control theory—disciplines that underpin many of today’s most impactful machine learning innovations. Its comprehensive treatment of probabilistic graphical models and information theory lays the groundwork for structured generative models and modern approaches to representation learning. The discussion of variational inference and Monte Carlo techniques directly supports scalable inference in applications such as variational autoencoders and Bayesian neural networks.</p>
<p>Reinforcement learning is explored through a probabilistic lens, emphasizing the “control as inference” paradigm, which has gained traction in contemporary deep reinforcement learning frameworks. The causal inference section equips readers with formal tools like do-calculus and instrumental variables, essential for understanding causality-aware systems now central to policy evaluation and scientific discovery.</p>
<p>Practical applications that embody these methods include: uncertainty quantification in medical diagnostics using Bayesian neural networks; model-based reinforcement learning in robotic control systems; causal effect estimation in healthcare and social science through counterfactual reasoning; high-fidelity image generation using diffusion models; and structural learning in genomics with probabilistic graphical models. These examples demonstrate how the book’s theoretical foundation translates into applied machine learning systems that operate under uncertainty with reliability and interpretability.</p>
<p>Overall, the volume serves as a bridge connecting classical probabilistic theory with the most recent developments in generative AI, causal discovery, and probabilistic decision-making under uncertainty.</p>
</section>
<section id="relevance-today" class="level5">
<h5 class="anchored" data-anchor-id="relevance-today">Relevance today</h5>
<p>Few books capture the breadth and depth of modern probabilistic machine learning like this one. The inclusion of cutting-edge generative models, reinforcement learning frameworks, and causal reasoning makes it highly relevant. The tight integration with the current state of research ensures that the book will remain a key reference for years to come.</p>
<p>However, it is not intended for beginners. It presupposes comfort with advanced probability, linear algebra, and statistical inference, as well as practical fluency with modern ML tools.</p>
</section>
</section>
</section>
<section id="recommendations" class="level3">
<h3 class="anchored" data-anchor-id="recommendations">Recommendations</h3>
<p>The following table is designed to help readers at different levels identify the most appropriate entry point into Kevin Murphy’s trilogy. Whether you’re a student just beginning to explore probabilistic methods or a researcher seeking depth in advanced generative modeling, this summary suggests a tailored path through the books. The guidance considers both the technical demands and the intended pedagogical focus of each volume.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 44%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Reader profile</th>
<th>Suggested reading path</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Undergraduate ML student</td>
<td><em>Probabilistic Machine Learning: An Introduction</em></td>
</tr>
<tr class="even">
<td>Graduate student in ML/statistics</td>
<td><em>Introduction</em> → <em>Advanced Topics</em></td>
</tr>
<tr class="odd">
<td>ML instructor or course designer</td>
<td><em>Introduction</em> as textbook, <em>Advanced Topics</em> for graduate seminar</td>
</tr>
<tr class="even">
<td>Probabilistic programming/research engineer</td>
<td><em>Advanced Topics</em> (esp.&nbsp;inference and generative models)</td>
</tr>
<tr class="odd">
<td>Causal inference/decision science researcher</td>
<td><em>Advanced Topics</em>, especially Part VI</td>
</tr>
<tr class="even">
<td>Deep learning expert seeking interpretability and uncertainty tools</td>
<td><em>Advanced Topics</em>, especially Bayesian deep learning and conformal prediction</td>
</tr>
<tr class="odd">
<td>General ML practitioner</td>
<td><em>Introduction</em> for foundation, followed by selected chapters from <em>Advanced Topics</em></td>
</tr>
</tbody>
</table>
</section>
<section id="relation-to-contemporary-trends-in-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="relation-to-contemporary-trends-in-machine-learning">Relation to contemporary trends in machine learning</h3>
<p>Murphy’s trilogy is deeply relevant to the dominant themes and architectures that currently define machine learning research and application. Notably, the series intersects with several state-of-the-art paradigms such as large language models (LLMs), diffusion models, and other generative frameworks that have become increasingly influential.</p>
<p>In the context of LLMs, while Murphy’s books do not focus on transformer architectures directly, they provide the underlying probabilistic theory necessary to reason about uncertainty, representation learning, and approximate inference in high-dimensional models. This is critical for efforts in Bayesian deep learning applied to LLMs, such as uncertainty-aware language generation, model calibration, and adaptive fine-tuning. Additionally, the treatment of autoregressive modeling and structured sequence generation in the <em>Advanced Topics</em> volume aligns closely with the mathematical principles that underlie language models.</p>
<p>Regarding diffusion models—which have become central to generative image and video synthesis—Murphy’s final volume offers one of the few textbook treatments of these architectures from a probabilistic perspective. It places diffusion models in the broader landscape of score-based generative models, stochastic differential equations, and probabilistic denoising. This not only clarifies how these models work but also situates them in a principled framework for likelihood-based training and sampling.</p>
<p>Moreover, the books’ emphasis on latent variable models, variational inference, and probabilistic programming provides essential context for understanding hybrid approaches that combine deterministic deep networks with stochastic components—an increasingly common design in modern ML systems.</p>
<p>In sum, while Murphy’s books are not focused on deep learning trends per se, they offer foundational and theoretical insight that is crucial for interpreting, extending, and critiquing today’s most influential machine learning models.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p>Kevin Murphy’s <em>Probabilistic Machine Learning</em> trilogy offers an exceptional and enduring contribution to the field of machine learning. It accomplishes what few educational resources have: it builds a conceptual and mathematical bridge between foundational statistical thinking and the evolving frontier of machine learning research.</p>
<p><em>Machine Learning: A Probabilistic Perspective</em> serves as a deep and comprehensive reference text, best suited to readers with a solid foundation in mathematics and a desire to explore classical probabilistic modeling in depth. Despite its age, it remains highly relevant for understanding the theoretical underpinnings of the field.</p>
<p><em>Probabilistic Machine Learning: An Introduction</em> is the most accessible and pedagogically refined volume. It strikes a balance between formal rigor and practical usability, making it the best entry point for students, practitioners, and instructors aiming to teach or learn probabilistic reasoning in modern contexts.</p>
<p><em>Probabilistic Machine Learning: Advanced Topics</em> is a masterful synthesis of recent innovations, making it a must-read for researchers, PhD students, and experienced engineers interested in state-of-the-art techniques for uncertainty modeling, generative modeling, causality, and decision-making under uncertainty.</p>
<p>Collectively, these volumes are more than just textbooks—they form a modern curriculum for anyone serious about understanding and building intelligent systems capable of reasoning under uncertainty. Whether used in academia, research, or applied settings, Murphy’s trilogy provides the theoretical backbone and practical insight necessary to advance the field of machine learning responsibly and rigorously.</p>
</section>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>Probabilistic Machine Learning (3-volume series)</td>
</tr>
<tr class="even">
<td><strong>Years</strong></td>
<td>2012 (<em>MLAPP</em>), 2022 (<em>Introduction</em>), 2025 (<em>Advanced Topics</em>, draft)</td>
</tr>
<tr class="odd">
<td><strong>Author</strong></td>
<td><a href="https://www.cs.ubc.ca/~murphyk/">Kevin P. Murphy</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Bayesian statistics, probabilistic inference, graphical models, generative models, reinforcement learning, causality</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><p><a href="https://github.com/probml/pml-book/releases/latest/download/book1.pdf"><em>Introduction</em> PDF | CC BY-NC-ND 4.0</a></p>
<p><a href="https://github.com/probml/pml2-book/releases/latest/download/book2.pdf"><em>Advanced Topics</em> PDF | CC BY-NC-ND 4.0</a></p></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><p><a href="https://probml.github.io/">Books site</a></p>
<p><a href="https://github.com/probml/pyprobml">GitHub notebooks</a></p></td>
</tr>
<tr class="odd">
<td><strong>ISBNs</strong></td>
<td>978-0262046824 (<em>Introduction</em>), 978-0262048378 (<em>Advanced Topics</em>)</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td><p><a href="https://mitpress.mit.edu/9780262046824/probabilistic-machine-learning/">MIT Press – Introduction</a></p>
<p><a href="https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/">MIT Press – Advanced Topics</a></p></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>🇬🇧</category>
  <guid>https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/</guid>
  <pubDate>Sat, 05 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/murphy-probabilistic-machine-learning-series/3.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Differential Privacy - Simson L. Garfinkel</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/m_9780262382168-cover.jpeg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>This monograph provides a non-mathematical yet rigorous conceptual introduction to the theory and application of <strong>differential privacy (DP)</strong>. It is part of MIT Press’s <em>Essential Knowledge</em> series and is explicitly written for a <strong>general audience</strong>, without requiring prior familiarity with statistics, mathematics, or computer science.</p>
<p>Rather than offering algorithmic recipes or formal proofs, the book aims to:</p>
<ul>
<li>Build <strong>intuitive understanding</strong> of the DP paradigm.</li>
<li>Trace the <strong>historical development</strong> of the field.</li>
<li>Contextualize DP through <strong>real-world implementations</strong> (notably the 2020 U.S. Census).</li>
<li>Examine the <strong>philosophical and policy implications</strong> of deploying DP systems.</li>
</ul>
<section id="content-summary" class="level3">
<h3 class="anchored" data-anchor-id="content-summary">Content summary</h3>
<section id="conceptual-foundations" class="level4">
<h4 class="anchored" data-anchor-id="conceptual-foundations">Conceptual foundations</h4>
<p>Garfinkel introduces differential privacy as a <strong>mathematically formal approach to privacy protection</strong>, contrasted with heuristic or “best effort” techniques such as de-identification and data masking.</p>
<p>Key theoretical components are explained using analogies:</p>
<ul>
<li><strong>Sensitivity</strong> and the use of <strong>calibrated noise</strong> via the Laplace or Gaussian mechanisms.</li>
<li>The <strong>privacy loss budget</strong> (ε) and the implications of tuning it.</li>
<li><strong>Composability</strong> and how DP offers provable guarantees even under repeated queries.</li>
<li>The <strong>mosaic effect</strong>, demonstrating how auxiliary data enables reidentification even from aggregated or anonymized data.</li>
</ul>
<p>Although mathematical notation is absent, the conceptual treatment aligns with the formal definitions of (ε, δ)-differential privacy as used in foundational literature.</p>
</section>
<section id="historical-and-institutional-context" class="level4">
<h4 class="anchored" data-anchor-id="historical-and-institutional-context">Historical and institutional context</h4>
<p>The author documents the trajectory of DP from its introduction by Dwork, McSherry, Nissim, and Smith (2006) to its adoption in high-stakes settings such as:</p>
<ul>
<li>The U.S. Census Bureau’s <strong>Disclosure Avoidance System</strong>.</li>
<li>Google’s and Apple’s telemetry systems.</li>
<li>NIST’s evolving standards for privacy-enhancing technologies (e.g., SP 800-188).</li>
</ul>
<p>The tension between <strong>statistical utility and privacy guarantees</strong> is explored through case studies, such as the controversy surrounding the use of DP in the 2020 Census.</p>
</section>
<section id="critique-and-alternatives" class="level4">
<h4 class="anchored" data-anchor-id="critique-and-alternatives">Critique and alternatives</h4>
<p>Garfinkel acknowledges the limitations of DP:</p>
<ul>
<li>Not suitable for <strong>non-tabular data</strong> (e.g., video, audio).</li>
<li>Noise injection can <strong>degrade data utility</strong>, especially for small subpopulations.</li>
<li>DP is often <strong>challenging to tune</strong>, and the setting of ε is as much a <strong>policy decision</strong> as a technical one.</li>
</ul>
<p>He contrasts DP with legacy disclosure limitation methods (e.g., k-anonymity, cell suppression, top-coding), arguing that while they lack formal guarantees, they may provide better practical utility in some scenarios.</p>
<p>Strengths:</p>
<ul>
<li><strong>Clarity of exposition</strong>: Abstract concepts are explained using accessible examples (e.g., student grades, public statistics).</li>
<li><strong>Policy relevance</strong>: The author’s experience at NIST and the Census Bureau enables an authoritative treatment of institutional use cases.</li>
<li><strong>Balanced perspective</strong>: Both the promises and criticisms of DP are articulated, with references to academic and legal debates (e.g., lawsuits surrounding the 2020 Census).</li>
</ul>
<p>Limitations:</p>
<ul>
<li><strong>Lack of formalism</strong>: No mathematical definitions, algorithms, or implementation details are presented.</li>
<li><strong>Limited scope of mechanisms</strong>: The book focuses almost exclusively on the <strong>central differential privacy model</strong>, with minimal treatment of <strong>local DP</strong>, <strong>privacy accounting methods</strong> (e.g., Rényi DP), or advanced techniques like <strong>privacy amplification</strong>.</li>
<li><strong>No discussion of implementation frameworks</strong>: Libraries like Google’s DP library, OpenDP, and SmartNoise are only briefly mentioned in the notes, with no comparative discussion.</li>
</ul>
</section>
</section>
<section id="related-works" class="level3">
<h3 class="anchored" data-anchor-id="related-works">Related works</h3>
<p>The book is best read alongside more technical treatments, such as:</p>
<ul>
<li>Dwork, C., &amp; Roth, A. (2014). <em>The algorithmic foundations of differential privacy</em>. Now Publishers. <a href="https://doi.org/10.1561/0400000042">DOI</a></li>
<li>Gaboardi, M., Honaker, J., &amp; Stoddard, J. (2023). <em>Programming differential privacy</em>. O’Reilly Media.</li>
<li>National Institute of Standards and Technology. (2023). <em>De-identifying government datasets: Techniques and governance (NIST Special Publication 800-188)</em>. U.S. Department of Commerce. <a href="https://doi.org/10.6028/NIST.SP.800-188">DOI</a></li>
</ul>
</section>
<section id="recommendation" class="level3">
<h3 class="anchored" data-anchor-id="recommendation">Recommendation</h3>
<p>This book is recommended as a policy- and systems-oriented primer for:</p>
<ul>
<li>Privacy professionals and data protection officers.</li>
<li>Statisticians and social scientists new to formal privacy models.</li>
<li>Government and industry decision-makers evaluating privacy technologies.</li>
</ul>
<p>It is <strong>not</strong> a substitute for a formal or computational introduction for practitioners seeking to implement DP mechanisms in production systems.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p>Simson L. Garfinkel’s <em>Differential Privacy</em> is an essential conceptual guide that democratizes access to one of the most profound ideas in data privacy. While limited in technical depth, it succeeds in making the case for DP’s significance and encourages broader adoption and informed critique.</p>
</section>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>Differential Privacy</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025</td>
</tr>
<tr class="odd">
<td><strong>Author</strong></td>
<td><a href="https://simson.net/page/Main_Page">Simson L. Garfinkel</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Differential privacy, Statistical disclosure limitation, Privacy-preserving data analysis, Data governance</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf">PDF | CC BY-NC-ND 4.0</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://direct.mit.edu/books/book/5935/Differential-Privacy">Publisher book page</a></td>
</tr>
<tr class="odd">
<td><strong>ISBN</strong></td>
<td>9780262382168</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td><a href="https://mitpress.mit.edu/9780262551656/differential-privacy/">MIT Press</a></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>data privacy</category>
  <category>🇬🇧</category>
  <guid>https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/</guid>
  <pubDate>Tue, 01 Apr 2025 22:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/garfinkel-differential-privacy/m_9780262382168-cover.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Design Rules, Volume 2: How Technology Shapes Organizations - Carliss Y. Baldwin</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/m_9780262380232-cover.jpeg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In a world increasingly dominated by digital technologies, the traditional boundaries of organizations are being redrawn. Vertically integrated firms are giving way to globally distributed ecosystems. Software is being built by loosely affiliated communities rather than centralized R&amp;D labs. And platform-based businesses like Apple, Amazon, and Google now dominate entire sectors. But what exactly is driving these structural shifts in how work is organized? What is the role of technology in shaping the form and function of modern organizations?</p>
<p>These are the questions at the heart of <em>Design Rules, Volume 2: How Technology Shapes Organizations</em>, the latest contribution by Harvard Business School professor <strong>Carliss Y. Baldwin</strong>. This volume is the long-anticipated sequel to <em>Design Rules, Volume 1: The Power of Modularity</em> (2000), co-authored with Kim B. Clark. Together, the <em>Design Rules</em> series represents one of the most ambitious efforts in the past two decades to build a <strong>unified theory of technology and organizational design</strong>.</p>
<p>Where Volume 1 introduced the foundational concept of <strong>modularity</strong>—the idea that complex technical systems can be broken into loosely coupled, well-defined components—Volume 2 builds on this to offer a much broader and more general framework. The new volume is not just about modularity, but about <strong>how technologies, by their very structure and dynamics, shape the organizations that implement them</strong>. It goes beyond software and hardware, offering a theory with implications for economics, management science, industrial organization, and systems engineering.</p>
<p>Baldwin’s central premise is simple but profound: the <strong>design of technology</strong> imposes both <strong>constraints and affordances</strong> on how organizations must be structured if they are to implement that technology efficiently and capture value from it. Organizations, in this view, are not just social or legal constructs—they are functional responses to the demands of the technologies they use.</p>
<p>Through theoretical innovation and in-depth case analysis, Baldwin argues that different technologies give rise to different forms of organization: from tightly controlled hierarchies to loose, distributed ecosystems and open-source communities. The key lies in understanding the <strong>complementarity relationships</strong> among the components of a technical system—whether they are so tightly interdependent that they require centralized control, or loosely connected enough to permit decentralized innovation.</p>
<p>Volume 2 is a sweeping, multidisciplinary work that synthesizes insights from organizational economics, evolutionary theory, systems engineering, and the history of technology. It is both conceptually rigorous and empirically grounded, offering a unique and powerful perspective on how the deep structure of technology influences the surface form of organizational life.</p>
<section id="key-contributions" class="level3">
<h3 class="anchored" data-anchor-id="key-contributions">Key contributions</h3>
<p>At the heart of <em>Design Rules, Volume 2</em> lies an ambitious effort to explain how the structure of technology determines the structure of the organizations that use it. Carliss Baldwin lays out four key contributions that form the backbone of this sweeping theoretical and empirical work.</p>
<section id="a-general-theory-of-technology-and-organization" class="level4">
<h4 class="anchored" data-anchor-id="a-general-theory-of-technology-and-organization">A general theory of technology and organization</h4>
<p>First, Baldwin develops a general theory that places technology at the center of organizational design. She argues that technologies are not neutral tools—they come with <strong>material requirements</strong> and <strong>structural affordances</strong> that shape how organizations must be arranged in order to implement them effectively. Those organizations that align well with the constraints and opportunities presented by a given technology are more likely to survive, scale, and capture value in a competitive environment. This co-evolutionary view reframes technology as a driving force in the emergence, transformation, and demise of organizational forms.</p>
</section>
<section id="the-spectrum-of-complementarity" class="level4">
<h4 class="anchored" data-anchor-id="the-spectrum-of-complementarity">The spectrum of complementarity</h4>
<p>Second, Baldwin introduces the concept of the <strong>spectrum of complementarity</strong>, a powerful framework for understanding how the relationships between components of a technical system influence organizational choices. On one end of the spectrum are <strong>strong complements</strong>, where components must work together in tightly integrated ways. These favor <strong>centralized, hierarchical firms</strong> with unified governance. On the other end are <strong>weak complements</strong>, where components can be combined more flexibly and independently. These favor <strong>modular ecosystems</strong>, <strong>platform-based business models</strong>, and even <strong>open-source networks</strong>, where distributed governance, minimal hierarchy, and radical transparency can flourish. The degree of complementarity thus predicts the most efficient and sustainable form of organization for a given technology.</p>
</section>
<section id="value-structure-analysis" class="level4">
<h4 class="anchored" data-anchor-id="value-structure-analysis">Value structure analysis</h4>
<p>Third, the book introduces a novel method for visualizing and analyzing technical systems: <strong>value structure analysis</strong>. This approach maps the components of a system in terms of the tasks they perform and the value they contribute. By focusing on <strong>“thin crossing points”</strong>—interfaces between loosely coupled modules—Baldwin identifies where <strong>transaction costs are lowest</strong> and where opportunities for coordination, specialization, or platformization are greatest. This method is used throughout the book to analyze how systems create value, how that value can be captured (or lost), and how technical design decisions ripple outward into economic and organizational consequences.</p>
</section>
<section id="historical-and-contemporary-case-studies" class="level4">
<h4 class="anchored" data-anchor-id="historical-and-contemporary-case-studies">Historical and contemporary case studies</h4>
<p>Finally, Baldwin grounds her theory in a series of richly detailed case studies that span more than a century of industrial evolution. She traces the transition from the <strong>vertically integrated mass production firms</strong> of the early 20th century (like Ford and IBM) to the <strong>horizontally layered ecosystems</strong> of the digital age (such as Wintel, Dell, and Google). She explores the rise of <strong>open-source software</strong> and <strong>DevOps cultures</strong> as new organizational responses to the unique properties of software as a technology. And she examines the economic dynamics unleashed by <strong>Moore’s Law</strong>, showing how rapid technical change incentivized modular architectures and platform governance. These examples serve not just to illustrate the theory, but to demonstrate its explanatory power across a wide range of industries and technological domains.</p>
</section>
</section>
<section id="strengths-and-impact" class="level3">
<h3 class="anchored" data-anchor-id="strengths-and-impact">Strengths and impact</h3>
<p>One of the book’s greatest strengths is its theoretical originality. Baldwin brings together insights from engineering design, economics, organizational theory, and innovation studies to craft a truly interdisciplinary framework. Her use of concepts like value structure maps and the spectrum of complementarity is both novel and methodologically robust, offering readers tools to think more precisely about the relationship between technology and organizational form.</p>
<p>Equally impressive is the book’s empirical rigor. Each theoretical idea is grounded in detailed case studies spanning multiple industries and historical periods—from the rise of mass production in the early 20th century to the modern dominance of digital platforms and open-source ecosystems. Baldwin’s treatment of these examples is comprehensive, deeply researched, and intellectually generous, allowing the reader to see how abstract models play out in the real world.</p>
<p>Despite the complexity of its subject matter, the book maintains a remarkable degree of conceptual clarity. Technical ideas—such as design structure matrices (DSMs), modularity, or transaction cost placement—are illustrated through intuitive diagrams and historical analogies that make the material accessible even to those outside the academic core of the field. This clarity, combined with Baldwin’s forward-looking perspective, makes the book especially relevant for anyone trying to understand contemporary shifts in how value is created and captured in an era of rapid technological change.</p>
</section>
<section id="challenges-and-limitations" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and limitations</h3>
<p>That said, Design Rules, Volume 2 is not a light read. It carries a high cognitive load, packed with formal reasoning, abstract models, and layered argumentation. Readers unfamiliar with the foundations of organizational theory or economic modeling may need to invest time and effort to fully absorb its arguments.</p>
<p>Additionally, while the book offers a powerful framework, its case studies and examples are primarily drawn from technology-intensive sectors, especially computing, electronics, and software. As such, the theory’s direct applicability to more traditional or service-based industries may require further adaptation. Finally, although the book stands alone as a major contribution, readers who have not engaged with Design Rules, Volume 1 may occasionally feel they are missing important conceptual background—especially regarding the original treatment of modularity.</p>
</section>
<section id="who-should-read-this" class="level3">
<h3 class="anchored" data-anchor-id="who-should-read-this">Who should read this?</h3>
<p>Design Rules, Volume 2 is ideally suited for researchers and graduate students in fields such as organizational theory, innovation management, and platform economics. It is also a valuable resource for technology strategists and digital transformation leaders seeking to align business strategy with technical architecture. Policy makers working on industrial policy and digital governance will find it useful in understanding the structural underpinnings of modern ecosystems. Finally, systems engineers and enterprise architects will benefit from its deep insights into the interplay between design decisions and organizational constraints.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p><em>Design Rules, Volume 2</em> is a landmark work—ambitious in scope, rigorous in method, and deeply relevant to understanding the shifting terrain of technology and organization in the 21st century. Carliss Y. Baldwin does not merely build on the foundations laid in Volume 1; she expands the intellectual architecture to accommodate a more complex, dynamic world—one where platforms, ecosystems, and distributed forms of collaboration are rapidly supplanting the vertically integrated structures of the past.</p>
<p>What sets this book apart is not only its explanatory power but its <strong>unifying vision</strong>. Baldwin succeeds in weaving together disparate fields—economics, engineering, organizational design, strategy—into a coherent framework that makes sense of how technologies evolve and how they reshape the organizational landscape around them. Her concepts, particularly the <strong>spectrum of complementarity</strong> and <strong>value structure analysis</strong>, are not only insightful but immediately applicable. They offer a way of seeing and reasoning about organizations that transcends conventional business school wisdom.</p>
<p>This is not just a book for theorists. For practitioners—particularly those leading digital transformation efforts, architecting platform strategies, or navigating the rise of open-source and remote collaboration—<em>Design Rules, Volume 2</em> offers a vocabulary and logic for understanding what works, what doesn’t, and why. It provides a strategic lens through which to evaluate not just individual decisions, but the overall coherence between an organization’s technological infrastructure and its structure, governance, and long-term viability.</p>
<p>That said, it is also a demanding book. Readers without prior exposure to Volume 1 or to the literature on modularity, platform strategy, or organizational economics may find the intellectual terrain challenging. But for those willing to invest the time, the <strong>intellectual return is substantial</strong>.</p>
<p>In the end, <em>Design Rules, Volume 2</em> doesn’t just describe how technology shapes organizations—it shows us how to <strong>design organizations for a technological world</strong>. It will likely stand as a foundational reference for years to come, not only in academia but in boardrooms, design studios, and strategy workshops wherever technology meets enterprise.</p>
</section>
<section id="further-readings" class="level3">
<h3 class="anchored" data-anchor-id="further-readings">Further readings</h3>
<p>Core foundations:</p>
<ul>
<li><p>Baldwin, C. Y. (2023). Design rules: Past and future. <em>Industrial and Corporate Change, 32</em>(1), 11–27. <a href="https://doi.org/10.1093/icc/dtac055">DOI</a></p></li>
<li><p>Baldwin, C. Y., &amp; Clark, K. B. (2000). <em>Design rules, volume 1: The power of modularity</em>. MIT Press. <a href="https://mitpress.mit.edu/9780262024662/design-rules-vol-1/">Link</a></p></li>
<li><p>Simon, H. A. (1962). The architecture of complexity. <em>Proceedings of the American Philosophical Society</em>, <em>106</em>(6), 467–482. <a href="https://www.jstor.org/stable/985254">Link</a></p></li>
</ul>
<p>Organizational theory and economics:</p>
<ul>
<li><p>Williamson, O. E. (1985). <em>The economic institutions of capitalism: Firms, markets, relational contracting</em>. Free Press. ISBN: 002934820X, 9780029348208</p></li>
<li><p>Milgrom, P., &amp; Roberts, J. (1992). <em>Economics, organization and management</em>. Prentice Hall. ISBN: 9780132239677</p></li>
<li><p>Nelson, R. R., &amp; Winter, S. G. (1982). <em>An evolutionary theory of economic change</em>. Harvard University Press. <a href="https://www.hup.harvard.edu/books/9780674272286">Link</a></p></li>
</ul>
<p>Platforms and ecosystems:</p>
<ul>
<li><p>Cusumano, M. A., Gawer, A., &amp; Yoffie, D. B. (2019). <em>The business of platforms: Strategy in the age of digital competition, innovation, and power</em>. Harper Business. <a href="https://www.harpercollins.com/products/the-business-of-platforms-michael-a-cusumanoannabelle-gawerdavid-b-yoffie">Link</a></p></li>
<li><p>Jacobides, M. G., Cennamo, C., &amp; Gawer, A. (2018). Towards a theory of ecosystems. <em>Strategic Management Journal</em>, <em>39</em>(8), 2255–2276. <a href="https://doi.org/10.1002/smj.2904">DOI</a></p></li>
</ul>
<p>Software and Open Source</p>
<ul>
<li><p>von Hippel, E. (2005). <em>Democratizing innovation</em>. MIT Press. <a href="https://direct.mit.edu/books/book/2821/Democratizing-Innovation">Link</a></p></li>
<li><p>Raymond, E. S. (2001). <em>The cathedral and the bazaar: Musings on Linux and open source by an accidental revolutionary</em>. O’Reilly Media. <a href="https://www.oreilly.com/library/view/the-cathedral-the/0596001088/">Link</a></p></li>
</ul>
</section>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td><h1 id="subject">Subject</h1>
<p><strong>Title</strong></p></td>
<td><h1 id="content">Content</h1>
<p>Design Rules, Volume 2: How Technology Shapes Organization</p></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Year</strong></td>
<td>2024</td>
<td></td>
</tr>
<tr class="even">
<td><strong>Author</strong></td>
<td><a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=6417">Carliss Y. Baldwin</a></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Publisher</strong></td>
<td><a href="https://mitpress.mit.edu/">The MIT Press</a></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Language</strong></td>
<td>English</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Topics</strong></td>
<td>Modularity, Organizational design, Technology strategy, Platform ecosystems, Value networks</td>
<td></td>
</tr>
<tr class="even">
<td><strong>Downloads</strong></td>
<td><a href="https://direct.mit.edu/books/book-pdf/2487108/book_9780262380232.pdf">PDF | CC BY-NC-ND 4.0</a></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Other links</strong></td>
<td><a href="https://direct.mit.edu/books/oa-monograph/5887/Design-Rules-Volume-2How-Technology-Shapes">Publisher book page</a></td>
<td></td>
</tr>
<tr class="even">
<td><strong>ISBN/DOI</strong></td>
<td>9780262380232</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Buy online</strong></td>
<td><a href="https://mitpress.mit.edu/9780262049337/design-rules-volume-2/">MIT Press</a></td>
<td></td>
</tr>
</tbody>
</table>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>enterprise architecture</category>
  <category>🇬🇧</category>
  <guid>https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/</guid>
  <pubDate>Fri, 21 Feb 2025 23:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/collections/fkposts/baldwin-design-rules-volume-2-how-technology-shapes-organizations/m_9780262380232-cover.jpeg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
