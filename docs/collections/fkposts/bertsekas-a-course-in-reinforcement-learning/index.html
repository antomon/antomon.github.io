<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Antonio Montano">
<meta name="dcterms.date" content="2025-04-19">
<meta name="description" content="A Course in Reinforcement Learning (2nd Edition) by Dimitri P. Bertsekas presents a mathematically grounded yet accessible framework for understanding and applying reinforcement learning (RL) to sequential decision-making problems. Centered on the duality between off-line training and on-line play, and inspired by systems like AlphaZero and TD-Gammon, the book explores a unified methodology grounded in approximate dynamic programming. With emphasis on approximation in value space, the text bridges concepts from control theory, operations research, and artificial intelligence. Suitable for graduate-level courses or self-study, it offers a modular structure, visual intuitions, and real-world insight into key methods such as rollout algorithms, policy iteration, and deep learning-based value approximation. This edition integrates contemporary advances and expands discussions on topics like multi-agent systems, adaptive control, and model predictive control, while remaining focused on rigorous yet intuitive explanations.">

<title>A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas – Random Bits of Knowledge</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-1100dc645a9806080dc50effd6ccb0f7.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-efcabdd787298c2db09eab6dea954178.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=663ff7b280d7c0001914e592&amp;product=sticky-share-buttons" async="async"></script>
<script src="https://cdn.jsdelivr.net/npm/typewriter-effect@latest/dist/core.js"></script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas – Random Bits of Knowledge">
<meta property="og:description" content="A Course in Reinforcement Learning (2nd Edition) by Dimitri P. Bertsekas presents a mathematically grounded yet accessible framework for understanding and applying reinforcement learning (RL) to sequential decision-making problems. Centered on the duality between off-line training and on-line play, and inspired by systems like AlphaZero and TD-Gammon, the book explores a unified methodology grounded in approximate dynamic programming. With emphasis on approximation in value space, the text bridges concepts from control theory, operations research, and artificial intelligence. Suitable for graduate-level courses or self-study, it offers a modular structure, visual intuitions, and real-world insight into key methods such as rollout algorithms, policy iteration, and deep learning-based value approximation. This edition integrates contemporary advances and expands discussions on topics like multi-agent systems, adaptive control, and model predictive control, while remaining focused on rigorous yet intuitive explanations.">
<meta property="og:image" content="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg">
<meta property="og:site_name" content="Random Bits of Knowledge">
<meta name="twitter:title" content="A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas – Random Bits of Knowledge">
<meta name="twitter:description" content="A Course in Reinforcement Learning (2nd Edition) by Dimitri P. Bertsekas presents a mathematically grounded yet accessible framework for understanding and applying reinforcement learning (RL) to sequential decision-making problems. Centered on the duality between off-line training and on-line play, and inspired by systems like AlphaZero and TD-Gammon, the book explores a unified methodology grounded in approximate dynamic programming. With emphasis on approximation in value space, the text bridges concepts from control theory, operations research, and artificial intelligence. Suitable for graduate-level courses or self-study, it offers a modular structure, visual intuitions, and real-world insight into key methods such as rollout algorithms, policy iteration, and deep learning-based value approximation. This edition integrates contemporary advances and expands discussions on topics like multi-agent systems, adaptive control, and model predictive control, while remaining focused on rigorous yet intuitive explanations.">
<meta name="twitter:image" content="https://antomon.github.io/collections/fkposts/bertsekas-a-course-in-reinforcement-learning/Course-in-RL-2nd-ed.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../favicon.png" alt="AM logo" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Random Bits of Knowledge</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/services.html"> 
<span class="menu-text">Services</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-collections" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Collections</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-collections">    
        <li>
    <a class="dropdown-item" href="../../../collections/bookmarks-inspiration.html">
 <span class="dropdown-text">Bookmarks of Inspiration</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../collections/cabinet-digital-curiosities.html">
 <span class="dropdown-text">Cabinet of Digital Curiosities</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../collections/free-knowledge.html">
 <span class="dropdown-text">Free Knowledge</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://4m4.it/corso-python/"> 
<span class="menu-text">Corso Python</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/montano/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/antomon"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/antomon"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas</h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="5">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#review" id="toc-review" class="nav-link active" data-scroll-target="#review">Review</a>
  <ul class="collapse">
  <li><a href="#conceptual-focus-value-centric-reinforcement-learning" id="toc-conceptual-focus-value-centric-reinforcement-learning" class="nav-link" data-scroll-target="#conceptual-focus-value-centric-reinforcement-learning">Conceptual focus: value-centric reinforcement learning</a></li>
  <li><a href="#structure-and-content-overview" id="toc-structure-and-content-overview" class="nav-link" data-scroll-target="#structure-and-content-overview">Structure and content overview</a></li>
  <li><a href="#highlights-and-unique-strengths" id="toc-highlights-and-unique-strengths" class="nav-link" data-scroll-target="#highlights-and-unique-strengths">Highlights and unique strengths</a></li>
  <li><a href="#enduring-relevance-in-a-fast-evolving-field" id="toc-enduring-relevance-in-a-fast-evolving-field" class="nav-link" data-scroll-target="#enduring-relevance-in-a-fast-evolving-field">Enduring relevance in a fast-evolving field</a></li>
  <li><a href="#who-is-this-book-for" id="toc-who-is-this-book-for" class="nav-link" data-scroll-target="#who-is-this-book-for">Who is this book for?</a></li>
  <li><a href="#supplementary-materials" id="toc-supplementary-materials" class="nav-link" data-scroll-target="#supplementary-materials">Supplementary materials</a></li>
  <li><a href="#verdict" id="toc-verdict" class="nav-link" data-scroll-target="#verdict">Verdict</a></li>
  <li><a href="#further-readings" id="toc-further-readings" class="nav-link" data-scroll-target="#further-readings">Further readings</a></li>
  </ul></li>
  <li><a href="#about-the-author" id="toc-about-the-author" class="nav-link" data-scroll-target="#about-the-author">About the author</a></li>
  <li><a href="#info" id="toc-info" class="nav-link" data-scroll-target="#info">Info</a></li>
  <li><a href="#social-media" id="toc-social-media" class="nav-link" data-scroll-target="#social-media">Social media</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">A Course in Reinforcement Learning (2nd Edition) - Dimitri P. Bertsekas</h1>
<p class="subtitle lead">A deep dive into the math and meaning behind RL algorithms</p>
  <div class="quarto-categories">
    <div class="quarto-category">machine learning</div>
    <div class="quarto-category">🇬🇧</div>
  </div>
  </div>

<div>
  <div class="description">
    <em>A Course in Reinforcement Learning (2nd Edition)</em> by <strong>Dimitri P. Bertsekas</strong> presents a mathematically grounded yet accessible framework for understanding and applying reinforcement learning (RL) to sequential decision-making problems. Centered on the duality between off-line training and on-line play, and inspired by systems like AlphaZero and TD-Gammon, the book explores a unified methodology grounded in approximate dynamic programming. With emphasis on approximation in value space, the text bridges concepts from control theory, operations research, and artificial intelligence. Suitable for graduate-level courses or self-study, it offers a modular structure, visual intuitions, and real-world insight into key methods such as rollout algorithms, policy iteration, and deep learning-based value approximation. This edition integrates contemporary advances and expands discussions on topics like multi-agent systems, adaptive control, and model predictive control, while remaining focused on rigorous yet intuitive explanations.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Antonio Montano </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 19, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">April 19, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>



<div class="no-row-height column-margin column-container"><div class="">
<p><img src="Course-in-RL-2nd-ed.jpg" class="img-fluid"></p>
</div></div><section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Dimitri P. Bertsekas’ <strong>A Course in Reinforcement Learning</strong> (2nd Edition) is a deeply insightful and uniquely structured textbook that bridges the gap between classical optimization and the modern field of reinforcement learning (RL). Unlike most RL books that emerge from the machine learning community and focus heavily on empirical performance and implementation, Bertsekas approaches the subject from the lens of <strong>dynamic programming (DP)</strong>, <strong>control theory</strong>, and <strong>operations research</strong>, offering a mathematically grounded, algorithmically rigorous, and conceptually unified perspective.</p>
<section id="conceptual-focus-value-centric-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-focus-value-centric-reinforcement-learning">Conceptual focus: value-centric reinforcement learning</h3>
<p>The core philosophy of the book revolves around <strong>approximation in value space</strong>—a concept that plays a foundational role in dynamic programming and is extended here to RL settings. This is in contrast to the increasingly popular <strong>policy-gradient</strong> methods that dominate deep RL literature. Rather than focusing solely on training end-to-end policies with neural networks, Bertsekas places heavy emphasis on computing or approximating <strong>value functions</strong>, and using those to derive or improve policies.</p>
<p>This is a natural continuation of the ideas developed in his seminal works on <strong>Dynamic Programming and Optimal Control</strong>. It also reflects the underlying architecture of high-performing RL systems such as <strong>AlphaZero</strong> and <strong>TD-Gammon</strong>, both of which use trained evaluators in conjunction with powerful online search or rollout techniques. In fact, one of the book’s major contributions is to formalize and explain these empirical successes within a Newton-like optimization framework for solving Bellman’s equation.</p>
</section>
<section id="structure-and-content-overview" class="level3">
<h3 class="anchored" data-anchor-id="structure-and-content-overview">Structure and content overview</h3>
<p>The book is divided into <strong>three major chapters</strong>, designed to support both linear reading and modular, instructor-tailored course planning:</p>
<section id="chapter-1-exact-and-approximate-dynamic-programming" class="level4">
<h4 class="anchored" data-anchor-id="chapter-1-exact-and-approximate-dynamic-programming">Chapter 1: exact and approximate dynamic programming</h4>
<p>This foundational chapter provides a broad overview of the RL landscape, rooted in dynamic programming. It begins with deterministic and stochastic finite-horizon problems, develops into approximation methods, and presents the conceptual framework of <strong>offline training + online play</strong>, drawing detailed analogies with AlphaZero and model predictive control (MPC). It also introduces the reader to concepts like <strong>rollout</strong>, <strong>policy iteration</strong>, <strong>cost-to-go approximations</strong>, and <strong>terminal cost evaluation</strong>. The chapter is comprehensive and forms a standalone platform for readers unfamiliar with RL but versed in optimization.</p>
</section>
<section id="chapter-2-approximation-in-value-space-rollout-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="chapter-2-approximation-in-value-space-rollout-algorithms">Chapter 2: approximation in value space – rollout algorithms</h4>
<p>Here, Bertsekas deepens the discussion on value function-based methods. He details variants of <strong>rollout algorithms</strong>—methods that combine a base heuristic with lookahead or simulation to improve decisions in real-time. Topics include constrained optimization, Monte Carlo Tree Search (MCTS), randomized rollout, Bayesian optimization, and their application to deterministic and stochastic control, POMDPs, and even adversarial games. This chapter shows the versatility of the rollout paradigm across discrete and continuous domains.</p>
</section>
<section id="chapter-3-learning-values-and-policies" class="level4">
<h4 class="anchored" data-anchor-id="chapter-3-learning-values-and-policies">Chapter 3: learning values and policies</h4>
<p>This is where the book engages with <strong>neural networks</strong> and other parametric function approximators. It examines how value functions and policies can be learned from data using <strong>fitted value iteration</strong>, <strong>Q-learning</strong>, <strong>SARSA</strong>, and <strong>policy gradient methods</strong>. While coverage of policy optimization is relatively lean compared to deep RL books like Sutton &amp; Barto’s <em>Reinforcement Learning</em>, it is sufficient to understand core ideas and their integration into the Newton-based value approximation framework. The focus remains on conceptual clarity and practical convergence issues.</p>
<p>Each chapter ends with <strong>detailed notes, sources, and exercises</strong>, often pointing to supplementary material in Bertsekas’ other books, such as <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em> (2020) or <em>Lessons from AlphaZero</em> (2022). These references make the book an excellent launchpad for research or deeper specialization.</p>
</section>
</section>
<section id="highlights-and-unique-strengths" class="level3">
<h3 class="anchored" data-anchor-id="highlights-and-unique-strengths">Highlights and unique strengths</h3>
<p>What sets <em>A Course in Reinforcement Learning</em> apart is its rare synthesis of intellectual depth, conceptual clarity, and practical relevance. Bertsekas leverages <strong>decades of foundational work in optimization theory and dynamic programming</strong>, crafting a perspective on RL that feels both timeless and sharply attuned to contemporary developments like AlphaZero. Rather than presenting RL as a bag of tricks or an empirical race to outperform benchmarks, the book offers a <strong>structured, principled framework</strong> for thinking about sequential decision-making under uncertainty.</p>
<p>A central strength lies in the <strong>elegant articulation of the synergy between offline learning and online planning</strong>. Instead of treating training as a static preprocessing step, Bertsekas shows how real-time control and model predictive strategies can interact meaningfully with learned approximations—a conceptual bridge that is both underexplored and urgently needed in modern RL discourse. This also gives the book a unique relevance to high-stakes engineering domains where stability, safety, and interpretability matter.</p>
<p>Another distinguishing trait is the author’s ability to <strong>speak fluently across disciplinary boundaries</strong>. Readers from control theory, operations research, artificial intelligence, and applied mathematics will all find familiar anchors—but also be challenged to expand their mental models. The book avoids dense formalism without sacrificing rigor, preferring <strong>geometric insights, intuitive visualizations, and algorithmic thinking</strong> to heavy abstraction. This makes it an accessible yet intellectually satisfying read for those seeking more than surface-level understanding.</p>
<p>Moreover, the text is <strong>modular and customizable</strong>, making it ideal for various course structures—from short introductory classes to advanced research seminars on RL theory. Each chapter is self-contained yet richly interlinked, allowing instructors or self-learners to navigate the material according to their background and goals.</p>
</section>
<section id="enduring-relevance-in-a-fast-evolving-field" class="level3">
<h3 class="anchored" data-anchor-id="enduring-relevance-in-a-fast-evolving-field">Enduring relevance in a fast-evolving field</h3>
<p>As machine learning matures beyond supervised pattern recognition, the methods described in <em>A Course in Reinforcement Learning</em> are becoming increasingly vital. Bertsekas’ emphasis on <strong>approximation in value space</strong>, <strong>policy iteration</strong>, and <strong>offline-online synergy</strong> aligns directly with the architecture of some of the most successful and well-known AI systems to date—<strong>AlphaZero</strong>, <strong>MuZero</strong>, and <strong>ChatGPT’s planning-inspired extensions</strong>. For example, MuZero (Schrittwieser et al., 2020) generalizes AlphaZero by learning its own model dynamics, yet it retains the <strong>value-based planning loop</strong> that Bertsekas formalizes through Newton-like updates on Bellman’s equation. Similarly, the resurgence of <strong>model-based reinforcement learning</strong> (e.g., DreamerV3, EfficientZero) is built on the same principle: learning value estimates offline and refining them online via planning—precisely the synergy this book explores in depth.</p>
<p>In addition, current research on <strong>LLMs with planning capabilities</strong> (e.g., ReAct, Tree of Thoughts, and OpenAI’s work on tool-use agents) echoes the structure of <strong>lookahead planning guided by learned evaluators</strong>, another core theme in the book. These methods increasingly blend <strong>rollout-based reasoning</strong>, <strong>value estimation</strong>, and <strong>policy improvement</strong>—even if not framed as reinforcement learning per se. Likewise, in robotics and safety-critical applications, algorithms must <strong>generalize reliably, adapt to perturbations</strong>, and <strong>provide interpretable decision-making</strong>—objectives far better served by Bertsekas’ structured, optimization-rooted methods than by end-to-end neural policy training alone.</p>
<p>Thus, what this book presents is not a retrospective—it is a <strong>forward-looking foundation</strong>. As machine learning turns toward <strong>long-horizon reasoning, planning under uncertainty, and adaptive control</strong>, the concepts in Bertsekas’ work are proving to be not only relevant, but essential.</p>
</section>
<section id="who-is-this-book-for" class="level3">
<h3 class="anchored" data-anchor-id="who-is-this-book-for">Who is this book for?</h3>
<p>This textbook is not for everyone—and that’s a strength, not a weakness.</p>
<ul>
<li>Ideal readers include:
<ul>
<li>Graduate students in <strong>electrical engineering</strong>, <strong>applied mathematics</strong>, <strong>operations research</strong>, or <strong>control systems</strong>.</li>
<li>Researchers and professionals interested in <strong>optimization</strong>, <strong>model predictive control</strong>, <strong>robotics</strong>, or <strong>multiagent systems</strong>.</li>
<li>Advanced undergraduates with a solid mathematical background and interest in decision-making under uncertainty.</li>
<li>Practitioners who have experience with algorithms and modeling, and want to understand the “why” behind reinforcement learning, not just the “how”.</li>
</ul></li>
<li>Less ideal for:
<ul>
<li>Beginners looking for an introductory, code-heavy book. For this, <em>Reinforcement Learning: An Introduction</em> by Sutton &amp; Barto or <em>Deep Reinforcement Learning Hands-On</em> by Maxim Lapan may be more approachable.</li>
<li>Readers wanting a focus on PyTorch/TensorFlow, environment setups, or software engineering best practices for RL pipelines.</li>
</ul></li>
</ul>
<p>Instead, Bertsekas offers something increasingly rare: a textbook that <strong>doesn’t treat reinforcement learning as a subfield of deep learning</strong>, but rather as a <strong>mathematically grounded discipline</strong> that can integrate with—but also stand apart from—modern AI trends.</p>
</section>
<section id="supplementary-materials" class="level3">
<h3 class="anchored" data-anchor-id="supplementary-materials">Supplementary materials</h3>
<p>The book is accompanied by <strong>video lectures, slides</strong>, and supplemental content from Bertsekas’ Arizona State University course, accessible via his website. This makes the book particularly useful for <strong>self-study</strong>, especially for learners who appreciate visual explanation and conceptual repetition across formats.</p>
</section>
<section id="verdict" class="level3">
<h3 class="anchored" data-anchor-id="verdict">Verdict</h3>
<p><em>A Course in Reinforcement Learning (2nd Edition)</em> is a rigorous, insightful, and conceptually rich text that helps readers <strong>understand reinforcement learning as a structured decision-making framework</strong>, not just a toolbox of tricks. If you’re serious about applying RL to real-world, high-stakes systems—where stability, interpretability, and theoretical guarantees matter—this is the book you’ve been looking for.</p>
<p>It challenges, rewards, and broadens the reader’s view of what reinforcement learning is and what it can be. Highly recommended for those looking to <strong>build durable understanding</strong>, not just quick implementations.</p>
</section>
<section id="further-readings" class="level3">
<h3 class="anchored" data-anchor-id="further-readings">Further readings</h3>
<ul>
<li><p>Bertsekas, D. P. (2022). <em>Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control</em>. Athena Scientific. <a href="https://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf">Download from author website</a></p></li>
<li><p>Bertsekas, D. P. (2020). <em>Rollout, Policy Iteration, and Distributed Reinforcement Learning</em>. Athena Scientific. <a href="http://www.athenasc.com/rolloutbook_athena.html">More information</a></p></li>
<li><p>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., … &amp; Silver, D. (2020). <em>Mastering Atari, Go, Chess and Shogi by planning with a learned model</em>. Nature, 588(7839), 604–609. <a href="https://www.nature.com/articles/s41586-020-03051-4">Read the paper</a></p></li>
<li><p>Hafner, D., Lillicrap, T., Norouzi, M., &amp; Ba, J. (2023). <em>Mastering diverse domains through world models</em>. arXiv preprint arXiv:2301.04104. <a href="https://arxiv.org/abs/2301.04104">Read the paper</a></p></li>
<li><p>Ye, J., Lin, G., Xu, H., Liao, R., Yang, E., Lu, H., … &amp; Liu, S. (2021). <em>Mastering Atari Games with Limited Data</em>. Advances in Neural Information Processing Systems, 34. <a href="https://arxiv.org/abs/2111.00210">Read the paper</a></p></li>
<li><p>Yao, S., Zhao, J., Yu, D., Narasimhan, K., &amp; Zhang, Y. (2023). <em>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</em>. arXiv preprint arXiv:2305.10601. <a href="https://arxiv.org/abs/2305.10601">Read the paper</a></p></li>
<li><p>Yao, S., Zhao, J., Yu, D., Kasai, J., Wong, A., &amp; Zhang, Y. (2022). <em>ReAct: Synergizing Reasoning and Acting in Language Models</em>. arXiv preprint arXiv:2210.03629. <a href="https://arxiv.org/abs/2210.03629">Read the paper</a></p></li>
<li><p>Aghzal, M., Plaku, E., Stein, G. J., &amp; Yao, Z. (2025). <em>A Survey on Large Language Models for Automated Planning</em>. arXiv preprint arXiv:2502.12435. <a href="https://arxiv.org/abs/2502.12435">Read the paper</a></p></li>
<li><p>Tantakoun, M., Zhu, X., &amp; Muise, C. (2025). <em>LLMs as Planning Modelers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models</em>. arXiv preprint arXiv:2503.18971. <a href="https://arxiv.org/abs/2503.18971">Read the paper</a></p></li>
<li><p>Li, H., Chen, Z., Zhang, J., &amp; Liu, F. (2024). <em>LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning</em>. arXiv preprint arXiv:2409.01806. <a href="https://arxiv.org/abs/2409.01806">Read the paper</a></p></li>
<li><p>Changle, Q., et al.&nbsp;(2024). <em>Tool Learning with Large Language Models: A Survey</em>. Frontiers of Computer Science. <a href="https://arxiv.org/abs/2405.17935">Read the paper</a></p></li>
</ul>
</section>
</section>
<section id="about-the-author" class="level2">
<h2 class="anchored" data-anchor-id="about-the-author">About the author</h2>
<p>Dimitri P. Bertsekas is a world-renowned scholar in optimization and control theory. He received his Ph.D.&nbsp;in system science from MIT and has held faculty positions at Stanford, the University of Illinois, and MIT, where he remains McAfee Professor of Engineering. Since 2019, he has been Fulton Professor of Computational Decision Making at Arizona State University.</p>
<p>Over a prolific academic career, Bertsekas has authored over twenty influential books covering topics from nonlinear programming to data networks, and notably, dynamic programming and reinforcement learning. His contributions have earned him numerous accolades, including the IEEE Control Systems Award, INFORMS John von Neumann Theory Prize, and election to the U.S. National Academy of Engineering. His recent focus on RL reflects decades of foundational work in dynamic programming and optimization, making him a key figure in bridging classical control with modern machine learning.</p>
</section>
<section id="info" class="level2">
<h2 class="anchored" data-anchor-id="info">Info</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Subject</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Title</strong></td>
<td>A Course in Reinforcement Learning</td>
</tr>
<tr class="even">
<td><strong>Year</strong></td>
<td>2025 (2nd edition)</td>
</tr>
<tr class="odd">
<td><strong>Authors</strong></td>
<td><a href="https://www.mit.edu/~dimitrib/home.html">Dimitri P. Bertsekas</a></td>
</tr>
<tr class="even">
<td><strong>Publisher</strong></td>
<td><a href="http://www.athenasc.com/">Athena Scientific</a></td>
</tr>
<tr class="odd">
<td><strong>Language</strong></td>
<td>English</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>Reinforcement learning, Model predictive control, Dynamics programming, Machine learning</td>
</tr>
<tr class="odd">
<td><strong>Downloads</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE%202ndEDITION.pdf">Book PDF</a></td>
</tr>
<tr class="even">
<td><strong>Other links</strong></td>
<td><a href="https://web.mit.edu/dimitrib/www/RLbook.html">Course videolectures and materials</a></td>
</tr>
<tr class="odd">
<td><strong>ISBN/DOI</strong></td>
<td>1-886529-29-9</td>
</tr>
<tr class="even">
<td><strong>Buy online</strong></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="social-media" class="level2">
<h2 class="anchored" data-anchor-id="social-media">Social media</h2>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I am pleased to share a review of my book "A course in reinforcement learning" (2nd edition) <a href="https://t.co/VkRcIl6TB2">https://t.co/VkRcIl6TB2</a><br>This is the textbook for my RL course at ASU (free PDF at <a href="https://t.co/rB9CcChOHS">https://t.co/rB9CcChOHS</a>)<a href="https://twitter.com/hashtag/reinforcementlearning?src=hash&amp;ref_src=twsrc%5Etfw">#reinforcementlearning</a> <a href="https://twitter.com/hashtag/machinelearning?src=hash&amp;ref_src=twsrc%5Etfw">#machinelearning</a>
</p>
— Dimitri Bertsekas (<span class="citation" data-cites="DBertsekas">@DBertsekas</span>) <a href="https://twitter.com/DBertsekas/status/1913627106552209493?ref_src=twsrc%5Etfw">April 19, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/antomon\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Antonio Montano’s Personal Website</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../contents/services.html">
<p>Services</p>
</a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/montano/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/antomon">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/antomon">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 © Antonio Montano, 2022-2025
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>