<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Antonio Montano">
<meta name="dcterms.date" content="2022-04-19">

<title>Harnessing Focus: Merging AI and Market Dynamics ‚Äì Antonio Montano‚Äôs Personal Website</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=663ff7b280d7c0001914e592&amp;product=sticky-share-buttons" async="async"></script>
<script src="https://cdn.jsdelivr.net/npm/typewriter-effect@latest/dist/core.js"></script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Harnessing Focus: Merging AI and Market Dynamics ‚Äì Antonio Montano‚Äôs Personal Website">
<meta property="og:description" content="The attention paradigm: bridging natural language processing and economic theory">
<meta property="og:image" content="https://antomon.github.io/posts/attentions/attentions.webp">
<meta property="og:site_name" content="Antonio Montano's Personal Website">
<meta name="twitter:title" content="Harnessing Focus: Merging AI and Market Dynamics ‚Äì Antonio Montano‚Äôs Personal Website">
<meta name="twitter:description" content="The attention paradigm: bridging natural language processing and economic theory">
<meta name="twitter:image" content="https://antomon.github.io/posts/attentions/attentions.webp">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../favicon.png" alt="AM logo" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Antonio Montano‚Äôs Personal Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../contents/services.html"> 
<span class="menu-text">Services</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://4m4.it/corso-python/"> 
<span class="menu-text">Corso Python</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/montano/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/antomon"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/antomon"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Posts by language</li><li class="breadcrumb-item"><a href="../../posts/category-theory-functional-programming/index.html">üá¨üáß</a></li><li class="breadcrumb-item"><a href="../../posts/attentions/index.html">Harnessing Focus: Merging AI and Market Dynamics</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Posts by language</li><li class="breadcrumb-item"><a href="../../posts/category-theory-functional-programming/index.html">üá¨üáß</a></li><li class="breadcrumb-item"><a href="../../posts/attentions/index.html">Harnessing Focus: Merging AI and Market Dynamics</a></li></ol></nav>
      <h1 class="title">Harnessing Focus: Merging AI and Market Dynamics</h1>
            <p class="subtitle lead">The attention paradigm: bridging natural language processing and economic theory</p>
                                <div class="quarto-categories">
                <div class="quarto-category">machine learning</div>
                <div class="quarto-category">essay</div>
                <div class="quarto-category">üá¨üáß</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Antonio Montano </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 19, 2022</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">July 31, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Posts by language</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">üá¨üáß</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/category-theory-functional-programming/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Relationship Between Category Theory, Lambda Calculus, and Functional Programming in Haskell</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/ai-important-as-fire/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI important as fire, generative AI as printing press, autonomous agents as wheel?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/color-space-sampling-101/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Color Space Sampling 101</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/attentions/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Harnessing Focus: Merging AI and Market Dynamics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/gpt-4-anniversary/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT-4 Anniversary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/intervento-dabs-day-2024-ca-foscari/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intervento al DABS Day 2024 - Universit√† Ca‚Äô Foscari</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">üáÆüáπ</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/category-theory-functional-programming/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Relationship Between Category Theory, Lambda Calculus, and Functional Programming in Haskell</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/ai-important-as-fire/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI important as fire, generative AI as printing press, autonomous agents as wheel?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/color-space-sampling-101/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Color Space Sampling 101</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/attentions/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Harnessing Focus: Merging AI and Market Dynamics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/gpt-4-anniversary/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT-4 Anniversary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/intervento-dabs-day-2024-ca-foscari/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intervento al DABS Day 2024 - Universit√† Ca‚Äô Foscari</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">All posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/category-theory-functional-programming/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Relationship Between Category Theory, Lambda Calculus, and Functional Programming in Haskell</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/ai-important-as-fire/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI important as fire, generative AI as printing press, autonomous agents as wheel?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/color-space-sampling-101/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Color Space Sampling 101</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/attentions/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Harnessing Focus: Merging AI and Market Dynamics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/gpt-4-anniversary/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT-4 Anniversary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/intervento-dabs-day-2024-ca-foscari/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intervento al DABS Day 2024 - Universit√† Ca‚Äô Foscari</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="3">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#prologue" id="toc-prologue" class="nav-link active" data-scroll-target="#prologue">Prologue</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#attention-mechanisms" id="toc-attention-mechanisms" class="nav-link" data-scroll-target="#attention-mechanisms">Attention mechanisms</a>
  <ul class="collapse">
  <li><a href="#historical-development" id="toc-historical-development" class="nav-link" data-scroll-target="#historical-development">Historical development</a></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications">Applications</a></li>
  <li><a href="#technical-details" id="toc-technical-details" class="nav-link" data-scroll-target="#technical-details">Technical details</a></li>
  <li><a href="#novelty-and-success" id="toc-novelty-and-success" class="nav-link" data-scroll-target="#novelty-and-success">Novelty and success</a></li>
  </ul></li>
  <li><a href="#attention-economics" id="toc-attention-economics" class="nav-link" data-scroll-target="#attention-economics">Attention economics</a>
  <ul class="collapse">
  <li><a href="#definition-and-core-principles" id="toc-definition-and-core-principles" class="nav-link" data-scroll-target="#definition-and-core-principles">Definition and core principles</a></li>
  <li><a href="#historical-context" id="toc-historical-context" class="nav-link" data-scroll-target="#historical-context">Historical context</a></li>
  <li><a href="#cognitive-basis" id="toc-cognitive-basis" class="nav-link" data-scroll-target="#cognitive-basis">Cognitive basis</a></li>
  <li><a href="#applications-1" id="toc-applications-1" class="nav-link" data-scroll-target="#applications-1">Applications</a></li>
  </ul></li>
  <li><a href="#bridging-nlp-and-attention-economics" id="toc-bridging-nlp-and-attention-economics" class="nav-link" data-scroll-target="#bridging-nlp-and-attention-economics">Bridging NLP and attention economics</a>
  <ul class="collapse">
  <li><a href="#conceptual-overlap" id="toc-conceptual-overlap" class="nav-link" data-scroll-target="#conceptual-overlap">Conceptual overlap</a></li>
  <li><a href="#improving-attention-mechanisms" id="toc-improving-attention-mechanisms" class="nav-link" data-scroll-target="#improving-attention-mechanisms">Improving attention mechanisms</a></li>
  <li><a href="#practical-implications" id="toc-practical-implications" class="nav-link" data-scroll-target="#practical-implications">Practical implications</a></li>
  <li><a href="#adversarial-implications" id="toc-adversarial-implications" class="nav-link" data-scroll-target="#adversarial-implications">Adversarial implications</a></li>
  <li><a href="#future-research-directions" id="toc-future-research-directions" class="nav-link" data-scroll-target="#future-research-directions">Future research directions</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="prologue" class="level2">
<h2 class="anchored" data-anchor-id="prologue">Prologue</h2>
<p>In his seminal work, <em>The Principles of Psychology</em>, William James profoundly observed, ‚ÄúMy experience is what I agree to attend to. Only those items which I notice shape my mind‚Äîwithout selective interest, experience is an utter chaos‚Äù (James, 1890). This statement encapsulates the essence of how attention shapes our reality. Our selective focus not only filters the overwhelming influx of information but also constructs the very framework of our knowledge and experience. This insight forms the bedrock of my exploration into the relationship between attention mechanisms in natural language processing (NLP) and attention economics.</p>
<p>The act of attending is more than just a cognitive process; it is a fundamental determinant of how we perceive, interpret, and interact with the world. James‚Äôs reflection on attention reveals that our conscious experience is a curated narrative, constructed from the myriad stimuli we choose to acknowledge. This selective process is crucial not only in shaping individual cognition but also in driving the collective knowledge within various fields.</p>
<p>This essay is born out of my fascination with how such a seemingly simple concept‚Äîthe act of paying attention‚Äîcan bridge two ostensibly disparate domains: the technical intricacies of NLP and the economic principles governing human focus. Both fields, though distinct in their methodologies and applications, fundamentally rely on the efficient allocation of attention. Whether it is an AI model sifting through vast datasets to find relevance or an economist studying how people allocate their cognitive resources, the underlying principle remains the same: our attention is the gatekeeper of our experience and knowledge.</p>
<p>By exploring these connections, I aim to uncover how advancements in understanding attention can enrich both artificial intelligence and economic theories, ultimately enhancing our ability to manage and utilize information in an era of unprecedented data abundance. This journey through the intersections of cognitive science, technology, and economics underscores a personal quest to understand how the meticulous act of attending shapes not just individual minds, but the collective progression of human knowledge.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In an era characterized by information overload, the concept of <strong>attention</strong> has gained paramount importance across various disciplines. From cognitive science to computer engineering and economics, the mechanisms of focusing on relevant information while filtering out the irrelevant have become a central area of study. This essay explores the fascinating parallel between attention mechanisms in natural language processing (NLP) and the theory of attention economics, two seemingly disparate fields that share a common foundation in the management of information resources.</p>
<p>Attention, in cognitive science, refers to the mental process of selectively concentrating on specific aspects of the environment while ignoring others. This fundamental cognitive ability has inspired the development of attention mechanisms in NLP, i.e., computational models that allow artificial systems to focus on the most relevant parts of input data. Concurrently, in the realm of economics, a novel approach known as <strong>attention economics</strong> has emerged, treating human attention as a scarce and valuable commodity in an information-rich world (Davenport &amp; Beck, 2001).</p>
<p>The parallel development of attention mechanisms in NLP and the theory of attention economics offers profound insights into both human cognition and artificial intelligence, with far-reaching implications for information management and technology design. This essay aims to explore these connections, highlighting how the attention paradigm serves as a bridge between computational models and economic theory, potentially reshaping our understanding of information processing in both human and artificial systems.</p>
</section>
<section id="attention-mechanisms" class="level2">
<h2 class="anchored" data-anchor-id="attention-mechanisms">Attention mechanisms</h2>
<p>Attention mechanisms in NLP are sophisticated computational techniques that allow AI models to dynamically focus on specific parts of the input data when performing language-related tasks. Inspired by human cognitive processes, these mechanisms enable AI systems to assign varying levels of importance, or ‚Äúattention weights,‚Äù to different elements in a sequence, typically words or phrases in a sentence.</p>
<p>The core principle behind attention mechanisms is the ability to weigh the relevance of different input elements contextually. This allows the model to prioritize important information and de-emphasize less relevant details, leading to improved performance across various language tasks (Vaswani et al., 2017). Attention mechanisms work by creating query, key, and value representations of the input data. The model then calculates attention scores by comparing the query with the keys and uses these scores to weigh the values. This process allows the model to focus on different parts of the input with varying intensity, mimicking the way humans selectively focus on certain aspects of information while processing language.</p>
<section id="historical-development" class="level3">
<h3 class="anchored" data-anchor-id="historical-development">Historical development</h3>
<p>The concept of attention in NLP emerged as a solution to the limitations of traditional sequence-to-sequence models, particularly in machine translation. In 2014, Bahdanau et al.&nbsp;introduced the first attention mechanism in their seminal paper ‚ÄúNeural Machine Translation by Jointly Learning to Align and Translate‚Äù (Bahdanau et al., 2014). This breakthrough allowed models to selectively focus on parts of the source sentence while generating each word of the translation, significantly improving translation quality.</p>
<p>The evolution of attention mechanisms accelerated rapidly after this initial breakthrough. In 2015, Xu et al.&nbsp;introduced the concept of ‚Äúsoft‚Äù and ‚Äúhard‚Äù attention in the context of image captioning, further expanding the applicability of attention mechanisms. Soft attention allows the model to consider all parts of the input with varying weights, while hard attention focuses on specific parts of the input with discrete choices.</p>
<p>The year 2017 marked a significant milestone with the introduction of the Transformer model by Vaswani et al.&nbsp;in their paper ‚ÄúAttention Is All You Need‚Äù (Vaswani et al., 2017). This model relied entirely on attention mechanisms without using recurrent or convolutional layers, demonstrating unprecedented efficiency and performance in various NLP tasks. The Transformer‚Äôs use of self-attention and multi-head attention enabled parallel processing of inputs and capturing long-range dependencies, setting a new standard for NLP models.</p>
<p>The success of the Transformer architecture led to the development of powerful pre-trained language models such as BERT (Bidirectional Encoder Representations from Transformers) by Devlin et al.&nbsp;in 2018 and GPT (Generative Pre-trained Transformer) by OpenAI. BERT introduced bidirectional attention, allowing the model to consider the context from both directions, which significantly improved tasks like question answering and named entity recognition. GPT focused on unidirectional generative tasks, excelling in text generation and language modeling.</p>
<p>Recent developments have continued to build on these foundations. Models like T5 (Text-to-Text Transfer Transformer) unified various NLP tasks into a single framework, and Retrieval-Augmented Generation (RAG) combined attention mechanisms with retrieval systems, enabling models to access and integrate external knowledge dynamically. These advancements have further solidified the importance of attention mechanisms in modern NLP.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    classDef default fill:#ffffff,stroke:#0000ff,stroke-width:2px,color:#000000,font-weight:bold
    linkStyle default stroke:#0000ff,stroke-width:2px

    A[Attention Mechanisms] --&gt; B[Sequence-to-Sequence Models]
    B --&gt; C[Machine Translation]
    C --&gt; D[Neural Machine Translation by Bahdanau et al., 2014]
    D --&gt; E[Soft and Hard Attention by Xu et al., 2015]
    E --&gt; F[Transformer Model by Vaswani et al., 2017]
    F --&gt; G[BERT by Devlin et al., 2018]
    F --&gt; H[GPT by OpenAI, 2018]
    G --&gt; I[Bidirectional Attention]
    H --&gt; J[Unidirectional Generation]
    I --&gt; K[Improved Question Answering]
    I --&gt; L[Enhanced Named Entity Recognition]
    J --&gt; M[Advanced Text Generation]
    F --&gt; N[T5]
    N --&gt; O[Unified NLP Framework]
    F --&gt; P[RAG]
    P --&gt; Q[Dynamic External Knowledge Integration]
</pre>
</div>
<p></p><figcaption> Historical development of attention mechanisms</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="applications" class="level3">
<h3 class="anchored" data-anchor-id="applications">Applications</h3>
<p>Attention mechanisms have found widespread applications across numerous NLP tasks, revolutionizing performance throughout the field. In machine translation, these mechanisms have been particularly transformative. They allow models to focus on relevant words in the source language when generating each word in the target language, significantly improving the fluency and accuracy of translations (Bahdanau et al., 2014). This capability is especially valuable when dealing with languages that have different word orders, as the model can dynamically align relevant parts of the input and output sequences.</p>
<p>Text summarization has also benefited greatly from attention mechanisms. Models equipped with these mechanisms can identify and focus on the most important sentences or phrases in a document, enabling the creation of more coherent and informative summaries. This ability to distill the essence of longer texts into concise summaries has proven invaluable in various applications, from news aggregation to academic research.</p>
<p>In the realm of question answering, attention mechanisms have led to more sophisticated and context-aware systems. These models can efficiently locate and focus on relevant information within a given text to answer specific questions. This has resulted in more accurate and nuanced responses, as the model can weigh the importance of different parts of the input text in relation to the question at hand (Devlin et al., 2018).</p>
<p>Sentiment analysis has seen significant improvements with the introduction of attention mechanisms. Models can now focus on words or phrases that are most indicative of sentiment, leading to more accurate classification of the overall sentiment expressed in a piece of text. This enhanced capability has found applications in areas such as social media monitoring, customer feedback analysis, and market research.</p>
<p>Speech recognition systems have also leveraged attention mechanisms to great effect. These mechanisms help align audio signals with text transcriptions, enhancing the accuracy of speech-to-text systems. This has led to more robust and reliable voice recognition technologies, improving user experiences in applications ranging from virtual assistants to transcription services.</p>
<p>In the field of named entity recognition, attention mechanisms have proven invaluable. They allow models to better identify and classify named entities by focusing on contextual cues, leading to more accurate extraction of important information such as names, organizations, and locations from unstructured text (Devlin et al., 2018).</p>
<p>Text generation tasks, including story generation and conversational AI, have been revolutionized by attention mechanisms. These mechanisms help models maintain coherence and context over long sequences of text, resulting in more natural and contextually appropriate generated content. This has led to significant advancements in chatbots, creative writing assistance, and other generative language tasks (Brown et al., 2020).</p>
<p>Moreover, attention mechanisms have found applications in document classification, where they help models focus on the most relevant parts of long documents to determine their category or topic. In machine reading comprehension, these mechanisms enable models to better understand and reason about complex passages of text, leading to more human-like comprehension abilities.</p>
<p>The versatility of attention mechanisms has also led to their adoption in multimodal tasks that combine language with other forms of data. For instance, in image captioning, attention allows models to focus on relevant parts of an image while generating descriptive text. Similarly, in video understanding tasks, attention mechanisms help models align textual descriptions or questions with relevant frames or segments of video.</p>
<p>As research in NLP continues to advance, the applications of attention mechanisms continue to expand, touching virtually every aspect of language processing and understanding. Their ability to dynamically focus on relevant information has made them a fundamental component in the ongoing quest to create more intelligent and human-like language processing systems.</p>
</section>
<section id="technical-details" class="level3">
<h3 class="anchored" data-anchor-id="technical-details">Technical details</h3>
<p>The development of various attention models has been driven by the need to address specific limitations of preceding models and to enhance the capabilities of NLP systems. Each type of attention mechanism builds on previous concepts, offering improvements and specialized functionalities for different tasks.</p>
<p>Self-Attention, also known as scaled dot-product attention, was a major innovation introduced in the Transformer paper by Vaswani et al.&nbsp;(2017). Self-Attention allows a model to consider the relationships between all words in a sentence, regardless of their position. It works by assigning importance scores to each word in relation to every other word.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    classDef default fill:#ffffff,stroke:#0000ff,stroke-width:2px,color:#000000,font-weight:bold
    linkStyle default stroke:#0000ff,stroke-width:2px

    A[Input Sequence] --&gt; B[Query]
    A --&gt; C[Key]
    A --&gt; D[Value]
    B --&gt; E[Attention Scores]
    C --&gt; E
    E --&gt; F[Weighted Sum]
    D --&gt; F
    F --&gt; G[Output]
</pre>
</div>
<p></p><figcaption> Multi-Head Attention mechanism</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>In this process, each word generates a query, key, and value. The query of each word is compared with the keys of all words to produce attention scores, which are then used to create a weighted sum of the values. Self-Attention captures long-range dependencies effectively and allows parallel processing, leading to faster training times. It also provides interpretability through attention weights. However, it is computationally expensive for very long sequences due to quadratic scaling with sequence length and requires large amounts of data and compute resources.</p>
<p>To enhance the model‚Äôs capacity to learn different aspects of relationships between words, Multi-Head Attention was introduced in the same Transformer paper. Multi-Head Attention extends the idea of self-attention by performing multiple self-attention operations in parallel. Each ‚Äúhead‚Äù can focus on different aspects of the relationship between words, such as grammar, semantics, or context. The results from all heads are then combined to produce the final output.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    classDef default fill:#ffffff,stroke:#0000ff,stroke-width:2px,color:#000000,font-weight:bold
    linkStyle default stroke:#0000ff,stroke-width:2px

    A[Input] --&gt; B[Head 1]
    A --&gt; C[Head 2]
    A --&gt; D[Head 3]
    B --&gt; E[Combine]
    C --&gt; E
    D --&gt; E
    E --&gt; F[Output]
</pre>
</div>
<p></p><figcaption> Cross-Head Attention mechanism</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>Multi-Head Attention enhances the model‚Äôs ability to focus on different types of relationships simultaneously, improving its robustness and flexibility, and increasing its representational capacity (Vaswani et al., 2017). However, it is more computationally intensive due to multiple attention heads and has higher memory consumption, requiring more hardware resources.</p>
<p>Cross-Attention, another key mechanism introduced in the Transformer paper, is used in the encoder-decoder structure of the Transformer. It is crucial in tasks that involve translating from one sequence to another, such as in machine translation. Cross-Attention allows the model to focus on relevant parts of the input sequence (from the encoder) when generating each word of the output sequence (in the decoder).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    classDef default fill:#ffffff,stroke:#0000ff,stroke-width:2px,color:#000000,font-weight:bold
    linkStyle default stroke:#0000ff,stroke-width:2px

    A[Input Sequence] --&gt; B[Encoder]
    B --&gt; C[Cross-Attention]
    D[Output So Far] --&gt; E[Decoder]
    E --&gt; C
    C --&gt; F[Next Output Word]
</pre>
</div>
<p></p><figcaption> Sparse Attention mechanism</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>Cross-Attention enables effective mapping between different sequences, improving translation quality and facilitating the handling of alignment in sequence-to-sequence tasks. However, its complexity increases with the length of input and output sequences, requiring significant computational resources for large-scale translations.</p>
<p>To efficiently handle very long sequences, Sparse Attention was introduced by Child et al.&nbsp;(2019) as an improvement upon Self-Attention. Sparse Attention reduces the number of word pairs considered, focusing instead on a strategic subset. This can be based on proximity (attending to nearby words), fixed patterns (attending to every nth word), or learned patterns of importance. Sparse Attention reduces computational load, making it feasible to handle very long sequences while maintaining the ability to capture essential dependencies with fewer computations. However, it may miss some important relationships if the sparsity pattern is not well-chosen and can be complex to implement and optimize effectively.</p>
<p>These attention mechanisms have dramatically enhanced the ability of NLP models to understand and generate language. By allowing models to dynamically focus on relevant information and capture complex relationships within data, attention mechanisms have become fundamental to modern NLP architectures. They enable models to better grasp context, handle long-range dependencies, and produce more coherent and contextually appropriate outputs across a wide range of language tasks.</p>
</section>
<section id="novelty-and-success" class="level3">
<h3 class="anchored" data-anchor-id="novelty-and-success">Novelty and success</h3>
<p>The introduction of attention mechanisms marked a significant paradigm shift in NLP. Their novelty lies in several key aspects. Unlike previous models that processed all input elements equally, attention mechanisms allow models to dynamically focus on relevant parts of the input. This mimics human cognitive processes more closely, as we naturally focus on specific words or phrases when understanding or translating language (Vaswani et al., 2017). Additionally, attention mechanisms, especially in models like the Transformer, allow for parallel processing of input sequences, in contrast to recurrent neural networks (RNNs) that process inputs sequentially. This parallelization was made possible by advancements in hardware, particularly GPUs and TPUs, which significantly accelerated the training and inference processes. The synergy between attention mechanisms and modern hardware has been crucial in handling the large-scale computations required by models like GPT-3. Moreover, attention allows models to capture relationships between words regardless of their distance in the input sequence, addressing a major limitation of RNNs and convolutional neural networks (CNNs). Furthermore, the attention weights provide a degree of interpretability, allowing researchers to visualize which parts of the input the model is focusing on for each output.</p>
<p>Attention mechanisms added several critical capabilities to NLP that were present in earlier models but lacked the success seen with GPT. For instance, traditional sequence-to-sequence models struggled with maintaining context over long texts, often leading to loss of important information. The introduction of the Transformer architecture was a game-changer. Transformers, leveraging self-attention mechanisms, efficiently handled long-range dependencies and context, a task that RNNs and LSTMs found challenging.</p>
<p>The success of attention mechanisms can be attributed to several factors. Attention-based models consistently outperform previous state-of-the-art models across a wide range of NLP tasks, from machine translation to text summarization. For example, BERT (Devlin et al., 2018) and GPT-3 (Brown et al., 2020) have set new benchmarks in numerous NLP tasks. The ability to process inputs in parallel allows attention-based models to scale efficiently to larger datasets and more complex tasks. The use of multi-head attention in the Transformer model enables it to learn different aspects of the data simultaneously. The same basic attention mechanism can be adapted for various NLP tasks with minimal task-specific modifications. For example, BERT‚Äôs bidirectional attention allows it to understand context from both directions, making it highly effective for tasks like question answering and sentiment analysis. The concept of attention aligns with our understanding of human cognition, making these models more intuitive and potentially more aligned with how our brains process language. Attention mechanisms, particularly in Transformer-based models, work exceptionally well with pre-training on large corpora. This has led to powerful language models like BERT and GPT, which can be fine-tuned for specific tasks with impressive results. For instance, GPT-3‚Äôs success in generating coherent and contextually appropriate text can be attributed to its extensive pre-training on diverse datasets, followed by fine-tuning. Furthermore, the development of models like Retrieval-Augmented Generation (RAG) by Lewis et al.&nbsp;(2020) showcases the combination of attention mechanisms with retrieval systems. RAG combines pre-trained language models with a retrieval component, allowing the model to access and integrate external knowledge dynamically. This hybrid approach significantly enhances the model‚Äôs ability to generate accurate and contextually rich responses by retrieving relevant documents or information during the generation process.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    classDef default fill:#ffffff,stroke:#0000ff,stroke-width:2px,color:#000000,font-weight:bold
    linkStyle default stroke:#0000ff,stroke-width:2px

    A[Attention Mechanisms] --&gt; B[Dynamic Focus]
    A --&gt; C[Parallelization]
    A --&gt; D[Long-range Dependencies]
    A --&gt; E[Interpretability]
    A --&gt; F[Improved Performance]
    A --&gt; G[Scalability]
    A --&gt; H[Versatility]
    A --&gt; I[Biological Plausibility]
    A --&gt; J[Synergy with Pre-training]
    A --&gt; K[Enhanced Capabilities with RAG]
</pre>
</div>
<p></p><figcaption> Novelty and success of attention mechanisms</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>The combination of these novel features and success factors has led to attention mechanisms becoming a cornerstone of modern NLP. They have enabled more nuanced understanding and generation of language, pushing the boundaries of what‚Äôs possible in artificial language processing. As research continues, attention mechanisms are likely to evolve further, potentially leading to even more sophisticated language models that can better capture the complexities and nuances of human communication.</p>
</section>
</section>
<section id="attention-economics" class="level2">
<h2 class="anchored" data-anchor-id="attention-economics">Attention economics</h2>
<section id="definition-and-core-principles" class="level3">
<h3 class="anchored" data-anchor-id="definition-and-core-principles">Definition and core principles</h3>
<p>Attention economics is an approach to managing information that recognizes human attention as a scarce and valuable commodity. In an environment abundant with information, the primary challenge becomes not the acquisition of information but the allocation of attention. This theory underscores the scarcity of attention in contrast to the overwhelming availability of information, emphasizing the need to allocate it efficiently.</p>
<p>A fundamental principle of attention economics is the concept of attention as a scarce resource. Unlike information, which can be produced and replicated infinitely, human attention is inherently limited. This limitation elevates the value of attention, making it a critical focus for individuals and organizations alike. Consequently, various stimuli‚Äîfrom advertisements to social media content‚Äîcompete fiercely for individuals‚Äô attention. This competition necessitates that individuals make deliberate choices about where to direct their attention, thus making attention allocation a significant aspect of personal and professional decision-making processes. Moreover, attention is viewed as a form of capital; the ability to capture and sustain attention can be monetized, influencing business models and marketing strategies (Davenport &amp; Beck, 2001).</p>
</section>
<section id="historical-context" class="level3">
<h3 class="anchored" data-anchor-id="historical-context">Historical context</h3>
<p>The concept of attention economics emerged in response to the dramatic increase in available information during the late 20th and early 21st centuries. The advent of the internet and digital media exponentially increased the accessibility and volume of information, shifting the primary challenge from obtaining information to managing and prioritizing it effectively.</p>
<p>Nobel laureate Herbert Simon laid the groundwork for attention economics in a pivotal 1971 speech, where he observed that ‚Äúa wealth of information creates a poverty of attention‚Äù (Simon, 1971). Simon highlighted the paradox where the abundance of information leads to a scarcity of attention, emphasizing that in an information-rich world, attention becomes the limiting factor in consumption. This insight laid the theoretical foundation for what would later become attention economics.</p>
<p>Building on Simon‚Äôs ideas, Michael Goldhaber coined the term ‚Äúattention economy‚Äù in 1997. Goldhaber articulated that human attention is treated as a scarce and valuable commodity, arguing that in a society overflowing with information, attention becomes the new currency. He posited that the ability to attract and hold attention is essential for success in various fields, from business to media to personal interactions. Goldhaber‚Äôs work underscored the need to adapt traditional economic models to account for the scarcity of human attention (Goldhaber, 1997).</p>
<p>Thomas Davenport further developed the concept in his book ‚ÄúThe Attention Economy: Understanding the New Currency of Business,‚Äù bringing these ideas into mainstream business thinking and highlighting how businesses can thrive by effectively managing and capturing attention (Davenport &amp; Beck, 2001). Yochai Benkler explored the broader implications of attention economics within networked information environments, adding depth to the theoretical landscape and emphasizing the role of social networks and digital platforms in the attention economy (Benkler, 2006).</p>
</section>
<section id="cognitive-basis" class="level3">
<h3 class="anchored" data-anchor-id="cognitive-basis">Cognitive basis</h3>
<p>The cognitive basis of attention economics lies in understanding how the human brain processes and prioritizes information. Cognitive science reveals that humans have a limited capacity for attention and must constantly filter and prioritize incoming stimuli to function effectively. This selective attention process is governed by neural mechanisms that help focus cognitive resources on the most relevant and significant information while ignoring distractions.</p>
<p>Research in cognitive psychology and neuroscience has shown that attention is influenced by factors such as salience, relevance, and context. Salient stimuli‚Äîthose that stand out due to their intensity, novelty, or contrast‚Äîtend to capture attention more readily. Relevance, determined by personal interests and goals, also plays a crucial role in attention allocation. Additionally, the context in which information is presented can affect how attention is directed and maintained.</p>
<p>These cognitive principles have profound effects on individual and group beliefs. By capturing attention, information can influence perceptions, attitudes, and behaviors. For instance, repeated exposure to specific ideas or narratives can shape beliefs and reinforce existing biases. At a group level, the collective focus on particular topics can drive public discourse and societal norms. Understanding these cognitive mechanisms allows for the development of strategies to manage and direct attention effectively, both in beneficial ways and in ways that can manipulate or mislead.</p>
</section>
<section id="applications-1" class="level3">
<h3 class="anchored" data-anchor-id="applications-1">Applications</h3>
<p>In marketing, attention economics has profoundly influenced advertising strategies. The need to capture attention in a crowded media landscape has led to innovations such as native advertising and influencer marketing. These techniques are designed to engage audiences more effectively by integrating promotional content seamlessly into users‚Äô everyday experiences (Eckler &amp; Bolls, 2011).</p>
<p>User interface design is another area significantly impacted by the principles of attention economics. Designers focus on simplicity, clarity, and strategic use of visual elements to guide users‚Äô attention, enhancing usability and engagement. Websites, apps, and software interfaces are meticulously crafted to capture and sustain user attention by minimizing distractions and emphasizing important features (Nielsen &amp; Loranger, 2006).</p>
<p>In the realm of information management, attention economics has inspired new approaches to knowledge management within organizations. Effective filtering, prioritization, and presentation of information are essential to ensure that critical data receives the necessary attention amidst the vast amounts of available information (Davenport, 2005).</p>
<p>Social media platforms like Facebook, Twitter, and Instagram operate as attention marketplaces where content competes for user engagement. These platforms are designed to maximize user attention through algorithms that prioritize engaging content, fostering prolonged interaction and repeat visits (Kietzmann et al., 2011).</p>
<p>Content creation has also been shaped by attention economics, evident in the prevalence of clickbait headlines and sensationalist content. These tactics aim to capture initial attention, which is crucial for success in an environment where numerous pieces of content vie for visibility and engagement (Blom &amp; Hansen, 2015).</p>
<p>Understanding attention economics is essential in today‚Äôs information-saturated world. It provides a framework for analyzing how individuals, organizations, and technologies compete for and allocate the limited resource of human attention. Marketers have exploited attention economics to generate substantial revenues by developing strategies that capture and monetize user engagement. However, this same framework has been leveraged by bad actors, including state-backed propaganda efforts and terrorist organizations, to manipulate public perception, spread misinformation, and incite violence (Benkler et al., 2018; Byman, 2015). Recognizing both the beneficial and malicious uses of attention economics is crucial for developing strategies to safeguard the integrity of information and protect the public from manipulation.</p>
<p>The relevance of attention economics is further underscored by its profound impact on the growth and revenue models of big tech companies. Platforms like Google, Facebook, and YouTube have built their business empires on the ability to capture and monetize user attention through targeted advertising and engagement-driven content algorithms. This focus on maximizing user attention has fueled their unprecedented growth and reshaped entire sectors. Traditional media industries, such as television and newspapers, have been significantly outshined by these digital platforms, which have become dominant forces in the advertising market. The shift towards an attention-driven economy highlights the transformative power of managing and leveraging human attention in the digital age.</p>
</section>
</section>
<section id="bridging-nlp-and-attention-economics" class="level2">
<h2 class="anchored" data-anchor-id="bridging-nlp-and-attention-economics">Bridging NLP and attention economics</h2>
<section id="conceptual-overlap" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-overlap">Conceptual overlap</h3>
<p>The conceptual overlap between attention mechanisms in NLP and attention economics centers on the efficient allocation and prioritization of limited resources. In the realm of NLP, attention mechanisms are designed to allocate computational resources to the most pertinent parts of the input data, thereby optimizing performance and enhancing efficiency (Vaswani et al., 2017). Similarly, attention economics focuses on how individuals and organizations allocate their finite cognitive resources to the most relevant and valuable pieces of information. Both fields grapple with the challenge of managing scarcity: in NLP, the scarcity is in computational power and data processing capabilities, whereas in attention economics, it is the finite nature of human attention (Davenport &amp; Beck, 2001).</p>
</section>
<section id="improving-attention-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="improving-attention-mechanisms">Improving attention mechanisms</h3>
<p>The cognitive basis studied in attention economics and computational neuroscience provides valuable insights that can enhance how machine learning models ingest and process information. These insights contribute to two main areas: improving how tokens are transformed into meaning and improving how AI systems correctly respond to user requests.</p>
<section id="improving-token-to-meaning-transformation" class="level4">
<h4 class="anchored" data-anchor-id="improving-token-to-meaning-transformation">Improving token-to-meaning transformation</h4>
<p>The cognitive principles from attention economics can help enhance the transformation of tokens into meaning. Jakobson‚Äôs model of language functions is particularly relevant here, as it identifies key components necessary for effective communication:</p>
<ul>
<li><p>Cognitive insights can help NLP models better understand the context in which a word or phrase is used. This understanding improves the model‚Äôs ability to disambiguate meanings and provide more accurate interpretations.</p></li>
<li><p>Attention mechanisms can be refined to more effectively map tokens to the common code or language understood by both the sender (addresser) and receiver (addressee). This involves enhancing the model‚Äôs understanding of syntax, semantics, and pragmatics, ensuring that the generated meaning aligns with human language conventions.</p></li>
<li><p>By integrating principles from cognitive neuroscience, NLP models can improve their ability to construct coherent messages that accurately convey intended meanings. This involves not just stringing tokens together but ensuring that the overall message makes sense within the given context.</p></li>
</ul>
<p>For instance, in language translation, an enhanced attention mechanism can focus on cultural context and idiomatic expressions that are crucial for accurate translations, improving the quality and relevance of the output (Bahdanau et al., 2014).</p>
</section>
<section id="improving-response-to-user-requests" class="level4">
<h4 class="anchored" data-anchor-id="improving-response-to-user-requests">Improving response to user requests</h4>
<p>Attention economics and cognitive neuroscience can also enhance how AI systems respond to user requests by optimizing attention allocation to ensure relevant and valuable responses. This involves several components from Jakobson‚Äôs model:</p>
<ul>
<li><p>Understanding the sender‚Äôs intent is crucial for generating appropriate responses. Insights from cognitive science can help NLP models infer the sender‚Äôs goals and priorities, improving the relevance of the responses.</p></li>
<li><p>Tailoring responses to the receiver‚Äôs needs and preferences is essential for effective communication. By analyzing user interaction patterns and preferences, AI systems can prioritize responses that are most likely to satisfy the user‚Äôs needs.</p></li>
<li><p>Ensuring effective communication requires maintaining a connection between the sender and receiver. Attention mechanisms can be designed to keep track of the ongoing conversation context, ensuring continuity and coherence in multi-turn interactions.</p></li>
</ul>
<p>For example, in question answering systems, attention mechanisms inspired by cognitive principles can maintain and leverage context over extended interactions, much like humans do when engaging in a conversation. This capability is critical for tasks such as document summarization and customer service, where understanding the broader context significantly enhances performance (Vaswani et al., 2017).</p>
<p>By understanding how humans allocate their cognitive resources, AI systems can be developed to prioritize and filter information more effectively. This alignment with human cognitive processes leads to more sophisticated attention mechanisms in NLP that can handle information in a manner akin to human attention patterns.</p>
</section>
</section>
<section id="practical-implications" class="level3">
<h3 class="anchored" data-anchor-id="practical-implications">Practical implications</h3>
<p>Incorporating principles from attention economics into the design of AI systems can significantly enhance their functionality. By optimizing AI models to prioritize information that is not only relevant but also contextually valuable, these systems can more closely mimic human decision-making processes. This approach can lead to AI systems that are not just efficient but also more intuitive and effective in real-world applications.</p>
<p>Understanding the principles of attention allocation can also lead to substantial improvements in human-machine interaction. AI systems designed with attention economics in mind can present information in a manner that aligns with human cognitive patterns. This alignment enhances user experience and engagement by ensuring that information is presented clearly and at the right time, reducing cognitive overload and increasing the efficiency of interactions.</p>
<p>Both attention mechanisms in NLP and attention economics emphasize the importance of filtering and prioritizing information. In environments saturated with data, AI systems can be designed to mimic human attention allocation by filtering out irrelevant data and prioritizing what is most important. This capability ensures that users receive the most pertinent information without being overwhelmed by the sheer volume of available data.</p>
<p>Leveraging attention mechanisms and economic principles can greatly enhance personalized content delivery. By understanding user preferences and attention patterns, AI systems can deliver content that is tailored to engage users more effectively. This personalized approach increases user satisfaction and retention by ensuring that the content meets individual needs and interests, thereby creating a more compelling user experience.</p>
<p>Moreover, insights from attention economics can improve how meaning and agency are embodied in language, benefiting attention mechanisms in NLP. Attention economics teaches us how individuals value and allocate their cognitive resources, which can inform the development of NLP systems that more accurately reflect human prioritization of information. By applying these principles, NLP models can be designed to focus not just on syntactic relevance but also on the contextual and pragmatic importance of information, much like humans do.</p>
<p>The logical steps linking attention economics to improvements in NLP start with understanding human attention allocation. Attention economics provides a framework for understanding how humans allocate their limited cognitive resources to different stimuli based on relevance and value. By integrating these principles, AI systems can be developed to prioritize and filter information in ways that align with human cognitive processes. This leads to more sophisticated attention mechanisms in NLP that not only process language based on syntactic importance but also incorporate contextual and pragmatic factors, mimicking human attention patterns. These enhanced NLP systems can interact with users in a more human-like manner, understanding and responding to nuances in language that reflect deeper meaning and intention. Finally, AI systems can leverage this understanding to deliver personalized and contextually relevant content, improving user engagement and satisfaction.</p>
</section>
<section id="adversarial-implications" class="level3">
<h3 class="anchored" data-anchor-id="adversarial-implications">Adversarial implications</h3>
<p>The relationship between attention economics and NLP also extends to adversarial usage, particularly in the realm of social media. Bad actors, including state-backed propaganda efforts and terrorist organizations, have leveraged these principles to manipulate public perception and influence individuals and groups (Byman, 2015). By understanding how attention is allocated, adversaries can craft messages that capture and exploit human attention, disseminating misleading or harmful information more effectively.</p>
<p>Social media platforms are prime targets for such manipulation, given their ability to rapidly spread information and engage vast audiences. Bad actors use sophisticated strategies to create compelling content that can distract, mislead, or radicalize users. These tactics not only threaten individual users but also pose broader societal risks, including the destabilization of communities and the erosion of trust in information sources (Benkler et al., 2018).</p>
<p>To counter these threats, it is essential to build robust mechanisms that protect users interacting with digital agents. This involves developing advanced AI systems capable of detecting and mitigating malicious content and influence campaigns. By integrating principles from attention economics, these systems can better identify patterns of manipulation and prioritize the filtering of harmful content.</p>
<p>Moreover, enhancing user education and awareness about attention manipulation can empower individuals to make more informed decisions about the information they consume and share. By fostering a deeper understanding of how their attention can be exploited, users can become more resilient to manipulation efforts.</p>
<p>The dual-edged nature of attention mechanisms highlights the need for ethical considerations in their deployment. Developing strategies that balance the benefits of attention optimization with safeguards against misuse is crucial for protecting the integrity of information and maintaining public trust.</p>
</section>
<section id="future-research-directions" class="level3">
<h3 class="anchored" data-anchor-id="future-research-directions">Future research directions</h3>
<p>Collaborative research between experts in NLP and economists can yield novel insights and methodologies for managing attention in both human and artificial systems. Such cross-disciplinary studies can uncover new ways to optimize attention allocation, benefiting both fields and leading to more advanced and effective solutions.</p>
<p>Developing more sophisticated attention models that incorporate economic principles can significantly advance AI systems. These models can better mimic human cognition and decision-making processes, leading to AI that not only processes information efficiently but also understands and responds to contextual nuances in a human-like manner.</p>
<p>Exploring the ethical implications of attention management, particularly in AI systems, is crucial for ensuring that these technologies are developed and deployed responsibly. Addressing ethical considerations can help safeguard user autonomy and well-being, ensuring that AI systems respect and enhance human experiences rather than exploit or manipulate them.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>The attention paradigm serves as a powerful bridge between NLP and economic theory, offering profound insights into the management of information resources. By exploring the parallels between attention mechanisms in NLP and attention economics, we gain a deeper understanding of both human cognition and artificial intelligence. This interdisciplinary approach not only enhances our theoretical knowledge but also has practical implications for the design of more efficient and human-centric technologies. As we continue to navigate an information-rich world, the integration of these fields will be crucial in shaping the future of information management and technology design. This convergence is especially relevant in an era where big tech companies have leveraged attention economics to drive their growth, overshadowing traditional media sectors such as television and newspapers. By harnessing the power of attention, these companies have revolutionized revenue models and reshaped entire industries, demonstrating the transformative impact of managing and monetizing attention in the digital age. Conversely, recognizing the potential for adversarial use of these principles underscores the importance of developing strategies to safeguard the integrity of information and protect the public from manipulation.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>James, W. (1890). <em>The Principles of Psychology</em>, Vol. 1. New York: Henry Holt and Company. Retrieved from <a href="https://www.gutenberg.org/ebooks/57628">Project Gutenberg</a>.</p>
<p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>. <em>arXiv preprint arXiv:1409.0473</em>.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., &amp; Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention Is All You Need</a>. <em>Advances in Neural Information Processing Systems, 30</em>, 5998-6008.</p>
<p>Child, R., Gray, S., Radford, A., &amp; Sutskever, I. (2019). <a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a>. <em>arXiv preprint arXiv:1904.10509</em>.</p>
<p>Benkler, Y. (2006). The Wealth of Networks: How Social Production Transforms Markets and Freedom. Yale University Press.</p>
<p>Benkler, Y., Faris, R., &amp; Roberts, H. (2018). Network Propaganda: Manipulation, Disinformation, and Radicalization in American Politics. Oxford University Press.</p>
<p>Blom, J. N., &amp; Hansen, K. R. (2015). <a href="https://www.sciencedirect.com/science/article/abs/pii/S0378216614001826">Click Bait: Forward-Reference as Lure in Online News Headlines</a>. <em>Journal of Pragmatics</em>, 76, 87-100.</p>
<p>Byman, D. (2015). Al Qaeda, the Islamic State, and the Global Jihadist Movement: What Everyone Needs to Know. Oxford University Press.</p>
<p>Davenport, T. H. (2005). Thinking for a Living: How to Get Better Performances and Results from Knowledge Workers. Harvard Business School Press.</p>
<p>Davenport, T. H., &amp; Beck, J. C. (2001). The Attention Economy: Understanding the New Currency of Business. Harvard Business School Press.</p>
<p>Eckler, P., &amp; Bolls, P. (2011). <a href="https://www.tandfonline.com/doi/abs/10.1080/15252019.2011.10722180">Spreading the Virus: Emotional Tone of Viral Advertising and Its Effect on Forwarding Intentions and Attitudes</a>. <em>Journal of Interactive Advertising</em>, 11(2), 1-11.</p>
<p>Goldhaber, M. H. (1997). <a href="https://firstmonday.org/ojs/index.php/fm/article/view/519/440">The Attention Economy and the Net</a>. <em>First Monday</em>, 2(4).</p>
<p>Kietzmann, J. H., Hermkens, K., McCarthy, I. P., &amp; Silvestre, B. S. (2011). <a href="https://www.sciencedirect.com/science/article/abs/pii/S0007681311000061">Social Media? Get Serious! Understanding the Functional Building Blocks of Social Media</a>. <em>Business Horizons</em>, 54(3), 241-251.</p>
<p>Nielsen, J., &amp; Loranger, H. (2006). Prioritizing Web Usability. New Riders.</p>
<p>Simon, H. A. (1971). Designing Organizations for an Information-Rich World. In Martin Greenberger (Ed.), <em>Computers, Communications, and the Public Interest</em> (pp.&nbsp;37-72). The Johns Hopkins Press.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/antomon\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="antomon/antomon-utterances" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Antonio Montano‚Äôs personal blog</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../contents/services.html">
<p>Services</p>
</a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/montano/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/antomon">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/antomon">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 ¬© Antonio Montano, 2024
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
<p><img src="https://raw.githubusercontent.com/antomon/antomon.github.io/cd7dbf5f81371c6eb1c3c7e8a32ac35f44b3bbce/by-nc-nd.svg" class="img-fluid"></p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>