<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Antonio Montano">
<meta name="dcterms.date" content="2024-03-15">

<title>Random Bits of Knowledge - GPT-4 Anniversary</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Random Bits of Knowledge - GPT-4 Anniversary">
<meta property="og:description" content="One year of a mass phenomenon">
<meta property="og:image" content="https://antomon.github.io/posts/gpt-4-anniversary/1-year-image.webp">
<meta property="og:site_name" content="Random Bits of Knowledge">
<meta name="twitter:title" content="Random Bits of Knowledge - GPT-4 Anniversary">
<meta name="twitter:description" content="One year of a mass phenomenon">
<meta name="twitter:image" content="https://antomon.github.io/posts/gpt-4-anniversary/1-year-image.webp">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../favicon.png" alt="AM logo" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Random Bits of Knowledge</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://cfknow.github.io/"> 
<span class="menu-text">CFKNow</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/montano/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/antomon"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/antomon"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">GPT-4 Anniversary</h1>
            <p class="subtitle lead">One year of a mass phenomenon</p>
                                <div class="quarto-categories">
                <div class="quarto-category">machine learning</div>
                <div class="quarto-category">generative ai</div>
                <div class="quarto-category">🇬🇧</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Antonio Montano </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 15, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">March 16, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="3">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#it-feels-like-a-lifetime-but-its-only-been-a-year" id="toc-it-feels-like-a-lifetime-but-its-only-been-a-year" class="nav-link active" data-scroll-target="#it-feels-like-a-lifetime-but-its-only-been-a-year">It feels like a lifetime, but it’s only been a year!</a></li>
  <li><a href="#outstanding-achievements" id="toc-outstanding-achievements" class="nav-link" data-scroll-target="#outstanding-achievements">Outstanding achievements</a>
  <ul class="collapse">
  <li><a href="#the-turing-test" id="toc-the-turing-test" class="nav-link" data-scroll-target="#the-turing-test">The Turing Test</a>
  <ul class="collapse">
  <li><a href="#what-is-about" id="toc-what-is-about" class="nav-link" data-scroll-target="#what-is-about">What is about</a></li>
  <li><a href="#ocean-big-5" id="toc-ocean-big-5" class="nav-link" data-scroll-target="#ocean-big-5">OCEAN Big-5</a></li>
  <li><a href="#the-research-from-jackson-et-al." id="toc-the-research-from-jackson-et-al." class="nav-link" data-scroll-target="#the-research-from-jackson-et-al.">The Research from Jackson et al.</a></li>
  </ul></li>
  <li><a href="#impact-on-work" id="toc-impact-on-work" class="nav-link" data-scroll-target="#impact-on-work">Impact on Work</a></li>
  </ul></li>
  <li><a href="#march-2024-landscape" id="toc-march-2024-landscape" class="nav-link" data-scroll-target="#march-2024-landscape">March 2024 landscape</a>
  <ul class="collapse">
  <li><a href="#openai-current-offering" id="toc-openai-current-offering" class="nav-link" data-scroll-target="#openai-current-offering">OpenAI current offering</a></li>
  <li><a href="#openai-pricing" id="toc-openai-pricing" class="nav-link" data-scroll-target="#openai-pricing">OpenAI pricing</a>
  <ul class="collapse">
  <li><a href="#gpt-4-turbo" id="toc-gpt-4-turbo" class="nav-link" data-scroll-target="#gpt-4-turbo"><strong>GPT-4 Turbo</strong></a></li>
  <li><a href="#gpt-4" id="toc-gpt-4" class="nav-link" data-scroll-target="#gpt-4"><strong>GPT-4</strong></a></li>
  </ul></li>
  <li><a href="#competitors" id="toc-competitors" class="nav-link" data-scroll-target="#competitors">Competitors</a>
  <ul class="collapse">
  <li><a href="#anthropic" id="toc-anthropic" class="nav-link" data-scroll-target="#anthropic">Anthropic</a></li>
  <li><a href="#cohere" id="toc-cohere" class="nav-link" data-scroll-target="#cohere">Cohere</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#lets-celebrate" id="toc-lets-celebrate" class="nav-link" data-scroll-target="#lets-celebrate">Let’s Celebrate!</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<!-- AddToAny BEGIN -->
<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
<a class="a2a_button_x"></a>
<a class="a2a_button_linkedin"></a>
<a class="a2a_button_threads"></a>
<a class="a2a_button_facebook a2a_counter"></a>
<a class="a2a_button_telegram"></a>
<a class="a2a_button_whatsapp"></a>
<a class="a2a_button_slashdot"></a>
<a class="a2a_button_email"></a>
<a class="a2a_button_printfriendly"></a>
<a class="a2a_button_reddit a2a_counter"></a>
<a class="a2a_dd a2a_counter" href="https://www.addtoany.com/share"></a>
</div>
<script async="" src="https://static.addtoany.com/menu/page.js"></script>
<!-- AddToAny END -->
<div style="margin-bottom:5px">
<section id="it-feels-like-a-lifetime-but-its-only-been-a-year" class="level1">
<h1>It feels like a lifetime, but it’s only been a year!</h1>
<p>March 2023 marked a turning point in the field of artificial intelligence with the release of <a href="https://openai.com/research/gpt-4">OpenAI’s GPT-4</a>. This powerful language model, boasting significant advancements over its predecessors, sent shockwaves through various industries and ignited discussions about the future of human-machine interaction. One year later, it’s clear that GPT-4’s impact has been wide-ranging and continues to evolve.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sama tweet.png" class="img-fluid figure-img"></p>
<figcaption>Sam Altman tweet</figcaption>
</figure>
</div>
<p>One of the most notable effects has been the acceleration of AI acceptance. GPT-4’s ability to perform exceptionally on standardized tests, generate human-quality writing, and integrate seamlessly with multimodal data like images and sound, has fostered a sense of legitimacy for large language models. This has emboldened researchers and businesses to explore AI applications with greater confidence.</p>
<p>In the course of evaluating the competencies of GPT-4, OpenAI subjected the model to a series of standardized academic and professional examinations, including the Uniform Bar Exam, the Law School Admission Test (LSAT), the Graduate Record Examination (GRE) Quantitative section, and assorted Advanced Placement (AP) subject tests. GPT-4 demonstrated proficiency across numerous assessments, achieving scores comparable to those of human test-takers. This implies that, were GPT-4 to be evaluated purely on its capacity to perform on these tests, it would possess the qualifications to gain admission into law schools and a broad range of universities.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gpt-4 exams.png" class="img-fluid figure-img"></p>
<figcaption>GPT-4 exams results from the March 2023 announcement</figcaption>
</figure>
</div>
<p>Prior LLMs often struggled with tasks requiring an understanding of context spread across long stretches of text. With a context windows from 8k to 32k tokens, GPT-4 was able to analyze a much larger chunk of text, allowing it to grasp complex relationships between ideas and follow long-range dependencies.</p>
<p>On September 25th, 2023, OpenAI <a href="https://openai.com/research/gpt-4v-system-card">announced</a> the rollout of two new features that extend how people can interact with its recent and most advanced model, GPT-4: the ability to ask questions about images and to use speech as an input to a query. Then, on November 6th, 2023, OpenAI announced API access to GPT-4 with Vision. This functionality marked GPT-4’s move into being a multimodal model. This means that the model can accept multiple “modalities” of input – text and images – and return results based on those inputs.</p>
<p>After a year, GPT-4 remains one of the most advanced LLMs, even though the competition is fierce and with formidable opponents. If the rumors are confirmed, in the coming months we will have an even more powerful version that will continue to amaze us, just like the previous ones.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="murati-1y.png" class="img-fluid figure-img"></p>
<figcaption>Mira Murati one-year anniversary celebration tweet</figcaption>
</figure>
</div>
</section>
<section id="outstanding-achievements" class="level1">
<h1>Outstanding achievements</h1>
<section id="the-turing-test" class="level2">
<h2 class="anchored" data-anchor-id="the-turing-test">The Turing Test</h2>
<section id="what-is-about" class="level3">
<h3 class="anchored" data-anchor-id="what-is-about">What is about</h3>
<p>The Turing Test, introduced by British mathematician and computer scientist Alan Turing in 1950, is a benchmark for evaluating a machine’s ability to exhibit intelligent behavior indistinguishable from that of a human. In his seminal paper, “Computing Machinery and Intelligence,” Turing proposed the question, “Can machines think?” and introduced the concept of the “imitation game” as a criterion for machine intelligence. The test involves a human judge engaging in natural language conversations with both a machine and a human without seeing them. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the Turing Test. This test has become a fundamental concept in the philosophy of artificial intelligence, sparking debates about the nature of intelligence and the potential of machines to emulate human-like consciousness and reasoning.</p>
<p>The Turing Test’s significance lies in its simplicity and profound implications. It provides a straightforward criterion for intelligence that does not rely on the machine’s ability to replicate the human brain’s workings but rather on the outcome of its interactions. Passing the Turing Test is considered a milestone for AI, suggesting that the machine can replicate human-like responses under certain conditions, thereby challenging the distinctions between human and machine intelligence.</p>
</section>
<section id="ocean-big-5" class="level3">
<h3 class="anchored" data-anchor-id="ocean-big-5">OCEAN Big-5</h3>
<p>Expanding on the OCEAN Big-5, also known as the Big Five personality traits, it’s a model based on common language descriptors of personality. These traits represent broad dimensions of human personality and include:</p>
<ol type="1">
<li><p><strong>Openness to experience</strong>: Characterized by imagination, creativity, and a willingness to try new things. High openness indicates a person who enjoys novelty, variety, and intellectual pursuits. Lower openness may suggest a more conventional and practical orientation.</p></li>
<li><p><strong>Conscientiousness</strong>: Involves self-discipline, orderliness, and a drive for achievement. Highly conscientious individuals are organized and responsible, often with a strong work ethic. Lower scores may indicate a more relaxed or spontaneous approach to life.</p></li>
<li><p><strong>Extraversion</strong>: Denotes sociability, excitement-seeking, and positive emotions. Extroverts are typically energetic and enjoy being around other people, while introverts (lower extraversion) may prefer solitude and more subdued environments.</p></li>
<li><p><strong>Agreeableness</strong>: Reflects a person’s altruistic, cooperative, and compassionate nature. High agreeableness is associated with trust and helpfulness, whereas lower agreeableness may manifest as skepticism or competitive behavior.</p></li>
<li><p><strong>Neuroticism</strong>: Pertains to emotional stability and the tendency to experience negative emotions. Higher neuroticism scores indicate a greater likelihood of feeling anxious, depressed, or angry, while lower scores suggest a calmer and more resilient disposition.</p></li>
</ol>
<p>These traits provide a framework for understanding human personality and predicting a wide range of behaviors, from academic and occupational success to relationships and well-being. In the context of AI, applying the OCEAN Big-5 to evaluate chatbots like ChatGPT allows researchers to assess how closely these systems mimic human personality traits, contributing to the ongoing exploration of machine “personality” and its implications for human-AI interaction.</p>
</section>
<section id="the-research-from-jackson-et-al." class="level3">
<h3 class="anchored" data-anchor-id="the-research-from-jackson-et-al.">The Research from Jackson et al.</h3>
<p>A research consortium led by Matthew Jackson, who holds the William D. Eberle Professorship of Economics within Stanford University’s School of Humanities and Sciences, conducted an empirical analysis of the behavioral and personality attributes of the AI-driven entities within ChatGPT, employing methodologies derived from psychology and behavioral economics. Their findings, documented in the paper <a href="https://www.pnas.org/doi/10.1073/pnas.2313925121">A Turing test of whether AI chatbots are behaviorally similar to humans</a> published in the Proceedings of the National Academy of Sciences, demonstrated that ChatGPT 4, exhibited indistinguishability from human participants in behavioral assessments. Notably, when the AI opted for atypical human behavioral patterns, it manifested increased levels of cooperativeness and altruism.</p>
<p>This investigative endeavor subjected versions 3 and 4 of ChatGPT to a prevalent personality assessment alongside a series of behavioral experiments designed to forecast socio-economic and ethical decision-making tendencies. These experiments encompassed standardized scenarios that required participants to make choices on dilemmas such as betraying a complicit criminal or allocating monetary resources under various incentive structures. The AI responses were benchmarked against a dataset comprising over 100,000 human participants spanning 50 nations.</p>
<p>Within the OCEAN Big-5, ChatGPT version 4 aligned with the normal human range for these traits but ranked in the lower third percentile in terms of agreeableness compared to the human sample. Despite passing the Turing Test, this level of agreeableness suggests limited social appeal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pnas.2313925121fig01.jpg" class="img-fluid figure-img" width="600"></p>
<figcaption>“Big Five” personality profiles of ChatGPT-4 and ChatGPT-3 compared with the distributions of human subjects. The blue, orange, and green lines correspond to the median scores of humans, ChatGPT-4, and ChatGPT-3 respectively; the shaded areas represent the middle 95% of the scores, across each of the dimensions. ChatGPT’s personality profiles are within the range of the human distribution, even though ChatGPT-3 scored noticeably lower in Openness.</figcaption>
</figure>
</div>
<p>Comparative analysis between versions 3 and 4 revealed significant advancements in the latter’s performance, with version 3 displaying agreeableness and openness to experience at the lower end of the human spectrum, indicative of a lesser capacity for novel ideas and experiences.</p>
<p>The methodology for assessing AI behavior in the experimental games involved calculating the frequency of specific actions (e.g., equitable distribution of funds) among both human participants and the AI. Subsequently, the researchers compared a randomly selected human action to one from the AI sessions to ascertain the likelihood of human origin. In the majority of these exercises, actions taken by version 4 were more consistently aligned with human behavior than those of version 3, which did not meet the Turing Test criteria.</p>
</section>
</section>
<section id="impact-on-work" class="level2">
<h2 class="anchored" data-anchor-id="impact-on-work">Impact on Work</h2>
<p>The study <a href="https://www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf">Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality</a> by Dell’Acqua et al.&nbsp;explores the impact of artificial intelligence (AI), specifically Large Language Models (LLMs) like GPT-4, on the productivity and quality of work among knowledge workers at Boston Consulting Group (BCG). This comprehensive experiment involved 758 consultants and aimed to understand how AI affects complex, knowledge-intensive tasks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Distribution-output-quality.png" class="img-fluid figure-img" width="900"></p>
<figcaption>Distribution of output quality across all the tasks. The blue group did not use AI, the green and red groups used AI, the red group got some additional training on how to use AI</figcaption>
</figure>
</div>
<p>The study introduces the concept of a “jagged technological frontier,” suggesting that AI capabilities are uneven across different tasks. Some tasks are significantly enhanced by AI, leading to improved productivity and quality, while others, seemingly similar in difficulty, lie outside AI’s current capabilities and can lead to decreased performance when AI is utilized.</p>
<p>Participants were divided into three groups: a control group with no AI access, a group with access to GPT-4, and a group with GPT-4 access plus a prompt engineering overview. The findings revealed that for tasks within AI’s capabilities, the use of AI led to a notable increase in both the quantity and quality of work. Consultants were able to complete more tasks and with better outcomes, demonstrating that AI can be a powerful tool for augmenting human capabilities in knowledge work.</p>
<p>However, for tasks selected to be outside the AI’s frontier, reliance on AI resulted in a decrease in performance. This highlights the importance of understanding AI’s limitations and suggests that indiscriminate use of AI can have negative consequences.</p>
<p>The study also observed two distinct patterns of AI integration among successful users: “Centaurs,” who strategically divided tasks between themselves and AI, and “Cyborgs,” who integrated AI more fully into their workflow. These findings suggest varying approaches to integrating AI into professional tasks, emphasizing the need for users to adapt their strategies based on the task at hand and AI’s capabilities.</p>
<p>In summary, the study provides empirical evidence on the dual role of AI in enhancing and sometimes detracting from professional knowledge work. It highlights the need for careful consideration of when and how to deploy AI tools, as well as the potential for AI to significantly impact work processes and outcomes within its capabilities. The concept of the jagged technological frontier offers a framework for understanding the complex and evolving relationship between AI and human work, underscoring the importance of navigating this frontier effectively to harness the benefits of AI while mitigating its risks.</p>
</section>
</section>
<section id="march-2024-landscape" class="level1">
<h1>March 2024 landscape</h1>
<section id="openai-current-offering" class="level2">
<h2 class="anchored" data-anchor-id="openai-current-offering">OpenAI current offering</h2>
<p>GPT-4 is available in the OpenAI API to <a href="https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4">paying customers</a>. Like <strong><code>gpt-3.5-turbo</code></strong>, GPT-4 is optimized for chat but works well for traditional completions tasks using the <a href="https://platform.openai.com/docs/api-reference/chat">Chat Completions API</a>. Learn how to use GPT-4 in our <a href="https://platform.openai.com/docs/guides/text-generation">text generation guide</a>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>MODEL</strong></th>
<th style="text-align: left;"><strong>DESCRIPTION</strong></th>
<th style="text-align: left;"><strong>CONTEXT WINDOW</strong></th>
<th style="text-align: left;"><strong>TRAINING DATA</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">gpt-4-0125-preview</td>
<td style="text-align: left;"><strong>GPT-4 Turbo</strong><br>
The latest GPT-4 model intended to reduce cases of “laziness” where the model doesn’t complete a task. Returns a maximum of 4,096 output tokens. <a href="https://openai.com/blog/new-embedding-models-and-api-updates">Learn more</a>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Dec 2023</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4-turbo-preview</td>
<td style="text-align: left;">Currently points to <code>gpt-4-0125-preview</code>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Dec 2023</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-1106-preview</td>
<td style="text-align: left;">GPT-4 Turbo model featuring improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. This is a preview model. <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">Learn more</a>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Apr 2023</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4-vision-preview</td>
<td style="text-align: left;">GPT-4 with the ability to understand images, in addition to all other GPT-4 Turbo capabilities. Currently points to <code>gpt-4-1106-vision-preview</code>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Apr 2023</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-1106-vision-preview</td>
<td style="text-align: left;">GPT-4 with the ability to understand images, in addition to all other GPT-4 Turbo capabilities. Returns a maximum of 4,096 output tokens. This is a preview model version. <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">Learn more</a>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Apr 2023</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4</td>
<td style="text-align: left;">Currently points to <code>gpt-4-0613</code>. See <a href="https://platform.openai.com/docs/models/continuous-model-upgrades">continuous model upgrades</a>.</td>
<td style="text-align: left;">8,192 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-0613</td>
<td style="text-align: left;">Snapshot of <code>gpt-4</code> from June 13th 2023 with improved function calling support.</td>
<td style="text-align: left;">8,192 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4-32k</td>
<td style="text-align: left;">Currently points to <code>gpt-4-32k-0613</code>. See <a href="https://platform.openai.com/docs/models/continuous-model-upgrades">continuous model upgrades</a>. This model was never rolled out widely in favor of GPT-4 Turbo.</td>
<td style="text-align: left;">32,768 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-32k-0613</td>
<td style="text-align: left;">Snapshot of <code>gpt-4-32k</code> from June 13th 2023 with improved function calling support. This model was never rolled out widely in favor of GPT-4 Turbo.</td>
<td style="text-align: left;">32,768 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
</tbody>
</table>
</section>
<section id="openai-pricing" class="level2">
<h2 class="anchored" data-anchor-id="openai-pricing">OpenAI pricing</h2>
<section id="gpt-4-turbo" class="level3">
<h3 class="anchored" data-anchor-id="gpt-4-turbo"><strong>GPT-4 Turbo</strong></h3>
<table class="table">
<colgroup>
<col style="width: 38%">
<col style="width: 30%">
<col style="width: 30%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>Input</strong></td>
<td><strong>Output</strong></td>
</tr>
<tr class="even">
<td>gpt-4-0125-preview</td>
<td>$10.00&nbsp;/ 1M tokens</td>
<td>$30.00&nbsp;/ 1M tokens</td>
</tr>
<tr class="odd">
<td>gpt-4-1106-preview</td>
<td>$10.00&nbsp;/ 1M tokens</td>
<td>$30.00&nbsp;/ 1M tokens</td>
</tr>
<tr class="even">
<td>gpt-4-1106-vision-preview</td>
<td>$10.00&nbsp;/ 1M tokens</td>
<td>$30.00&nbsp;/ 1M tokens</td>
</tr>
</tbody>
</table>
</section>
<section id="gpt-4" class="level3">
<h3 class="anchored" data-anchor-id="gpt-4"><strong>GPT-4</strong></h3>
<table class="table">
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>Input</strong></td>
<td><strong>Output</strong></td>
</tr>
<tr class="even">
<td>gpt-4</td>
<td>$30.00&nbsp;/ 1M tokens</td>
<td>$60.00&nbsp;/ 1M tokens</td>
</tr>
<tr class="odd">
<td>gpt-4-32k</td>
<td>$60.00&nbsp;/ 1M tokens</td>
<td>$120.00&nbsp;/ 1M tokens</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="competitors" class="level2">
<h2 class="anchored" data-anchor-id="competitors">Competitors</h2>
<p>Now in 2024, there is a fierce competition from Anthropic, Cohere, Google, and others.</p>
<section id="anthropic" class="level3">
<h3 class="anchored" data-anchor-id="anthropic">Anthropic</h3>
<p><a href="https://www.anthropic.com/news/claude-3-family" title="Claude 3">Claude 3</a> family of models employ various training methods, such as unsupervised learning and <a href="https://arxiv.org/abs/2212.08073" title="Constitutional AI">Constitutional AI</a>. A key enhancement in the Claude 3 family is multimodal input capabilities with text output, allowing users to upload images (e.g., tables, graphs, photos) along with text prompts for richer context and expanded use cases.</p>
<p>Opus, the most powerful model from Anthropic, outperforms GPT-4, GPT-3.5 and Gemini Ultra on a wide range of benchmarks. This includes topping the leaderboard on academic benchmarks like GSM-8k for mathematical reasoning and MMLU for expert-level knowledge.</p>
<p>Sonnet, the mid-range model, offers businesses a more cost-effective solution for routine data analysis and knowledge work, maintaining high performance without the premium price tag of the flagship model. Meanwhile, Haiku is designed to be swift and economical, suited for applications such as consumer-facing chatbots, where responsiveness and cost are crucial factors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="claude-comparison.webp" class="img-fluid figure-img" width="900"></p>
<figcaption>Comparison of the Claude 3 with leading models from the Anthropic announcement</figcaption>
</figure>
</div>
<p>In addition, Claude 3 models demonstrate sophisticated computer vision abilities on par with other state-of-the-art models. This new modality opens up use cases where enterprises need to extract information from images, documents, charts and diagrams.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="claude-comparison-vision.webp" class="img-fluid figure-img" width="900"></p>
<figcaption>Comparison of the Claude 3 vision capabilities with leading models from the Anthropic announcement</figcaption>
</figure>
</div>
</section>
<section id="cohere" class="level3">
<h3 class="anchored" data-anchor-id="cohere">Cohere</h3>
<p>While OpenAI has garnered widespread attention through the viral phenomenon of its ChatGPT chatbot, Cohere has adopted a more focused strategy, engaging directly with corporate clients to customize its AI models according to their unique requirements. This approach enables Cohere to achieve greater cost efficiency compared to competitors who aim at broad consumer markets.</p>
<table class="table">
<colgroup>
<col style="width: 31%">
<col style="width: 32%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Cohere API Pricing</strong></th>
<th style="text-align: left;"><strong>$ / M input tokens</strong></th>
<th style="text-align: left;"><strong>$ / M output tokens</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Command</td>
<td style="text-align: left;">$1.00</td>
<td style="text-align: left;">$2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">Command-R</td>
<td style="text-align: left;">$0.50</td>
<td style="text-align: left;">$1.50</td>
</tr>
</tbody>
</table>
<p><a href="https://txt.cohere.com/command-r/?_gl=1*1ckv15z*_ga*MTk1NTk5MTAyLjE3MDg5NzkzNzE.*_ga_CRGS116RZS*MTcxMDY3MTUwNC4zLjEuMTcxMDY3MjI0MC4zMS4wLjA." title="Command-R">Command-R</a> integrates seamlessly with Cohere’s Embed and Rerank models, providing retrieval-augmented generation (RAG) functionalities. A distinctive feature of Command-R is its ability to provide explicit citations in its outputs, reducing the occurrence of fabrications and facilitating user access to further information from the original sources.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Multilingual-Evals--1--1.png" class="img-fluid figure-img" width="700"></p>
<figcaption>Multilingual MMLU from Cohere announcement</figcaption>
</figure>
</div>
<p>The capability of Command-R to utilize external tools marks a significant advancement for developers in the corporate sector. This feature permits the model to link with external resources such as search engines, APIs, databases, and functions, thereby enriching its functionality through the utilization of data and operations available via these tools. This aspect is especially beneficial for businesses that store a substantial portion of their data in external repositories.</p>
<p>The adoption of tool usage opens the door to a broad spectrum of new applications. For example, developers can instruct Command-R to suggest a specific tool or a combination thereof, along with guidance on their usage. This enables chatbots to interact with customer relationship management (CRM) systems to update deal statuses or to employ Python interpreters for performing data science tasks. Additionally, it allows for the transformation of user inquiries into search commands for vector databases or search engines, empowering work assistants to autonomously navigate through various databases and platforms to gather pertinent information or execute comparative evaluations.</p>
<p>Tool usage with Command-R involves a four-stage process: initially, developers configure which tools the model can access and the format of interactions (e.g., API calls, JSON-formatted instructions). Command-R then judiciously selects the suitable tools and parameters for these interactions. Subsequently, developers execute these tool interactions, obtaining results, which are then fed back into Command-R to generate the final response.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="cohere-tool-use.png" class="img-fluid figure-img" width="900"></p>
<figcaption>Cohere tool usage</figcaption>
</figure>
</div>
<p>Beyond its RAG and tool integration features, Command-R benefits from an extended context window capability of up to 128k tokens and offers competitive pricing for Cohere’s hosted API service. Moreover, the model delivers robust performance across ten primary languages, encompassing English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.</p>
</section>
</section>
</section>
<section id="lets-celebrate" class="level1">
<h1>Let’s Celebrate!</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1-year-image-prompt.png" class="img-fluid figure-img"></p>
<figcaption>ChatGPT self-portrait</figcaption>
</figure>
</div>


</section>

</div><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/antomon\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="antomon/antomon-utterances" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Antonio Montano’s Personal Blog</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://cfknow.github.io/">
<p>CFKNow</p>
</a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/montano/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/antomon">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/antomon">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 © Antonio Montano, 2024
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
<p><img src="https://raw.githubusercontent.com/antomon/antomon.github.io/cd7dbf5f81371c6eb1c3c7e8a32ac35f44b3bbce/by-nc-nd.svg" class="img-fluid"></p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>