<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Bits of Knowledge</title>
<link>https://antomon.github.io/</link>
<atom:link href="https://antomon.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Antonio Montano&#39;s Personal Blog</description>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Thu, 14 Mar 2024 23:00:00 GMT</lastBuildDate>
<item>
  <title>GPT-4 Anniversary</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/posts/gpt-4-anniversary/</link>
  <description><![CDATA[ 





<section id="it-feels-like-a-lifetime-but-its-only-been-a-year" class="level1">
<h1>It feels like a lifetime, but it’s only been a year!</h1>
<p>March 2023 marked a turning point in the field of artificial intelligence with the release of <a href="https://openai.com/research/gpt-4">OpenAI’s GPT-4</a>. This powerful language model, boasting significant advancements over its predecessors, sent shockwaves through various industries and ignited discussions about the future of human-machine interaction. One year later, it’s clear that GPT-4’s impact has been wide-ranging and continues to evolve.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/sama tweet.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Sam Altman tweet"></p>
<figcaption>Sam Altman tweet</figcaption>
</figure>
</div>
<p>One of the most notable effects has been the acceleration of AI acceptance. GPT-4’s ability to perform exceptionally on standardized tests, generate human-quality writing, and integrate seamlessly with multimodal data like images and sound, has fostered a sense of legitimacy for large language models. This has emboldened researchers and businesses to explore AI applications with greater confidence.</p>
<p>In the course of evaluating the competencies of GPT-4, OpenAI subjected the model to a series of standardized academic and professional examinations, including the Uniform Bar Exam, the Law School Admission Test (LSAT), the Graduate Record Examination (GRE) Quantitative section, and assorted Advanced Placement (AP) subject tests. GPT-4 demonstrated proficiency across numerous assessments, achieving scores comparable to those of human test-takers. This implies that, were GPT-4 to be evaluated purely on its capacity to perform on these tests, it would possess the qualifications to gain admission into law schools and a broad range of universities.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/gpt-4 exams.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="GPT-4 exams results from the March 2023 announcement"></p>
<figcaption>GPT-4 exams results from the March 2023 announcement</figcaption>
</figure>
</div>
<p>Prior LLMs often struggled with tasks requiring an understanding of context spread across long stretches of text. With a context windows from 8k to 32k tokens, GPT-4 was able to analyze a much larger chunk of text, allowing it to grasp complex relationships between ideas and follow long-range dependencies.</p>
<p>On September 25th, 2023, OpenAI <a href="https://openai.com/research/gpt-4v-system-card">announced</a> the rollout of two new features that extend how people can interact with its recent and most advanced model, GPT-4: the ability to ask questions about images and to use speech as an input to a query. Then, on November 6th, 2023, OpenAI announced API access to GPT-4 with Vision. This functionality marked GPT-4’s move into being a multimodal model. This means that the model can accept multiple “modalities” of input – text and images – and return results based on those inputs.</p>
</section>
<section id="outstanding-achievements" class="level1">
<h1>Outstanding achievements</h1>
<section id="the-turing-test" class="level2">
<h2 class="anchored" data-anchor-id="the-turing-test">The Turing Test</h2>
<section id="what-is-about" class="level3">
<h3 class="anchored" data-anchor-id="what-is-about">What is about</h3>
<p>The Turing Test, introduced by British mathematician and computer scientist Alan Turing in 1950, is a benchmark for evaluating a machine’s ability to exhibit intelligent behavior indistinguishable from that of a human. In his seminal paper, “Computing Machinery and Intelligence,” Turing proposed the question, “Can machines think?” and introduced the concept of the “imitation game” as a criterion for machine intelligence. The test involves a human judge engaging in natural language conversations with both a machine and a human without seeing them. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the Turing Test. This test has become a fundamental concept in the philosophy of artificial intelligence, sparking debates about the nature of intelligence and the potential of machines to emulate human-like consciousness and reasoning.</p>
<p>The Turing Test’s significance lies in its simplicity and profound implications. It provides a straightforward criterion for intelligence that does not rely on the machine’s ability to replicate the human brain’s workings but rather on the outcome of its interactions. Passing the Turing Test is considered a milestone for AI, suggesting that the machine can replicate human-like responses under certain conditions, thereby challenging the distinctions between human and machine intelligence.</p>
</section>
<section id="ocean-big-5" class="level3">
<h3 class="anchored" data-anchor-id="ocean-big-5">OCEAN Big-5</h3>
<p>Expanding on the OCEAN Big-5, also known as the Big Five personality traits, it’s a model based on common language descriptors of personality. These traits represent broad dimensions of human personality and include:</p>
<ol type="1">
<li><p><strong>Openness to experience</strong>: Characterized by imagination, creativity, and a willingness to try new things. High openness indicates a person who enjoys novelty, variety, and intellectual pursuits. Lower openness may suggest a more conventional and practical orientation.</p></li>
<li><p><strong>Conscientiousness</strong>: Involves self-discipline, orderliness, and a drive for achievement. Highly conscientious individuals are organized and responsible, often with a strong work ethic. Lower scores may indicate a more relaxed or spontaneous approach to life.</p></li>
<li><p><strong>Extraversion</strong>: Denotes sociability, excitement-seeking, and positive emotions. Extroverts are typically energetic and enjoy being around other people, while introverts (lower extraversion) may prefer solitude and more subdued environments.</p></li>
<li><p><strong>Agreeableness</strong>: Reflects a person’s altruistic, cooperative, and compassionate nature. High agreeableness is associated with trust and helpfulness, whereas lower agreeableness may manifest as skepticism or competitive behavior.</p></li>
<li><p><strong>Neuroticism</strong>: Pertains to emotional stability and the tendency to experience negative emotions. Higher neuroticism scores indicate a greater likelihood of feeling anxious, depressed, or angry, while lower scores suggest a calmer and more resilient disposition.</p></li>
</ol>
<p>These traits provide a framework for understanding human personality and predicting a wide range of behaviors, from academic and occupational success to relationships and well-being. In the context of AI, applying the OCEAN Big-5 to evaluate chatbots like ChatGPT allows researchers to assess how closely these systems mimic human personality traits, contributing to the ongoing exploration of machine “personality” and its implications for human-AI interaction.</p>
</section>
<section id="the-research-from-jackson-et-al." class="level3">
<h3 class="anchored" data-anchor-id="the-research-from-jackson-et-al.">The Research from Jackson et al.</h3>
<p>A research consortium led by Matthew Jackson, who holds the William D. Eberle Professorship of Economics within Stanford University’s School of Humanities and Sciences, conducted an empirical analysis of the behavioral and personality attributes of the AI-driven entities within ChatGPT, employing methodologies derived from psychology and behavioral economics. Their findings, documented in the paper <a href="https://www.pnas.org/doi/10.1073/pnas.2313925121">A Turing test of whether AI chatbots are behaviorally similar to humans</a> published in the Proceedings of the National Academy of Sciences, demonstrated that ChatGPT 4, exhibited indistinguishability from human participants in behavioral assessments. Notably, when the AI opted for atypical human behavioral patterns, it manifested increased levels of cooperativeness and altruism.</p>
<p>This investigative endeavor subjected versions 3 and 4 of ChatGPT to a prevalent personality assessment alongside a series of behavioral experiments designed to forecast socio-economic and ethical decision-making tendencies. These experiments encompassed standardized scenarios that required participants to make choices on dilemmas such as betraying a complicit criminal or allocating monetary resources under various incentive structures. The AI responses were benchmarked against a dataset comprising over 100,000 human participants spanning 50 nations.</p>
<p>Within the OCEAN Big-5, ChatGPT version 4 aligned with the normal human range for these traits but ranked in the lower third percentile in terms of agreeableness compared to the human sample. Despite passing the Turing Test, this level of agreeableness suggests limited social appeal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/pnas.2313925121fig01.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
<figcaption>“Big Five” personality profiles of ChatGPT-4 and ChatGPT-3 compared with the distributions of human subjects. The blue, orange, and green lines correspond to the median scores of humans, ChatGPT-4, and ChatGPT-3 respectively; the shaded areas represent the middle 95% of the scores, across each of the dimensions. ChatGPT’s personality profiles are within the range of the human distribution, even though ChatGPT-3 scored noticeably lower in Openness.</figcaption>
</figure>
</div>
<p>Comparative analysis between versions 3 and 4 revealed significant advancements in the latter’s performance, with version 3 displaying agreeableness and openness to experience at the lower end of the human spectrum, indicative of a lesser capacity for novel ideas and experiences.</p>
<p>The methodology for assessing AI behavior in the experimental games involved calculating the frequency of specific actions (e.g., equitable distribution of funds) among both human participants and the AI. Subsequently, the researchers compared a randomly selected human action to one from the AI sessions to ascertain the likelihood of human origin. In the majority of these exercises, actions taken by version 4 were more consistently aligned with human behavior than those of version 3, which did not meet the Turing Test criteria.</p>
</section>
</section>
<section id="impact-on-work" class="level2">
<h2 class="anchored" data-anchor-id="impact-on-work">Impact on Work</h2>
<p>The study <a href="https://www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf">Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality</a> by Dell’Acqua et al.&nbsp;explores the impact of artificial intelligence (AI), specifically Large Language Models (LLMs) like GPT-4, on the productivity and quality of work among knowledge workers at Boston Consulting Group (BCG). This comprehensive experiment involved 758 consultants and aimed to understand how AI affects complex, knowledge-intensive tasks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/Distribution-output-quality.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Distribution of output quality across all the tasks" width="900"></p>
<figcaption>Distribution of output quality across all the tasks. The blue group did not use AI, the green and red groups used AI, the red group got some additional training on how to use AI</figcaption>
</figure>
</div>
<p>The study introduces the concept of a “jagged technological frontier,” suggesting that AI capabilities are uneven across different tasks. Some tasks are significantly enhanced by AI, leading to improved productivity and quality, while others, seemingly similar in difficulty, lie outside AI’s current capabilities and can lead to decreased performance when AI is utilized.</p>
<p>Participants were divided into three groups: a control group with no AI access, a group with access to GPT-4, and a group with GPT-4 access plus a prompt engineering overview. The findings revealed that for tasks within AI’s capabilities, the use of AI led to a notable increase in both the quantity and quality of work. Consultants were able to complete more tasks and with better outcomes, demonstrating that AI can be a powerful tool for augmenting human capabilities in knowledge work.</p>
<p>However, for tasks selected to be outside the AI’s frontier, reliance on AI resulted in a decrease in performance. This highlights the importance of understanding AI’s limitations and suggests that indiscriminate use of AI can have negative consequences.</p>
<p>The study also observed two distinct patterns of AI integration among successful users: “Centaurs,” who strategically divided tasks between themselves and AI, and “Cyborgs,” who integrated AI more fully into their workflow. These findings suggest varying approaches to integrating AI into professional tasks, emphasizing the need for users to adapt their strategies based on the task at hand and AI’s capabilities.</p>
<p>In summary, the study provides empirical evidence on the dual role of AI in enhancing and sometimes detracting from professional knowledge work. It highlights the need for careful consideration of when and how to deploy AI tools, as well as the potential for AI to significantly impact work processes and outcomes within its capabilities. The concept of the jagged technological frontier offers a framework for understanding the complex and evolving relationship between AI and human work, underscoring the importance of navigating this frontier effectively to harness the benefits of AI while mitigating its risks.</p>
</section>
</section>
<section id="march-2024-landscape" class="level1">
<h1>March 2024 landscape</h1>
<section id="openai-current-offering" class="level2">
<h2 class="anchored" data-anchor-id="openai-current-offering">OpenAI current offering</h2>
<p>GPT-4 is available in the OpenAI API to <a href="https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4">paying customers</a>. Like <strong><code>gpt-3.5-turbo</code></strong>, GPT-4 is optimized for chat but works well for traditional completions tasks using the <a href="https://platform.openai.com/docs/api-reference/chat">Chat Completions API</a>. Learn how to use GPT-4 in our <a href="https://platform.openai.com/docs/guides/text-generation">text generation guide</a>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>MODEL</strong></th>
<th style="text-align: left;"><strong>DESCRIPTION</strong></th>
<th style="text-align: left;"><strong>CONTEXT WINDOW</strong></th>
<th style="text-align: left;"><strong>TRAINING DATA</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">gpt-4-0125-preview</td>
<td style="text-align: left;"><strong>GPT-4 Turbo</strong><br>
The latest GPT-4 model intended to reduce cases of “laziness” where the model doesn’t complete a task. Returns a maximum of 4,096 output tokens. <a href="https://openai.com/blog/new-embedding-models-and-api-updates">Learn more</a>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Dec 2023</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4-turbo-preview</td>
<td style="text-align: left;">Currently points to <code>gpt-4-0125-preview</code>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Dec 2023</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-1106-preview</td>
<td style="text-align: left;">GPT-4 Turbo model featuring improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. This is a preview model. <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">Learn more</a>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Apr 2023</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4-vision-preview</td>
<td style="text-align: left;">GPT-4 with the ability to understand images, in addition to all other GPT-4 Turbo capabilities. Currently points to <code>gpt-4-1106-vision-preview</code>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Apr 2023</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-1106-vision-preview</td>
<td style="text-align: left;">GPT-4 with the ability to understand images, in addition to all other GPT-4 Turbo capabilities. Returns a maximum of 4,096 output tokens. This is a preview model version. <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">Learn more</a>.</td>
<td style="text-align: left;">128,000 tokens</td>
<td style="text-align: left;">Up to Apr 2023</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4</td>
<td style="text-align: left;">Currently points to <code>gpt-4-0613</code>. See <a href="https://platform.openai.com/docs/models/continuous-model-upgrades">continuous model upgrades</a>.</td>
<td style="text-align: left;">8,192 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-0613</td>
<td style="text-align: left;">Snapshot of <code>gpt-4</code> from June 13th 2023 with improved function calling support.</td>
<td style="text-align: left;">8,192 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
<tr class="even">
<td style="text-align: left;">gpt-4-32k</td>
<td style="text-align: left;">Currently points to <code>gpt-4-32k-0613</code>. See <a href="https://platform.openai.com/docs/models/continuous-model-upgrades">continuous model upgrades</a>. This model was never rolled out widely in favor of GPT-4 Turbo.</td>
<td style="text-align: left;">32,768 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
<tr class="odd">
<td style="text-align: left;">gpt-4-32k-0613</td>
<td style="text-align: left;">Snapshot of <code>gpt-4-32k</code> from June 13th 2023 with improved function calling support. This model was never rolled out widely in favor of GPT-4 Turbo.</td>
<td style="text-align: left;">32,768 tokens</td>
<td style="text-align: left;">Up to Sep 2021</td>
</tr>
</tbody>
</table>
</section>
<section id="openai-pricing" class="level2">
<h2 class="anchored" data-anchor-id="openai-pricing">OpenAI pricing</h2>
<section id="gpt-4-turbo" class="level3">
<h3 class="anchored" data-anchor-id="gpt-4-turbo"><strong>GPT-4 Turbo</strong></h3>
<table class="table">
<colgroup>
<col style="width: 38%">
<col style="width: 30%">
<col style="width: 30%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>Input</strong></td>
<td><strong>Output</strong></td>
</tr>
<tr class="even">
<td>gpt-4-0125-preview</td>
<td>$10.00&nbsp;/ 1M tokens</td>
<td>$30.00&nbsp;/ 1M tokens</td>
</tr>
<tr class="odd">
<td>gpt-4-1106-preview</td>
<td>$10.00&nbsp;/ 1M tokens</td>
<td>$30.00&nbsp;/ 1M tokens</td>
</tr>
<tr class="even">
<td>gpt-4-1106-vision-preview</td>
<td>$10.00&nbsp;/ 1M tokens</td>
<td>$30.00&nbsp;/ 1M tokens</td>
</tr>
</tbody>
</table>
</section>
<section id="gpt-4" class="level3">
<h3 class="anchored" data-anchor-id="gpt-4"><strong>GPT-4</strong></h3>
<table class="table">
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>Input</strong></td>
<td><strong>Output</strong></td>
</tr>
<tr class="even">
<td>gpt-4</td>
<td>$30.00&nbsp;/ 1M tokens</td>
<td>$60.00&nbsp;/ 1M tokens</td>
</tr>
<tr class="odd">
<td>gpt-4-32k</td>
<td>$60.00&nbsp;/ 1M tokens</td>
<td>$120.00&nbsp;/ 1M tokens</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="competitors" class="level2">
<h2 class="anchored" data-anchor-id="competitors">Competitors</h2>
<p>Now in 2024, there is a fierce competition from Anthropic, Cohere, Google, and others.</p>
<section id="anthropic" class="level3">
<h3 class="anchored" data-anchor-id="anthropic">Anthropic</h3>
<p><a href="https://www.anthropic.com/news/claude-3-family" title="Claude 3">Claude 3</a> family of models employ various training methods, such as unsupervised learning and Constitutional AI. A key enhancement in the Claude 3 family is multimodal input capabilities with text output, allowing users to upload images (e.g., tables, graphs, photos) along with text prompts for richer context and expanded use cases.</p>
<p>Opus, the most powerful model from Anthropic, outperforms GPT-4, GPT-3.5 and Gemini Ultra on a wide range of benchmarks. This includes topping the leaderboard on academic benchmarks like GSM-8k for mathematical reasoning and MMLU for expert-level knowledge.</p>
<p>Sonnet, the mid-range model, offers businesses a more cost-effective solution for routine data analysis and knowledge work, maintaining high performance without the premium price tag of the flagship model. Meanwhile, Haiku is designed to be swift and economical, suited for applications such as consumer-facing chatbots, where responsiveness and cost are crucial factors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/claude-comparison.webp" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Comparison of the Claude 3 with leading models" width="900"></p>
<figcaption>Comparison of the Claude 3 with leading models from the Anthropic announcement</figcaption>
</figure>
</div>
<p>In addition, Claude 3 models demonstrate sophisticated computer vision abilities on par with other state-of-the-art models. This new modality opens up use cases where enterprises need to extract information from images, documents, charts and diagrams.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/claude-comparison-vision.webp" class="img-fluid quarto-figure quarto-figure-center figure-img" width="900"></p>
<figcaption>Comparison of the Claude 3 vision capabilities with leading models from the Anthropic announcement</figcaption>
</figure>
</div>
</section>
<section id="cohere" class="level3">
<h3 class="anchored" data-anchor-id="cohere">Cohere</h3>
<p>While OpenAI has garnered widespread attention through the viral phenomenon of its ChatGPT chatbot, Cohere has adopted a more focused strategy, engaging directly with corporate clients to customize its AI models according to their unique requirements. This approach enables Cohere to achieve greater cost efficiency compared to competitors who aim at broad consumer markets.</p>
<table class="table">
<colgroup>
<col style="width: 31%">
<col style="width: 32%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Cohere API Pricing</strong></th>
<th style="text-align: left;"><strong>$ / M input tokens</strong></th>
<th style="text-align: left;"><strong>$ / M output tokens</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Command</td>
<td style="text-align: left;">$1.00</td>
<td style="text-align: left;">$2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">Command-R</td>
<td style="text-align: left;">$0.50</td>
<td style="text-align: left;">$1.50</td>
</tr>
</tbody>
</table>
<p><a href="https://txt.cohere.com/command-r/?_gl=1*1ckv15z*_ga*MTk1NTk5MTAyLjE3MDg5NzkzNzE.*_ga_CRGS116RZS*MTcxMDY3MTUwNC4zLjEuMTcxMDY3MjI0MC4zMS4wLjA." title="Command-R">Command-R</a> integrates seamlessly with Cohere’s Embed and Rerank models, providing superior Retrieval-Augmented Generation (RAG) functionalities. A distinctive feature of Command-R is its ability to provide explicit citations in its outputs, reducing the occurrence of fabrications and facilitating user access to further information from the original sources.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/Multilingual-Evals--1--1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Multilingual MMLU from Cohere announcement" width="700"></p>
<figcaption>Multilingual MMLU from Cohere announcement</figcaption>
</figure>
</div>
<p>The capability of Command-R to utilize external tools marks a significant advancement for developers in the corporate sector. This feature permits the model to link with external resources such as search engines, APIs, databases, and functions, thereby enriching its functionality through the utilization of data and operations available via these tools. This aspect is especially beneficial for businesses that store a substantial portion of their data in external repositories.</p>
<p>The adoption of tool usage opens the door to a broad spectrum of new applications. For example, developers can instruct Command-R to suggest a specific tool or a combination thereof, along with guidance on their usage. This enables chatbots to interact with Customer Relationship Management (CRM) systems to update deal statuses or to employ Python interpreters for performing data science tasks. Additionally, it allows for the transformation of user inquiries into search commands for vector databases or search engines, empowering work assistants to autonomously navigate through various databases and platforms to gather pertinent information or execute comparative evaluations.</p>
<p>Tool usage with Command-R involves a four-stage process: initially, developers configure which tools the model can access and the format of interactions (e.g., API calls, JSON-formatted instructions). Command-R then judiciously selects the suitable tools and parameters for these interactions. Subsequently, developers execute these tool interactions, obtaining results, which are then fed back into Command-R to generate the final response.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/cohere-tool-use.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Cohere tool usage" width="900"></p>
<figcaption>Cohere tool usage</figcaption>
</figure>
</div>
<p>Beyond its RAG and tool integration features, Command-R excels with an extended context window capability of up to 128k tokens and offers competitive pricing for Cohere’s hosted API service. Moreover, the model delivers robust performance across ten primary languages, encompassing English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.</p>
</section>
</section>
</section>
<section id="lets-celebrate" class="level1">
<h1>Let’s Celebrate!</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/gpt-4-anniversary/1-year-image-prompt.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="ChatGPT self-portrait"></p>
<figcaption>ChatGPT self-portrait</figcaption>
</figure>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>machine learning</category>
  <category>generative ai</category>
  <category>&amp;#127468;&amp;#127463;</category>
  <guid>https://antomon.github.io/posts/gpt-4-anniversary/</guid>
  <pubDate>Thu, 14 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/posts/gpt-4-anniversary/1-year-image.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Intervento al DABS Day 2024 - Università Ca’ Foscari</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/posts/intervento-dabs-day-2024-ca-foscari/</link>
  <description><![CDATA[ 





<p>Bel pomeriggio presso l’Università Ca’ Foscari di Venezia, ospite del Dipartimento di Economia e dell’evento DABS Day 2024.</p>
<p>Ho portato una serie di spunti utili al confronto con i ragazzi e il corpo docente, sulla intelligenza artificiale generativa.</p>
<p>Si possono scaricare le slide in formato <a href="https://github.com/antomon/antomon.github.io/blob/ab2afdffdaa015f1dad52fc0a1de3dd7cc171b35/posts/intervento-dabs-day-2024-ca-foscari/GENAI-TALK-20240311-FULL.pptx" title="PPTX">PPTX</a> (Powerpoint).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/intervento-dabs-day-2024-ca-foscari/brochure.png" class="img-fluid figure-img" alt="Brochure DABS Day"></p>
<figcaption>Brochure DABS Day</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/intervento-dabs-day-2024-ca-foscari/start.png" class="img-fluid figure-img" alt="Si inizia!"></p>
<figcaption>Si inizia!</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/intervento-dabs-day-2024-ca-foscari/aspettative.jpg" class="img-fluid figure-img" alt="Le montagne russe delle aspettative della IA!"></p>
<figcaption>Le montagne russe delle aspettative della IA!</figcaption>
</figure>
</div>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>talk</category>
  <category>machine learning</category>
  <category>generative ai</category>
  <category>&amp;#127470;&amp;#127481;</category>
  <guid>https://antomon.github.io/posts/intervento-dabs-day-2024-ca-foscari/</guid>
  <pubDate>Wed, 13 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/posts/intervento-dabs-day-2024-ca-foscari/sangiobbe2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>AI important as fire, generative AI as printing press, autonomous agents as wheel?</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/posts/ai-important-as-fire/</link>
  <description><![CDATA[ 





<section id="fire" class="level2">
<h2 class="anchored" data-anchor-id="fire">Fire</h2>
<p>Sundar Pichai, the CEO of Alphabet and Google, has repeatedly made a bold statement regarding the significance of artificial intelligence (AI), comparing its importance to that of fire and electricity. He believes that AI is a “profound technology” and possibly more critical than these monumental discoveries in human history. Pichai’s comparison underscores the transformative potential of AI across all aspects of human life, from healthcare and education to manufacturing and beyond, heralding a new era of innovation and societal change​​.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://antomon.github.io/posts/ai-important-as-fire/pichai-fire.png" class="img-fluid figure-img" alt="Sundar Pichai in 2018 and 2023"></p>
<figcaption>Sundar Pichai in 2018 and 2023</figcaption>
</figure>
</div>
<p>In a philosophical context, Pichai’s assertion invites us to explore the essence of transformative technologies and their impact on human civilization. The invention of fire was a pivotal moment in human history, providing warmth, protection, and the means to cook food, which significantly altered our nutritional intake and social structures. Similarly, the discovery of electricity revolutionized the industrial world, leading to the modern era of technology and convenience. These inventions not only changed the course of human history but also reshaped our relationship with the world and each other.</p>
<p>Artificial intelligence, according to Pichai, stands on the threshold of being the next great leap, akin to fire and electricity. This comparison is not merely about the utility of AI but its potential to redefine what it means to be human. AI challenges our understanding of intelligence, creativity, and even consciousness, pushing us to reconsider the boundaries between human and machine. The philosophical implications are profound: AI forces us to confront questions about autonomy, ethics, and the nature of the self in a world where machines can learn, make decisions, and potentially understand.</p>
<p>The comparison to fire and electricity also highlights the dual nature of transformative technologies: their capacity to benefit and to harm. Just as fire can warm and destroy, and electricity can illuminate and electrocute, AI holds immense potential for both positive and negative outcomes. It presents ethical dilemmas that humanity must navigate, such as privacy concerns, job displacement, and the potential for autonomous weapons. These challenges require a philosophical approach to ethics, governance, and the development of a societal framework that maximizes the benefits of AI while minimizing its risks.</p>
<p>Pichai’s statement also prompts us to consider the philosophical concept of progress. What does it mean for a technology to be as important as fire or electricity? It suggests a view of human history as a series of technological advancements that fundamentally alter our way of life. However, this perspective raises questions about the nature of progress itself. Is technological advancement inherently good, or does it bring new challenges that we must address? The philosophy of technology explores these questions, examining the relationship between human beings and their tools, and the ways in which technology shapes our values, society, and perception of reality.</p>
<p>In conclusion, Sundar Pichai’s comparison of AI to fire and electricity is not merely a statement about the potential of AI to change our world. It is an invitation to engage in deep philosophical reflection about the nature of technology, progress, and what it means to be human in an age where the boundaries between the natural and artificial are increasingly blurred. As we stand on the cusp of this new technological era, it is imperative that we approach AI with a blend of optimism and caution, guided by ethical considerations and a commitment to shaping a future that benefits all of humanity.</p>
</section>
<section id="printing-press" class="level2">
<h2 class="anchored" data-anchor-id="printing-press">Printing press</h2>
<p>If artificial intelligence (AI) as a whole can be compared to the importance of fire for the development of humanity, then generative AI, one of its most innovative subcategories, might be likened to the invention of the movable type printing press for its potential impact on society and the spread of knowledge.</p>
<p>The invention of the movable type printing press by Johannes Gutenberg in the 15th century marked a turning point for the dissemination of knowledge, culture, and education. It made books and documents more accessible, breaking down the barriers to education and knowledge that had been preserved in an elitist manner in manuscripts. This led to a democratization of knowledge, the spread of ideas, and progress in science, technology, and culture.</p>
<p>Similarly, generative AI has the potential to:</p>
<ul>
<li><p>Democratize content creation: It allows anyone to create complex content, such as texts, images, music, and videos, without necessarily having specialized skills in these areas. This could break down barriers to creative and innovative expression.</p></li>
<li><p>Accelerate innovation: Generative AI can speed up research and development in various fields, generating new ideas, solutions to complex problems, and even scientific discoveries that could take years of human effort.</p></li>
<li><p>Personalize education: It could revolutionize education by providing personalized and interactive educational materials that adapt to the learning level and style of each student, making education more effective and efficient.</p></li>
<li><p>Change in creative work: It will change how creative work is produced and distributed, introducing new tools and methods for artists, writers, designers, and creators in various fields.</p></li>
<li><p>Access to knowledge: It could transform access to information, making it easier for people to obtain complex answers and detailed analyses on vast datasets, similarly to how printing made written knowledge accessible. Just as the invention of the printing press marked the beginning of a new era in the dissemination of knowledge and cultural development, generative AI could usher in an era of unprecedented innovation in the way we generate, share, and interact with human creations and information.</p></li>
</ul>
</section>
<section id="wheel" class="level2">
<h2 class="anchored" data-anchor-id="wheel">Wheel</h2>
<p>Autonomous agents, a distinct category within the broad spectrum of artificial intelligence, can be compared to the invention of the wheel for their potential impact on society and the progress of humanity. While generative AI transforms the way we create and interact with content, autonomous agents change the way we interact with both the physical and digital world by automating tasks and decisions independently.</p>
<p>The invention of the wheel was crucial in the development of human societies, facilitating transportation, trade, and communication. It allowed civilizations to overcome physical limitations, expanding their geographical and cultural horizons.</p>
<p>Similarly, autonomous agents have the potential to:</p>
<ul>
<li><p>Transform transportation and logistics: Autonomous vehicles, drones, and automated delivery systems can revolutionize how we move people and goods, making transportation safer, more efficient, and accessible.</p></li>
<li><p>Industrial and domestic automation: From industrial manufacturing to home management, autonomous agents can take on repetitive or dangerous tasks, improving efficiency and safety at work and in daily life.</p></li>
<li><p>Personalized healthcare assistance: Autonomous robots and virtual assistants can provide personalized support to patients and the elderly, improving access to care and quality of life.</p></li>
<li><p>Environmental and urban management: They can be employed in sustainable resource management, environmental monitoring, and urban infrastructure maintenance, contributing to smarter and more sustainable cities.</p></li>
<li><p>Exploration and research: From space exploration to data collection in inaccessible or hazardous environments on Earth, autonomous agents can go where it is risky or impossible for humans, expanding our understanding of the world and beyond.</p></li>
</ul>
<p>Therefore, autonomous agents promise to be a driving force behind a wide range of advancements in various sectors, potentially revolutionizing our infrastructures, economies, and societies in ways we can only imagine today.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>AI, likened to fire and electricity, is posited as not just a tool but a fundamental force reshaping the fabric of human existence. This comparison underscores AI’s dual potential to both illuminate the path forward and, if mishandled, to unleash consequences we’re only beginning to grasp. The essay compellingly argues that AI’s impact transcends technological advancement, urging a philosophical introspection about humanity’s trajectory in this new era.</p>
<p>The analogy of generative AI to the printing press captures its democratizing power over knowledge and creation. Just as the printing press unlocked the gates of knowledge, generative AI promises to democratize creativity, making it accessible to all. This section of the essay envisions a future where barriers to educational, artistic, and scientific endeavors are dismantled, heralding a renaissance of mass creativity and innovation.</p>
<p>Drawing a parallel between autonomous agents and the wheel, the essay highlights how these technologies could redefine mobility, automation, and our interaction with the physical and digital realms. The potential for autonomous agents to transform logistics, healthcare, environmental management, and even explore uncharted territories speaks to their role as enablers of a new phase of human progress.</p>
<p>Final remarks:</p>
<ol type="1">
<li><p>The onset of a new human evolutionary era: Beyond technological marvel, AI invites a deeper philosophical inquiry into the essence of human intelligence, creativity, and ethics. As we intertwine our lives with AI, we’re not just shaping technology but redefining what it means to be human.</p></li>
<li><p>Ethical imperatives and governance: The transformative potential of AI, akin to fire’s ability to both warm and destroy, necessitates a careful ethical and regulatory approach. We urge the development of frameworks that balance innovation with safeguarding human dignity and rights.</p></li>
<li><p>Redefining progress: The narratives of AI as fire, generative AI as the printing press, and autonomous agents as the wheel challenge us to reconsider the meaning of progress. We posit that true progress lies not in technological advancement alone but in harnessing these tools for the collective good of humanity.</p></li>
<li><p>A call to action: Finally, this short essay serves as a call to action for policymakers, technologists, and society at large. It underscores the importance of informed, ethical stewardship of AI technologies to ensure they serve as engines of sustainable growth, equity, and human flourishing.</p></li>
</ol>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>essay</category>
  <category>machine learning</category>
  <category>generative ai</category>
  <category>agents</category>
  <category>&amp;#127468;&amp;#127463;</category>
  <guid>https://antomon.github.io/posts/ai-important-as-fire/</guid>
  <pubDate>Sat, 10 Feb 2024 23:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/posts/ai-important-as-fire/red-face.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Color Space Sampling 101</title>
  <dc:creator>Antonio Montano</dc:creator>
  <link>https://antomon.github.io/posts/color-space-sampling-101/</link>
  <description><![CDATA[ 





<section id="color-spaces" class="level2">
<h2 class="anchored" data-anchor-id="color-spaces">Color spaces</h2>
<p>Let’s break down the concept of a color space into simple terms first, and then delve into the technical aspects.</p>
<section id="layman-terms" class="level3">
<h3 class="anchored" data-anchor-id="layman-terms">Layman terms</h3>
<p>Imagine you have a huge box of crayons with every color you can think of. A color space is like picking a smaller box from this huge collection. This smaller box contains a specific range of colors that you can use for a particular purpose, like drawing a picture or printing a photograph.</p>
<p>Just like you can’t use the colors outside your chosen crayon box, a color space defines the range of colors (or ‘gamut’) that can be represented or reproduced in a medium, whether it’s a computer screen, a camera, or a printed page. Different color spaces are like different sets of crayons, each suited for different tasks or equipment.</p>
</section>
<section id="technically-speaking" class="level3">
<h3 class="anchored" data-anchor-id="technically-speaking">Technically speaking</h3>
<p>A color space is a specific organization of colors, which in a more formal setting can be described by the mathematics of color models. It’s a three-dimensional model where each color is represented by a unique point within a coordinate system.</p>
<p>Technically, a color space maps out a range of colors in terms of intensity values across different channels (like red, green, blue in RGB color space). It provides a standard by which we can define and reproduce colors across different devices and mediums.</p>
<p>Components of a color space are:</p>
<ul>
<li><p>Primary Colors: These are the reference colors used in a color model. For example, RGB uses red, green, and blue as primary colors.</p></li>
<li><p>Gamut: This is the complete subset of colors that can be accurately represented within a given color space.</p></li>
<li><p>Color model: The underlying mathematical model describing the way colors can be represented as tuples of numbers (e.g., RGB, CMYK, HSL).</p></li>
<li><p>Perceptual uniformity: Some color spaces (like CIELab) are designed to be perceptually uniform. This means that a change of the same amount in a color value should produce a change of about the same visual importance.</p></li>
<li><p>Device-dependent vs device-independent: Color spaces can be device-dependent (like Adobe RGB, specific to monitors and printers) or device-independent (like CIELab), which abstracts color definitions from specific devices, allowing for consistent color reproduction across different devices.</p></li>
<li><p>Standardization: Standards such as sRGB are established to ensure uniform color representation across different digital devices and platforms, crucial in digital media and web content.</p></li>
</ul>
<p>In essence, a color space is a framework that allows for consistent and precise color representation, ensuring that the colors you see and use are the same across various devices and mediums.</p>
</section>
</section>
<section id="rgb-and-srgb-color-spaces" class="level2">
<h2 class="anchored" data-anchor-id="rgb-and-srgb-color-spaces">RGB and sRGB Color Spaces</h2>
<p>The RGB color space, foundational in the realm of digital imaging and display technologies, represents colors through the additive combination of the red (R), green (G), and blue (B) primary colors. For instance, combining red and green light produces yellow, red and blue produce magenta, and green and blue create cyan.</p>
<p>The intensity of each primary color, typically represented by a value ranging from 0 to 255 in digital systems, combines to produce a wide spectrum of colors. This model is intrinsically linked to the way human vision perceives color through cone cells sensitive to these three color wavelengths.</p>
<p>In the digital context, the RGB color space is device-dependent, meaning the exact color rendition can vary across different devices like monitors, cameras, and scanners. This variation stems from differences in how devices are manufactured and the specific characteristics of their RGB color filters. As a result, a color seen on one RGB device might not look exactly the same on another, leading to inconsistencies in color reproduction.</p>
<p>sRGB, which stands for standard Red Green Blue, emerged as a standardization effort to tackle these inconsistencies, especially pertinent in consumer electronics and online content. Developed jointly by HP and Microsoft in 1996, sRGB provides a specific implementation of the RGB color space with well-defined chromaticities for the red, green, and blue primaries. It also specifies a transfer function (or gamma curve), which defines how the numerical values of R, G, and B map to actual luminance levels. In sRGB, this curve is a piecewise function: a linear segment in the darkest shades and a power function in the rest of the range, with a gamma value of approximately 2.2, which is close to the perceptual linearization of human vision.</p>
<p>One of the limitations of sRGB is its relatively narrow color gamut compared to other color spaces like Adobe RGB or ProPhoto RGB. This limitation is particularly evident in highly saturated colors, where sRGB can fail to reproduce the vibrancy seen in the real world or in wider-gamut color spaces. However, its ubiquity and standardization across a wide array of devices and software make it the default choice for web content, consumer electronics, and standard digital photography. Its compatibility and predictability across different platforms ensure that colors rendered in sRGB appear reasonably consistent on most modern displays, which are typically calibrated to this color space.</p>
<p>In current usage, while professional-grade equipment and applications might opt for wider-gamut color spaces like Adobe RGB, sRGB remains the principal color space for web-based content, ensuring that colors are represented uniformly across different viewing platforms. In essence, while RGB lays the foundation for digital color representation, sRGB standardizes this representation for widespread and consistent use in digital media.</p>
<section id="number-of-colors" class="level3">
<h3 class="anchored" data-anchor-id="number-of-colors">Number of Colors</h3>
<p>In both RGB and sRGB color spaces, the total number of colors that can be represented depends on the bit depth per channel. In typical scenarios where each of the RGB channels (Red, Green, Blue) is allocated 8 bits (which is quite common in consumer electronics and digital imagery), each channel can represent 2^8 or 256 distinct levels of intensity.</p>
<p>Since RGB and sRGB both use three channels, the total number of representable colors is calculated by multiplying the number of possibilities in each channel. So, the calculation would be:</p>
<blockquote class="blockquote">
<p>256 (Red) x 256 (Green) x 256 (Blue) = 16,777,216 total colors</p>
</blockquote>
<p>Therefore, both RGB and sRGB color spaces can represent approximately 16.7 million different colors when using an 8-bit per channel system. It’s important to note that this count is the same for both RGB and sRGB because the difference between these two spaces lies not in the number of colors they can represent but in how they interpret these colors (i.e., the color gamut and the mapping of color values to actual colors on a screen).</p>
<p>For images with higher bit depth per channel (like 10-bit, 12-bit, etc.), the total number of representable colors increases exponentially, allowing for a much richer and more nuanced color representation. However, the standard in most common digital applications remains 8-bit per channel.</p>
<p>Here are some examples of how certain colors are represented within this range:</p>
<ol type="1">
<li><p>Red: Pure red is represented as (255, 0, 0). This means the red channel is at its maximum, while green and blue are at their minimum.</p></li>
<li><p>Green: Pure green is (0, 255, 0), with the green channel at maximum and the others at minimum.</p></li>
<li><p>Blue: Pure blue appears as (0, 0, 255), with the blue channel at its maximum.</p></li>
<li><p>Yellow: Yellow is a mix of red and green, so it’s represented as (255, 255, 0).</p></li>
<li><p>Cyan: Cyan is a mix of green and blue, shown as (0, 255, 255).</p></li>
<li><p>Magenta: Magenta combines red and blue, represented as (255, 0, 255).</p></li>
<li><p>Black: Black is the absence of color in the RGB space, so all channels are at their minimum: (0, 0, 0).</p></li>
<li><p>White: White is the combination of all colors at their maximum intensity, so it’s (255, 255, 255).</p></li>
<li><p>Gray: Shades of gray are created when all three channels have equal intensity. For example, a medium gray might be (128, 128, 128).</p></li>
<li><p>Orange: Orange can vary in shade but is generally a mix of red and some green, such as (255, 165, 0).</p></li>
</ol>
<p>These examples provide a basic understanding of how different colors are represented in the RGB color space. By adjusting the intensity values of the red, green, and blue channels, a wide range of colors can be created.</p>
</section>
<section id="display-standards" class="level3">
<h3 class="anchored" data-anchor-id="display-standards">Display Standards</h3>
<p>The standard for most consumer TVs and monitors is typically an 8-bit per channel RGB color system. This means that each of the three color channels (Red, Green, Blue) can display 256 levels of intensity (from 0 to 255), resulting in 16,777,216 possible colors (256^3 = 16,777,216). This is often referred to as “True Color” or “24-bit color” (8 bits x 3 channels).</p>
<p>However, there is an increasing trend towards higher bit depths in newer, higher-end TVs and monitors, especially those geared towards professional use or high-quality entertainment experiences. These include:</p>
<ol type="1">
<li><p>10-bit color depth: With 10 bits per channel, a display can produce 1,024 levels of intensity per channel, resulting in a total of about 1.07 billion colors (1,024^3). This is significant for professional-grade monitors used in color-critical tasks like photo and video editing.</p></li>
<li><p>12-bit color depth: Some very high-end and specialized monitors and TVs offer 12-bit color, with 4,096 levels per channel, totaling around 68.7 billion colors (4,096^3). These are less common and are typically used in professional and cinematic settings.</p></li>
<li><p>HDR (high dynamic range): Modern high-end TVs and some monitors support HDR standards like HDR10, Dolby Vision, or HDR10+, which often use a 10-bit or even 12-bit color depth. HDR doesn’t just increase the number of colors; it also enhances the contrast and brightness, leading to a more dynamic and realistic image.</p></li>
<li><p>Wide color gamut: Apart from bit depth, many newer displays also support a wider color gamut (such as DCI-P3 or Rec. 2020), meaning they can display a broader range of colors than the traditional sRGB gamut.</p></li>
</ol>
<p>It’s important to note that to fully utilize these higher color depths and wider gamuts, the content being displayed (like movies, TV shows, or games) must also be created to support these standards, and the device’s hardware and software must be compatible with these advanced color features.</p>
</section>
<section id="complementary-colors" class="level3">
<h3 class="anchored" data-anchor-id="complementary-colors">Complementary Colors</h3>
<p>A complementary color is defined as a color that, when combined with a given color, produces a neutral color (white, gray, or black). Complementary colors are positioned opposite each other on the color wheel, a tool used to represent the relationships between colors.</p>
<p>In the RGB model, which is used for light-emitting sources like computer screens, the primary colors are red, green, and blue. The complementary color of red is cyan (a mix of green and blue), green’s complementary color is magenta (a mix of red and blue), and blue’s complementary color is yellow (a mix of red and green). When combined in this model, a color and its complementary produce white light. For example, combining red light with cyan light will result in white light.</p>
</section>
<section id="other-notations-for-rgb-color-space" class="level3">
<h3 class="anchored" data-anchor-id="other-notations-for-rgb-color-space">Other Notations for RGB Color Space</h3>
<section id="hex" class="level4">
<h4 class="anchored" data-anchor-id="hex">HEX</h4>
<p>HEX color notation is a staple in web and digital design, providing a succinct way to represent RGB colors. It encodes RGB values into a 6-digit hexadecimal number, prefaced by a hash symbol. Each pair of digits in this format, ranging from 00 to FF, corresponds to the red, green, and blue components of a color. This compact and efficient representation makes HEX particularly popular in coding and digital design environments.</p>
</section>
<section id="decimal" class="level4">
<h4 class="anchored" data-anchor-id="decimal">Decimal</h4>
<p>Decimal color notation is another way to describe RGB colors, similar to HEX but using decimal numbers. It presents colors with three values, each ranging from 0 to 255, for the red, green, and blue components. This approach is particularly user-friendly in programming and digital contexts, where working with decimal numbers is common.</p>
</section>
</section>
</section>
<section id="cmy-and-cmyk-color-spaces" class="level2">
<h2 class="anchored" data-anchor-id="cmy-and-cmyk-color-spaces">CMY and CMYK Color Spaces</h2>
<p>The CMY and CMYK color models are primarily used in color printing and are fundamentally different from the RGB color model, which is used in electronic displays. Both CMY and CMYK are based on the subtractive color model, unlike the additive nature of RGB.</p>
<section id="cmy" class="level3">
<h3 class="anchored" data-anchor-id="cmy">CMY</h3>
<p>CMY operates on the subtractive principle where colors are created by subtracting light. This model is based on the way light is absorbed and reflected off surfaces. It uses cyan, magenta, and yellow as its primary colors. These are the complementary colors of red, green, and blue (RGB), respectively.</p>
<p>In CMY, colors are created by partially or entirely subtracting the primary colors of light. For example, subtracting green from white light leaves magenta, subtracting red gives cyan, and subtracting blue yields yellow.</p>
<p>CMY is used in color printing. By combining varying amounts of cyan, magenta, and yellow, a wide range of colors can be reproduced. When all three colors are combined at their full intensity, they theoretically produce black, but in practice, they produce a muddy dark brown or gray.</p>
</section>
<section id="cmyk" class="level3">
<h3 class="anchored" data-anchor-id="cmyk">CMYK</h3>
<p>CMYK adds a fourth component, “key” (black), to the CMY model. The ‘K’ component is used because pure black cannot be created reliably through the combination of CMY inks due to imperfections in ink pigments. Adding black ink allows for deeper, more accurate, and consistent blacks.</p>
<p>CMYK creates colors through a subtractive process by layering different amounts of cyan, magenta, yellow, and black ink on paper. The more ink used, the darker the color becomes. Black ink in CMYK is also more economical and provides better shadow detail than CMY, making it a more efficient color model for full-color printing.</p>
</section>
<section id="differences-with-rgb" class="level3">
<h3 class="anchored" data-anchor-id="differences-with-rgb">Differences with RGB</h3>
<p>The most important difference is that RGB is an additive color model used in electronic displays, where colors are created by combining light. CMY and CMYK are subtractive, used in printing, where colors are created by subtracting light. Or, with different words, RGB is used for digital screens like monitors, TVs, and cameras, where light is emitted directly. CMY and CMYK are used in printing on physical media, where light is reflected.</p>
<p>In RGB, black is the absence of light, while in CMYK, black is a separate ink component for deeper and more uniform blacks.</p>
</section>
</section>
<section id="hsl-and-hsv-color-spaces" class="level2">
<h2 class="anchored" data-anchor-id="hsl-and-hsv-color-spaces">HSL and HSV Color Spaces</h2>
<p>Both HSL (hue, saturation, lightness) and HSV (hue, saturation, value) are color models used to represent the RGB color space in terms that are more intuitive for humans to understand and manipulate. These models describe colors in terms of their shade (hue), intensity (saturation), and brightness (lightness/value):</p>
<ul>
<li><p>HSL:</p>
<ul>
<li><p>Hue: Represents the type of color, or the color itself. It is typically measured in degrees around a color wheel, with red at 0°, green at 120°, and blue at 240°.</p></li>
<li><p>Saturation: Indicates the intensity or purity of the color. In HSL, saturation ranges from 0%, which is a shade of gray, to 100%, which is the full color.</p></li>
<li><p>Lightness: Also known as luminance, lightness defines how light or dark a color is. A lightness of 0% is black, 50% is the true color, and 100% is white.</p></li>
</ul></li>
<li><p>HSV:</p>
<ul>
<li><p>Hue: Similar to HSL, it defines the color itself.</p></li>
<li><p>Saturation: Measures the intensity or vibrancy of the color. It ranges from 0%, which is completely unsaturated (gray), to 100%, which is the most saturated form of the color.</p></li>
<li><p>Value: Also known as brightness, it represents the brightness or darkness of the color. A value of 0% is black, and 100% is the brightest form of the color.</p></li>
</ul></li>
</ul>
<section id="differences-with-rgb-1" class="level4">
<h4 class="anchored" data-anchor-id="differences-with-rgb-1">Differences with RGB</h4>
<p>RGB represents colors by specifying the intensity of each primary color, making it less intuitive for tasks like adjusting brightness or saturation. HSL and HSV are transformations of the RGB color model designed to be more intuitive for human perception. They allow for easier adjustments of color properties like shade, intensity, and brightness.</p>
<p>HSL and HSV are often used in color picker tools in graphic design software because they offer a more user-friendly way to select and manipulate colors. Moreover, they separate the chromatic information (hue and saturation) from the achromatic information (lightness/value), unlike RGB where all three parameters mix chromatic and achromatic components.</p>
<p>While RGB is suited for electronic displays and color mixing with light, HSL and HSV are more suited for tasks that involve adjusting and fine-tuning colors, like in graphic design and photo editing. In essence, HSL and HSV are used to represent the same colors as RGB but in a way that aligns more closely with how people think about and perceive colors. This makes them particularly useful in interfaces and applications where users need to make precise adjustments to color properties.</p>
</section>
</section>
<section id="yiq-and-yuv-color-spaces" class="level2">
<h2 class="anchored" data-anchor-id="yiq-and-yuv-color-spaces">YIQ and YUV Color Spaces</h2>
<p>YIQ and YUV are color spaces primarily used in the broadcasting industry, particularly in television systems. Both are designed to split a color signal into luminance and chrominance components, but they are used in different television standards.</p>
<p>The YIQ color space was predominantly used in the NTSC color television system, mainly in North America. In YIQ, ‘Y’ stands for the luminance component, which represents the brightness of the image. The ‘I’ and ‘Q’ components represent the chrominance or color information. ‘I’ carries information about the orange-cyan range, while ‘Q’ carries information about the green-magenta range. The separation of luminance and chrominance in YIQ allowed NTSC broadcasts to be compatible with black-and-white televisions. Luminance (Y) could be displayed by black-and-white TVs, while color TVs could use all three components (Y, I, Q) to display the full color image.</p>
<p>YUV is similar to YIQ in that it also separates the color signal into luminance (Y) and two chrominance components (U and V). YUV is used in the PAL and SECAM color television systems, prevalent in Europe and other parts of the world. The ‘Y’ component, like in YIQ, represents the image brightness. ‘U’ represents the blue-luminance difference, and ‘V’ represents the red-luminance difference. This separation was also designed for compatibility with black-and-white TVs, with the added advantage of better color quality compared to NTSC, although at a slightly lower resolution.</p>
<p>Both YIQ and YUV were developed to maximize the efficiency of color transmission in broadcasting and to ensure backward compatibility with black-and-white television systems. They differ from RGB, which is used in electronic displays and combines red, green, and blue light to produce colors. While RGB is more straightforward for generating colors electronically, YIQ and YUV are more efficient for broadcasting purposes because they separate the brightness of the image from the color information, which can be more efficiently compressed and transmitted.</p>
<p>The use of YIQ has declined with the shift towards digital broadcasting, which often uses other color spaces like YCbCr. YUV, on the other hand, is still relevant in many video processing applications and is closely related to the YCbCr color space used in digital video.</p>
</section>
<section id="cie-color-spaces" class="level2">
<h2 class="anchored" data-anchor-id="cie-color-spaces">CIE Color Spaces</h2>
<p>The International Commission on Illumination, known as CIE (Commission Internationale de l’Éclairage), is a significant organization in the field of color and lighting standards. CIE has introduced several critical color spaces, including XYZ, CIELab, and CIELCh, each serving unique purposes in color science.</p>
<section id="xyz" class="level3">
<h3 class="anchored" data-anchor-id="xyz">XYZ</h3>
<p>The CIE XYZ color space, established in 1931, is foundational in the field of colorimetry. It’s a device-independent model representing color perceptions of a standard observer. In XYZ, ‘X’ represents a mix of cone response curves, ‘Y’ denotes luminance, and ‘Z’ corresponds to blue stimulation. This color space serves as a reference, allowing for the translation of colors between different systems and devices. The gamut of XYZ encompasses all perceivable colors, making it a comprehensive standard for color representation.</p>
</section>
<section id="cielab" class="level3">
<h3 class="anchored" data-anchor-id="cielab">CIELab</h3>
<p>The CIELab (or Lab) color space, introduced in 1976, with its broad gamut and perceptually uniform characteristics, is designed to encompass the entire range of colors visible to the human eye. This extensive gamut means it can represent colors that are outside the range of many display systems and printers.</p>
<p>In CIELab:</p>
<ul>
<li><p>The ‘L’ component (lightness) ranges from 0 to 100, where 0 represents black, and 100 represents white. This vertical axis accounts for the luminance of colors.</p></li>
<li><p>The ‘a’ component operates on a green to red axis. Negative values of ‘a’ indicate green, while positive values indicate red.</p></li>
<li><p>The ‘b’ component works on a blue to yellow axis, with negative values representing blue and positive values indicating yellow.</p></li>
</ul>
<p>This structure allows for a precise and detailed representation of colors. For example:</p>
<ul>
<li><p>A strong green might be denoted as (L=50, a=-50, b=50), representing a mid-level lightness with a strong green component and a touch of yellow.</p></li>
<li><p>A deep red could be represented as (L=40, a=60, b=30), indicating a darker shade (lower lightness) with a dominant red component and some yellow.</p></li>
</ul>
<p>The notation in CIELab is quite distinct from RGB. While RGB specifies the intensity of red, green, and blue light to create colors (like RGB(255, 0, 0) for bright red), CIELab describes colors in terms of lightness and color-opponent dimensions, which align more closely with the human perception of colors.</p>
<p>This perceptual uniformity – where a given numerical change corresponds to a roughly equal perceptual change in color – is a key feature of CIELab. It ensures that when colors are altered or compared in this space, the perceived differences are consistent across the color spectrum.</p>
<p>CIELab’s broad gamut and perceptual uniformity make it a preferred choice in industries where accurate color differentiation and measurement are critical, like paint manufacturing, textile production, and quality control in various product design processes. It’s also commonly used in digital imaging and photography for color correction and editing, as it offers more intuitive control over color adjustments than RGB.</p>
<p>A classic example of colors that can be represented in CIELab but are often outside the gamut of many RGB devices are certain highly saturated cyans and blues. For instance, a very bright, saturated cyan might be represented in CIELab as something like (L=90, a=-40, b=-15). This color would be extremely vivid and might not be accurately displayed on a standard RGB monitor, which would struggle to reproduce its intensity and saturation. Similarly, some extremely bright and saturated yellows and greens can also fall outside the typical RGB gamut. These colors are so vivid that they can only be seen under intense lighting conditions, such as direct sunlight, and cannot be fully replicated on standard digital displays.</p>
</section>
<section id="cielch" class="level3">
<h3 class="anchored" data-anchor-id="cielch">CIELCh</h3>
<p>CIELCh is a color space closely related to CIELab but represented in cylindrical coordinates instead of Cartesian ones. It’s derived from the CIELab color space and is designed to represent color in a way that’s more intuitive and aligned with how humans perceive color changes.</p>
<p>In CIELCh, the components represent:</p>
<ol type="1">
<li><p>L (lightness): Just like in CIELab, ‘L’ in CIELCh represents the lightness of the color, with 0 being black and 100 being white.</p></li>
<li><p>C (chroma): This is essentially the saturation of the color. Chroma in CIELCh is derived from the a* and b* components of CIELab. It represents the vividness or intensity of the color. Higher chroma values indicate more intense, vivid colors, while lower chroma values result in duller, more washed-out colors.</p></li>
<li><p>h (hue angle): Instead of using the a* and b* Cartesian coordinates to define the hue, CIELCh uses an angle in a cylindrical space. This hue angle starts from the positive a* axis and is usually measured in degrees (0° to 360°). Different values correspond to different hues (colors), similar to positions on a traditional color wheel. For example, 0° or 360° represents red/magenta, 90° represents yellow, 180° represents green, and 270° represents blue.</p></li>
</ol>
<p>The transformation from CIELab to CIELCh is a conversion from Cartesian to cylindrical coordinates. The lightness (L) remains the same, but the a* and b* values in CIELab are converted to chroma (C) and hue (h) in CIELCh. The formulae for these conversions involve trigonometric functions where chroma (C) is calculated as the square root of (a*^2 + b*^2), and the hue angle (h) is calculated using the arctan function.</p>
<p>CIELCh is useful in various applications that require intuitive color adjustment and selection. The cylindrical representation makes it easier to understand and manipulate hue and saturation independently of lightness, which aligns more closely with how people think about and use color, especially in fields like graphic design, painting, and digital media.</p>
<p>This color space is particularly favored for tasks where color harmony and balance are important, as it allows for a straightforward manipulation of color relationships and contrasts.</p>
</section>
<section id="cieluv" class="level3">
<h3 class="anchored" data-anchor-id="cieluv">CIELUV</h3>
<p>CIELUV is a color space introduced by the International Commission on Illumination (CIE) to enable more effective color communication, especially for light emitting or reflecting surfaces. It’s part of the CIE 1976 color spaces, which also include CIELab.</p>
<p>The name CIELUV comes from the CIE L<em>u</em>v* color space. It’s designed similarly to CIELab, with ‘L’ representing lightness. However, while CIELab uses ‘a’ and ‘b’ for color-opponent dimensions, CIELUV uses ‘u*’ and ‘v*’ for chromaticity. These dimensions are based on the CIE 1960 u-v chromaticity diagram, which is a projection of the CIE XYZ color space.</p>
<p>CIELUV is particularly useful for applications like lighting design, video, and other emissive display applications where color gamut is crucial. One of its strengths lies in its ability to accurately represent highly saturated colors, a limitation in the CIELab color space.</p>
<p>In terms of technical details, the ‘L’ in CIELUV represents the perceived lightness, similar to CIELab. The ‘u*’ and ‘v*’ coordinates, however, are calculated differently, focusing on chromaticity. This difference stems from the way the two color spaces project the XYZ space into the color-opponent dimensions. In CIELUV, these projections are designed to better represent the way we perceive color in light-emitting sources.</p>
<p>When comparing CIELUV to CIELab, the key difference lies in their treatment of chromaticity and the types of applications they’re best suited for. CIELab is generally preferred for surface colors (like paint or ink), where color is a result of light reflecting off an object. In contrast, CIELUV is more suited for light-emitting sources (like displays or lights), where color is produced by light itself.</p>
<p>Both color spaces derive from the XYZ model and share the lightness dimension (L*). However, their approach to chromaticity makes them suitable for different applications and types of color processing. CIELUV’s emphasis on chromaticity makes it a valuable tool in industries dealing with light sources, displays, and environments where the light’s color itself is the primary concern.</p>
</section>
<section id="lchab" class="level3">
<h3 class="anchored" data-anchor-id="lchab">LCH(ab)</h3>
<p>The LCH(ab) color space, often simply referred to as LCH, is a color model derived from the CIELab color space. It represents colors in a more intuitive way compared to the Cartesian coordinates (a* and b*) used in CIELab. The LCH color model is based on cylindrical coordinates rather than Cartesian coordinates and consists of three components:</p>
<ol type="1">
<li><p>Lightness (L): Similar to the L* in CIELab, it represents the lightness of the color, where 0 is black, 100 is white, and values in between represent various shades of gray.</p></li>
<li><p>Chroma (C): Chroma in LCH is analogous to saturation in other color models. It represents the intensity or purity of the color. Higher chroma values indicate more vibrant colors, while lower values result in more muted tones.</p></li>
<li><p>Hue (H): Hue is represented as an angle (in degrees) around a color wheel. It defines the type of color (such as red, blue, green, yellow, etc.). In LCH, hue starts at 0 degrees for red and moves through the spectrum, with green at 120 degrees, blue at 240 degrees, and so forth.</p></li>
</ol>
<p>The LCH color space is particularly useful in applications where understanding and manipulating the color relationships and harmonies are important. It’s often used in graphic design, painting, and digital media for this reason. By separating the color components in this way, LCH allows designers to adjust hue and chroma independently of lightness, which can be more intuitive than working with the a* and b* coordinates in CIELab.</p>
<p>In essence, LCH(ab) offers a perceptually-based approach to color representation, aligning closely with how humans perceive and interpret color differences, making it a valuable tool in color-sensitive work.</p>
</section>
</section>
<section id="color-space-as-a-mathematical-space-subset" class="level2">
<h2 class="anchored" data-anchor-id="color-space-as-a-mathematical-space-subset">Color Space as a Mathematical Space Subset</h2>
<p>The concept of whether color spaces are subsets of integer or real mathematical spaces can be understood in terms of how they represent color values and the precision with which they operate.</p>
<ol type="1">
<li><p>RGB: RGB, commonly used in digital displays and imaging, typically uses integer values in practical applications, especially in 8-bit per channel systems where each color (Red, Green, Blue) is represented by an integer from 0 to 255. However, in more precise applications, such as high dynamic range (HDR) imaging or in professional color grading, RGB values can be represented in a floating-point format (real numbers), allowing for a finer gradation and a wider range of color intensities.</p></li>
<li><p>CIELab and CIELuv: Both CIELab and CIELuv are part of the CIE 1976 color space. They are generally considered to be subsets of the real number space. The L*, a*, b* (CIELab) and L*, u*, v* (CIELuv) coordinates are typically represented as real numbers to allow for a high degree of precision, which is crucial in color matching and colorimetric applications. This representation aligns with their design as perceptually uniform spaces, where small changes in values correspond to consistent perceptual differences in color.</p></li>
<li><p>HEX: The HEX color notation, used predominantly in web design, is based on integer values. It is essentially a hexadecimal representation of RGB values, where each color channel is represented by two hexadecimal digits, corresponding to an integer value between 0 and 255.</p></li>
<li><p>CIE XYZ: The CIE XYZ color space, which serves as a foundation for many other color spaces, including CIELab and CIELuv, represents colors using real numbers. This representation allows for a high degree of precision and is important for scientific and industrial applications where accurate color measurement and reproduction are necessary.</p></li>
<li><p>YIQ, YUV, and others: Used primarily in broadcasting and video processing, these color spaces often use real numbers for greater precision, especially in professional applications. However, for standard television broadcast and consumer electronics, these values are typically quantized into integer values.</p></li>
</ol>
<p>In summary, while practical implementations of these color spaces in digital devices often use integer values for ease of processing and storage, the theoretical models of most advanced color spaces, especially those used in colorimetry and professional applications, rely on real numbers for greater precision and a more accurate representation of color.</p>
</section>
<section id="color-spaces-conversion-libraries" class="level2">
<h2 class="anchored" data-anchor-id="color-spaces-conversion-libraries">Color Spaces Conversion Libraries</h2>
<section id="python-colormath" class="level3">
<h3 class="anchored" data-anchor-id="python-colormath">python-colormath</h3>
<p><a href="https://python-colormath.readthedocs.io/en/latest/" title="python-colormath">python-colormath</a> is a simple Python module that spares the user from directly dealing with color math. Some features include:</p>
<ul>
<li><p>Support for a wide range of color spaces. A good chunk of the CIE spaces, RGB, HSL/HSV, CMY/CMYK, and many more.</p></li>
<li><p>Conversions between the various color spaces. For example, XYZ to sRGB, Spectral to XYZ, CIELab to Adobe RGB.</p></li>
<li><p>Calculation of color difference. All CIE Delta E functions, plus CMC.</p></li>
<li><p>Chromatic adaptations (changing illuminants).</p></li>
<li><p>RGB to hex and vice-versa.</p></li>
<li><p>16-bit RGB support.</p></li>
<li><p>Runs on Python 2.7 and Python 3.3+.</p></li>
</ul>
<p>To convert a color from sRGB to CIELab using the Python <code>colormath</code> library, you first need to ensure that <code>colormath</code> is installed in your Python environment. You can install it using pip:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install colormath</span></code></pre></div>
<p>Once <code>colormath</code> is installed, you can use it to perform the conversion. Here’s a simple example:</p>
<div class="sourceCode" id="annotated-cell-2" style="background: #f1f3f5;"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colormath.color_objects <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sRGBColor, LabColor, XYZColor, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="annotated-cell-2-2">                                    LCHabColor, LCHuvColor, HSVColor, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="annotated-cell-2-3">                                    CMYColor, CMYKColor</span>
<span id="annotated-cell-2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> colormath.color_conversions <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> convert_color</span>
<span id="annotated-cell-2-5"></span>
<span id="annotated-cell-2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define an sRGB color (is_upscaled=True if you're using 0-255 range)</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-2-7" class="code-annotation-target">rgb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sRGBColor(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">128.</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">128.</span>, is_upscaled<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="annotated-cell-2-8"></span>
<span id="annotated-cell-2-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert the sRGB color to other color spaces</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-2-10" class="code-annotation-target">lab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_color(rgb, LabColor)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CIELab</span></span>
<span id="annotated-cell-2-11">xyz <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_color(rgb, XYZColor)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># XYZ </span></span>
<span id="annotated-cell-2-12">lch_ab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_color(rgb, LCHabColor) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># LCH(ab)</span></span>
<span id="annotated-cell-2-13">lch_uv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_color(rgb, LCHuvColor) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># LCH(uv)</span></span>
<span id="annotated-cell-2-14">hsv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_color(rgb, HSVColor)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># HSV</span></span>
<span id="annotated-cell-2-15">cmy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_color(rgb, CMYColor)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CMY</span></span>
<span id="annotated-cell-2-16">cmyk <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_color(rgb, CMYKColor)    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CMYK</span></span>
<span id="annotated-cell-2-17"></span>
<span id="annotated-cell-2-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Print the colors in different color spaces  </span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-2-19" class="code-annotation-target"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CIELab: "</span>, lab)</span>
<span id="annotated-cell-2-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CIELab:  LabColor (lab_l:29.7843 lab_a:58.9285 lab_b:-36.4932) </span></span>
<span id="annotated-cell-2-21"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"XYZ: "</span>, xyz)         </span>
<span id="annotated-cell-2-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># XYZ:  XYZColor (xyz_x:0.1280 xyz_y:0.0615 xyz_z:0.2093)</span></span>
<span id="annotated-cell-2-23"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LCH(ab): "</span>, lch_ab)  </span>
<span id="annotated-cell-2-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># LCH(ab):  LCHabColor (lch_l:29.7843 lch_c:69.3132 lch_h:328.2310)</span></span>
<span id="annotated-cell-2-25"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LCH(uv): "</span>, lch_uv)  </span>
<span id="annotated-cell-2-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># LCH(uv):  LCHuvColor (lch_l:29.7843 lch_c:67.8446 lch_h:307.7154)</span></span>
<span id="annotated-cell-2-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HSV: "</span>, hsv)         </span>
<span id="annotated-cell-2-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># HSV:  HSVColor (hsv_h:300.0000 hsv_s:1.0000 hsv_v:0.5020)</span></span>
<span id="annotated-cell-2-29"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CMY: "</span>, cmy)         </span>
<span id="annotated-cell-2-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CMY:  CMYColor (cmy_c:0.4980 cmy_m:1.0000 cmy_y:0.4980)</span></span>
<span id="annotated-cell-2-31"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CMYK: "</span>, cmyk)       </span>
<span id="annotated-cell-2-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># CMYK:  CMYKColor (cmyk_c:0.0000 cmyk_m:1.0000 cmyk_y:0.0000 cmyk_k:0.4980)</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="7" data-code-annotation="1">An sRGB color is defined with the red, green, and blue components. If you’re using values in the 0-255 range, set <code>is_upscaled=True</code> so that <code>colormath</code> knows to scale them down to 0-1.</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="10" data-code-annotation="2">The <code>convert_color</code> function is used to convert the defined sRGB color to the CIELab color space.</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="19" data-code-annotation="3">Finally, the resulting CIELab color is printed out. Other conversions follow.</span>
</dd>
</dl>
<p>The output will be the CIELab representation of the given sRGB color. Keep in mind that <code>colormath</code> handles these conversions assuming standard conditions and may not account for specific display or lighting characteristics unless explicitly specified.</p>
</section>
<section id="other-libraries" class="level3">
<h3 class="anchored" data-anchor-id="other-libraries">Other Libraries</h3>
<p>There are several other libraries in Python and other programming languages that can be used to convert between color spaces. Here are a few notable ones:</p>
<ol type="1">
<li><p>OpenCV (Python, C++, Java): Primarily known for its extensive functionalities in computer vision, OpenCV also offers color space conversion functions. It can handle conversions between various color spaces, including RGB, HSV, CIELab, and more.</p></li>
<li><p>Pillow (Python): The Pillow library, which is an extension of the Python Imaging Library (PIL), includes functions for converting images between different color spaces.</p></li>
<li><p>Color.js (JavaScript): A JavaScript library for color conversion and manipulation, it supports a wide range of color spaces and is particularly useful for web development.</p></li>
<li><p>D3.js (JavaScript): While primarily a library for producing interactive data visualizations, D3.js also includes methods for color space conversion, useful in the context of web design and visualizations.</p></li>
<li><p>Tinycolor (JavaScript): A small, fast library for color manipulation and conversion in JavaScript. It supports RGB, HSV, HSL, and HEX formats.</p></li>
<li><p>Colorspacious (Python): A Python library designed to convert and manipulate various color spaces with a focus on perceptual uniformity and color difference calculations.</p></li>
<li><p>Matplotlib (Python): Although mainly a plotting library, Matplotlib in Python can convert colors between RGB and other color spaces as part of its plotting functionalities.</p></li>
</ol>
<p>Each of these libraries has its own set of features and strengths, and the choice of library can depend on the specific requirements of your project, such as the programming language you’re using, the color spaces you need to work with, and the level of precision or control you need over the color conversion process.</p>
</section>
</section>
<section id="python-script-for-cielab-color-sampling-and-conversion" class="level2">
<h2 class="anchored" data-anchor-id="python-script-for-cielab-color-sampling-and-conversion">Python Script for CIELab Color Sampling and Conversion</h2>
<p>The Python script is designed to uniformly sample the CIELab color space and convert these samples to RGB. It also finds the nearest CIELab color to a given target color, either in CIELab or RGB space, and saves comparison charts. The script contains several key functions:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">generate_uniform_lab_samples(n_samples)</span></code></pre></div>
<p>This function generates uniformly distributed samples in the CIELab color space. It calculates the number of points per dimension based on the cubic root of the total number of desired samples, creating a grid of points in the CIELab space. If more points are generated than needed, it randomly samples from these points to get the desired number.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">lab_to_rgb(lab_arr)</span></code></pre></div>
<p>This function converts a batch of CIELab values to RGB and marks any colors that are approximated due to out-of-gamut issues. It uses the <code>skimage</code> library for the CIELab to RGB conversion and checks for any warnings during the conversion process, specifically looking for “negative Z values” which indicate an approximation.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">rgb_to_lab(rgb)</span></code></pre></div>
<p>This function converts an RGB color to the CIELab color space. It normalizes the RGB values (assuming they are in the 0-255 range) and uses the <code>colormath</code> library to perform the conversion.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">create_color_chart_with_spacing(lab_samples, rgb_samples, approx_flags, square_size_cm, spacing_cm, label_font_size, text_spacing_cm, save_path)</span></code></pre></div>
<p>This function creates a square image containing color squares with spacing between them. Each square represents a color sample. It calculates the total image size considering the spacing and text space and then uses <code>matplotlib</code> to create and save the image.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">find_nearest_color_lab(target_lab, generated_lab_samples)</span></code></pre></div>
<p>This function finds the nearest CIELab color to a given target color among generated samples using Delta E. It compares the target color with each generated sample using the <code>delta_e_cie2000</code> function from the <code>colormath</code> library.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">save_comparison_chart(target_lab, nearest_lab, square_size, spacing, save_path)</span></code></pre></div>
<p>This function saves an image with two squares: one for the target color and one for the nearest CIELab color. It draws the squares and saves the image using <code>matplotlib</code>.</p>
<p>The script also includes a section at the end for generating samples, converting them, and saving comparison charts.</p>
<p>This code is a comprehensive tool for exploring and visualizing the CIELab color space, its conversion to RGB, and the assessment of color proximity within this space.</p>
<p>Download the <a href="https://github.com/antomon/antomon.github.io/blob/b6d9822757db56d5906f1b3f8e6bb6e3040e7c3a/_static/posts/colors-101/cielab_sampler.ipynb">Jupyter notebook</a> or open it in Colab (click on the badge below) to sample the CIELab space and get the nearest sample of a given color.</p>
<p><a href="https://colab.research.google.com/github/antomon/antomon.github.io/blob/b6d9822757db56d5906f1b3f8e6bb6e3040e7c3a/_static/posts/colors-101/cielab_sampler.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Colab"></a></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><a href="https://cie.co.at/" title="CIE website">CIE website</a>: International Commission on Illumination official website</p>
<p><a href="http://www.brucelindbloom.com/" title="Bruce Justin Lindbloom's website">Bruce Justin Lindbloom’s website</a>: useful for color spaces conversion formulas</p>
<p><a href="https://johnthemathguy.blogspot.com/" title="John the Math Guy's website">John the Math Guy’s website</a>: outstanding resource for color theory</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>theory</category>
  <category>python</category>
  <category>&amp;#127468;&amp;#127463;</category>
  <guid>https://antomon.github.io/posts/color-space-sampling-101/</guid>
  <pubDate>Fri, 26 Jan 2024 23:00:00 GMT</pubDate>
  <media:content url="https://antomon.github.io/posts/color-space-sampling-101/colors101.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
