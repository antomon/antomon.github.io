
Last week, AI grabbed several Nobel prizes. Yet, there is one scholar who helped define the field but many people still don't know. This is the story of Frank Rosenblatt.

Today, Frank Rosenblatt is not a household name, at least not as much as other key contributors to AI. But in the 1950s and 1960s he was "the man."

In 1957, Rosenblatt created the perceptron, the first algorithm that could learn from mistakes. The perceptron can be thought of as the basic building block of today's neural networks. It transforms one layer of inputs into a single output, adjusting the weights connecting the inputs to the output each time it fails.

The perceptron was maybe the most important innovation in AI since the 1943 neuron designed by McCulloch and Pitts. But unlike McCulloch and Pitts neuron, Rosenblatt's neuron could learn. This distinction didn't go unnoticed. The press at the time talked about perceptrons that one day could walk and talk. The AI hype of the late 1950s and 1960s can be attributed partly back to Rosenblatt's discovery.

But Rosenblatt wasn't a mathematician or computer scientist. He was a psychologist looking to simulate and understand perception and cognition. Still, his single layer perceptron had a problem. It could only separate things that could be divided by a single line. 

This weakness was targeted by Marvin Minsky and Seymour Papert in their 1969 book Perceptrons. Minsky and Papert highlighted the idea that Rosenblatt's neuron couldn't solve non-linear problems (like the XOR function). Today, many blame Minsky and Papert's critique for reducing interest in neural networks and causing the "AI winter" of the 1970s. 

Rosenblatt was aware of this problem, and had the intuition that multiple layers of perceptrons could solve this problem. In fact, in his 1961 book called Principles of Neurodynamics, he included several sections on multilayer perceptrons, including even a section (13.3) on back-propagating error correction procedures. 

Tragically, Frank Rosenblatt’s promising career was cut short. He passed away in a boating accident while celebrating his birthday in 1971, at the young age of 43. His untimely death came at a time marked by a negative perception of his work, meaning that he  didn’t receive the recognition he deserved—at least, not immediately.

Fifty-three years later, we can now all agree on the long term impact of the work of this brilliant scholar. The principles he developed are fundamental to the neural networks that drive today's AI systems. From self-driving cars to natural language processing, AI owes a great deal to Rosenblatt’s early insights. As we stand at the forefront of the AI revolution, it is important to remember and honor the work of this visionary psychologist whose legacy continues to shape how we build and understand our world.

The Perceptron Controversy
Connectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.

https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/